{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYejP7ggjas4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6sBdzU0_m1k",
        "outputId": "25020969-7aee-4f0b-bd18-a0c88b318280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import statements"
      ],
      "metadata": {
        "id": "ScT6oWtqjxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, dok_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import dok_matrix"
      ],
      "metadata": {
        "id": "ZJa1HVyZjw0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cu5MtwKFyQrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LBFGS"
      ],
      "metadata": {
        "id": "XCekCfVsj5w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The m value in L-BFGS (Limited-memory BFGS) refers to the number of correction vectors (or memory size) used to approximate the inverse of the Hessian matrix. It essentially controls how much of the past optimization history is stored and used for the two-loop recursion to compute the search direction."
      ],
      "metadata": {
        "id": "OLCtJ0TuyPTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Initialization:\n",
        "At the beginning, the weights are flattened from the input data_matrix, and the initial gradient is computed.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "x = data_matrix.values.flatten()  # Flatten the binary matrix (initial weights)\n",
        "grad = grad_func(x)  # Compute the initial gradient\n",
        "The initial weight vector x is just a flattened version of the input data matrix, and the initial gradient grad is computed based on the current weights (which is just the starting point of optimization).\n",
        "\n",
        "###2. Gradient Calculation (Steepest Descent Direction):\n",
        "In the optimization process, the gradient grad is calculated at each iteration. The gradient represents how much the objective function (squared error) changes with respect to each weight. The L-BFGS method then uses this gradient to determine the direction of the weight update.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "grad = grad_func(x)  # Gradient of the current weights\n",
        "The grad_func(x) function computes the gradient of the squared error function:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def grad_func(x):\n",
        "    mat = x.reshape(n_rows, n_cols)  # Reshape the flattened x into a matrix\n",
        "    return 2 * (mat - data_matrix.values).flatten()  # Gradient of squared error\n",
        "This means that the gradient points in the direction of steepest ascent, and in L-BFGS, we want to update the weights in the opposite direction (steepest descent).\n",
        "\n",
        "###3. Two-Loop Recursion (Approximate Inverse Hessian):\n",
        "The L-BFGS algorithm approximates the inverse Hessian using previously computed gradients and weight changes. This is done using the two-loop recursion to compute the search direction:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "q = grad.copy()  # Copy of the gradient for use in the two-loop recursion\n",
        "alpha = []  # Storage for the alpha coefficients\n",
        "for i in range(len(s_list) - 1, -1, -1):  # Loop through previous s and y\n",
        "    rho = rho_list[i]  # The rho coefficient (1 / (y_i * s_i))\n",
        "    alpha_i = rho * np.dot(s_list[i], q)  # Compute alpha_i\n",
        "    alpha.append(alpha_i)  # Store alpha_i\n",
        "    q -= alpha_i * y_list[i]  # Update q (gradient direction)\n",
        "\n",
        "if len(s_list) > 0:  # Check if any previous corrections exist\n",
        "    H_0 = np.dot(y_list[-1], s_list[-1]) / np.dot(y_list[-1], y_list[-1])  # Compute an estimate of H_0\n",
        "else:\n",
        "    H_0 = 1.0  # Default if no previous corrections\n",
        "\n",
        "z = H_0 * q  # Scale q by H_0 to get initial search direction\n",
        "for i in range(len(s_list)):\n",
        "    rho = rho_list[i]\n",
        "    beta = rho * np.dot(y_list[i], z)  # Compute beta coefficient\n",
        "    z += s_list[i] * (alpha[i] - beta)  # Update search direction z\n",
        "\n",
        "direction = -z  # The negative of z gives the descent direction\n",
        "Here’s what happens in this step:\n",
        "\n",
        "Two-loop recursion: The algorithm uses past information (changes in weights s_i and gradients y_i) to recursively compute an approximation to the inverse Hessian. This gives a direction to adjust the weights that considers second-order information (curvature).\n",
        "Direction calculation: The search direction direction is calculated by updating the gradient using this recursion. It is the direction in which the weights will be updated.\n",
        "###4. Line Search (Finding Step Size):\n",
        "Next, the algorithm performs a line search to determine the appropriate step size for the weight update.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "step_size = 1.0  # Start with an initial step size\n",
        "while True:\n",
        "    x_new = np.clip(x + step_size * direction, 0, 1).round()  # Update weights using the step size and direction\n",
        "    f_new = func(x_new)  # Compute the new function value\n",
        "    if f_new < f_val:  # Check if the new function value is lower\n",
        "        break\n",
        "    step_size *= 0.5  # If not, reduce the step size and try again\n",
        "The line search finds a step size (step_size) that minimizes the objective function along the search direction.\n",
        "The weights x are updated by taking a step in the calculated direction, and the step size is adjusted to ensure a decrease in the objective function.\n",
        "###5. Update Weights:\n",
        "After determining the correct step size, the weights x_new are updated, and the gradient is recalculated for the new weights.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "x_new = x_new.round()  # Keep the weights binary (rounded to 0 or 1)\n",
        "grad_new = grad_func(x_new)  # Recompute the gradient for the updated weights\n",
        "Weight update: The weights are updated by adding the scaled search direction to the current weights (x + step_size * direction).\n",
        "Binary weights: Since the weights are constrained to be binary, the updated weights are rounded to 0 or 1.\n",
        "###6. L-BFGS History Update:\n",
        "Finally, the algorithm updates the historical information (s_list, y_list, and rho_list) for use in future iterations.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "s = x_new - x  # Change in weights\n",
        "y = grad_new - grad  # Change in gradients\n",
        "if np.dot(s, y) > 1e-10:  # Ensure numerical stability\n",
        "    if len(s_list) >= m:\n",
        "        s_list.pop(0)\n",
        "        y_list.pop(0)\n",
        "        rho_list.pop(0)\n",
        "    s_list.append(s)\n",
        "    y_list.append(y)\n",
        "    rho_list.append(1.0 / np.dot(s, y))  # Store s, y, and rho for future use\n",
        "This step ensures that the algorithm maintains a record of the changes in weights (s) and gradients (y), which are used in the two-loop recursion to approximate the inverse Hessian in future iterations.\n",
        "\n",
        "###7. Final Weight Calculation:\n",
        "Once the optimization converges (the gradient is small enough), the final weights are computed by averaging the weight values for each feature (column) in the matrix:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "weights = x.reshape(n_rows, n_cols).mean(axis=0)  # Average weight for each column (feature)\n",
        "This final weights represents the optimized set of weights for each feature in the input data matrix."
      ],
      "metadata": {
        "id": "kEHs0EW14qRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lbfgs_estimate_weights(data_matrix, m=10, tol=1e-5, max_iter=100):\n",
        "    \"\"\"\n",
        "    L-BFGS algorithm for estimating weights for each column of a binary 2D matrix.\n",
        "\n",
        "    Parameters:\n",
        "        data_matrix (numpy.ndarray): Binary 2D matrix (input data for optimization).\n",
        "        m (int): Number of corrections to approximate the inverse Hessian.\n",
        "        tol (float): Tolerance for convergence.\n",
        "        max_iter (int): Maximum number of iterations.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Weights for each column (feature).\n",
        "        float: Normalization parameter (estimated bias term).\n",
        "        float: Final optimized function value.\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if not np.array_equal(data_matrix, data_matrix.astype(bool)):\n",
        "        raise ValueError(\"Input data_matrix must be binary (0s and 1s).\")\n",
        "\n",
        "    # Flatten the binary matrix for optimization\n",
        "    n_rows, n_cols = data_matrix.shape\n",
        "    x = data_matrix.values.flatten()  # Convert to NumPy array and flatten\n",
        "\n",
        "    # Define the objective function (sum of squares minimization example)\n",
        "    def func(x):\n",
        "        # Reshape x back to the matrix to calculate the function value\n",
        "        mat = x.reshape(n_rows, n_cols)\n",
        "        return np.sum((mat - data_matrix) ** 2)  # Simple squared error\n",
        "\n",
        "    # Define the gradient of the objective function\n",
        "    def grad_func(x):\n",
        "        # Reshape x back to the matrix to calculate the gradient\n",
        "        mat = x.reshape(n_rows, n_cols)\n",
        "        return 2 * (mat - data_matrix.values).flatten()  # Gradient of squared error\n",
        "\n",
        "    # Storage for L-BFGS two-loop recursion\n",
        "    s_list = []\n",
        "    y_list = []\n",
        "    rho_list = []\n",
        "\n",
        "    # Initialize gradient and function value\n",
        "    f_val = func(x)\n",
        "    grad = grad_func(x)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        # Check convergence\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if grad_norm < tol:\n",
        "            print(f\"Converged in {iteration} iterations with grad_norm: {grad_norm:.5f}\")\n",
        "            break\n",
        "\n",
        "        # Two-loop recursion to compute search direction\n",
        "        q = grad.copy()\n",
        "        alpha = []\n",
        "        for i in range(len(s_list) - 1, -1, -1):\n",
        "            rho = rho_list[i]\n",
        "            alpha_i = rho * np.dot(s_list[i], q)\n",
        "            alpha.append(alpha_i)\n",
        "            q -= alpha_i * y_list[i]\n",
        "        if len(s_list) > 0:\n",
        "            H_0 = np.dot(y_list[-1], s_list[-1]) / np.dot(y_list[-1], y_list[-1])\n",
        "        else:\n",
        "            H_0 = 1.0\n",
        "        z = H_0 * q\n",
        "        for i in range(len(s_list)):\n",
        "            rho = rho_list[i]\n",
        "            beta = rho * np.dot(y_list[i], z)\n",
        "            z += s_list[i] * (alpha[i] - beta)\n",
        "\n",
        "        direction = -z\n",
        "\n",
        "        # Line search\n",
        "        step_size = 1.0\n",
        "        while True:\n",
        "            x_new = np.clip(x + step_size * direction, 0, 1).round()\n",
        "            f_new = func(x_new)\n",
        "            if f_new < f_val:\n",
        "                break\n",
        "            step_size *= 0.5\n",
        "\n",
        "        # Update the function value and gradient\n",
        "        x_new = x_new.round()  # Keep binary values\n",
        "        grad_new = grad_func(x_new)\n",
        "\n",
        "        # Update L-BFGS storage\n",
        "        s = x_new - x\n",
        "        y = grad_new - grad\n",
        "        if np.dot(s, y) > 1e-10:  # Ensure numerical stability\n",
        "            if len(s_list) >= m:\n",
        "                s_list.pop(0)\n",
        "                y_list.pop(0)\n",
        "                rho_list.pop(0)\n",
        "            s_list.append(s)\n",
        "            y_list.append(y)\n",
        "            rho_list.append(1.0 / np.dot(s, y))\n",
        "\n",
        "        # Update variables for next iteration\n",
        "        x = x_new\n",
        "        f_val = f_new\n",
        "        grad = grad_new\n",
        "\n",
        "    # Reshape weights back to the original matrix dimensions\n",
        "    weights = x.reshape(n_rows, n_cols).mean(axis=0)  # Average weight for each column (feature)\n",
        "\n",
        "    # Normalization parameter (bias term)\n",
        "    normalization_param = weights.sum() / n_cols\n",
        "\n",
        "    return weights, normalization_param\n"
      ],
      "metadata": {
        "id": "b9JUvcKvjzVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Categorize->Dice->intervals->missing->binarize->transform->filter"
      ],
      "metadata": {
        "id": "IyUaNRCkkg2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_column(dataset, dependent_column_name, num_intervals):\n",
        "    # Fill NaN values with the mean of the column\n",
        "    dataset_filled = dataset.fillna(dataset[dependent_column_name].mean())\n",
        "\n",
        "    # Extract the specified dependent column\n",
        "    value_ag_filled = dataset_filled[dependent_column_name]\n",
        "\n",
        "    # Define the target range (0-1)\n",
        "    target_range = (0, 1)\n",
        "\n",
        "    # Scale to the target range [0, 1] using np.interp\n",
        "    value_ag_scaled_filled = np.interp(value_ag_filled, (value_ag_filled.min(), value_ag_filled.max()), target_range)\n",
        "\n",
        "    # Create intervals and assign labels for scaled values\n",
        "    data_inter_filled = pd.cut(value_ag_scaled_filled, bins=num_intervals, labels=[chr(ord('A') + i) for i in range(num_intervals)])\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'Category': data_inter_filled,\n",
        "        dependent_column_name: value_ag_scaled_filled\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to your Temporary_SXmat directory where X_interval files are saved\n",
        "save_path = '/content/drive/MyDrive/Temporary_SXmat'\n",
        "\n",
        "# Load your frequent_patterns dataframe (replace with your actual frequent patterns dataframe)\n",
        "# For example, assuming `support_df` is loaded from a CSV\n",
        "# support_df = pd.read_csv('path_to_support_patterns.csv')\n",
        "\n",
        "def calculate_dice_similarity_demo(dataset_df, support_df):\n",
        "    \"\"\"\n",
        "    Calculate the Dice-Sørensen Coefficient (DSC) matrix for a given dataset and frequent patterns.\n",
        "\n",
        "    Args:\n",
        "        dataset_df (pd.DataFrame): The dataset dataframe.\n",
        "        support_df (pd.DataFrame): The frequent patterns dataframe (support patterns).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe containing the DSC matrix.\n",
        "    \"\"\"\n",
        "    unique_items = set()\n",
        "    for pattern in support_df.iloc[:, 0]:  # Assuming patterns are in the first column\n",
        "        pattern = str(pattern)\n",
        "        if pd.notna(pattern):\n",
        "            try:\n",
        "                items = pattern.split(', ')\n",
        "                unique_items.update(items)\n",
        "            except AttributeError:\n",
        "                print(f\"Skipping pattern {pattern} as it's not a valid string.\")\n",
        "\n",
        "    unique_item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
        "    bit_vector_length = len(unique_item_mapping)\n",
        "\n",
        "    # Convert dataset rows to bit vectors\n",
        "    def row_to_bit_vector(row, item_mapping, bit_length):\n",
        "        bit_vector = np.zeros(bit_length, dtype=int)\n",
        "        for col, value in row.items():\n",
        "            item = f\"{col}={value}\"\n",
        "            if item in item_mapping:\n",
        "                bit_vector[item_mapping[item]] = 1\n",
        "        return bit_vector\n",
        "\n",
        "    dataset_bit_vectors = np.array([row_to_bit_vector(row, unique_item_mapping, bit_vector_length) for _, row in dataset_df.iterrows()])\n",
        "\n",
        "    # Convert support patterns to bit vectors\n",
        "    def pattern_to_bit_vector(pattern, item_mapping, bit_length):\n",
        "        bit_vector = np.zeros(bit_length, dtype=int)\n",
        "        try:\n",
        "            pattern = str(pattern)\n",
        "            for item in pattern.split(', '):\n",
        "                if item in item_mapping:\n",
        "                    bit_vector[item_mapping[item]] = 1\n",
        "        except AttributeError:\n",
        "            print(f\"Skipping pattern {pattern} as it's not a valid string.\")\n",
        "        return bit_vector\n",
        "\n",
        "    support_pattern_bit_vectors = np.array([\n",
        "        pattern_to_bit_vector(pattern, unique_item_mapping, bit_vector_length)\n",
        "        for pattern in support_df.iloc[:, 0]  # Assuming patterns are in the first column\n",
        "        if pd.notna(pattern)\n",
        "    ])\n",
        "\n",
        "    # Initialize the DSC matrix with the correct shape\n",
        "    dsc_matrix = np.zeros((len(dataset_bit_vectors), len(support_pattern_bit_vectors)))\n",
        "\n",
        "    # Calculate DSC for each combination of dataset row and support pattern\n",
        "    for i, data_vector in enumerate(dataset_bit_vectors):\n",
        "        for j, pattern_vector in enumerate(support_pattern_bit_vectors):\n",
        "            intersection = np.sum(data_vector & pattern_vector)\n",
        "            union_cardinality = np.sum(data_vector) + np.sum(pattern_vector)\n",
        "            if union_cardinality > 0:\n",
        "                dsc_matrix[i, j] = (2 * intersection) / union_cardinality\n",
        "            else:\n",
        "                dsc_matrix[i, j] = 0\n",
        "\n",
        "    # Create a dataframe with the DSC matrix\n",
        "    dsc_df = pd.DataFrame(dsc_matrix, columns=[f'Pattern_{k}' for k in range(len(support_pattern_bit_vectors))])\n",
        "    return dsc_df\n",
        "\n",
        "def create_intervals_df(data_frame, column_name, num_intervals):\n",
        "    # Extract the specified column\n",
        "    value_ag = data_frame[column_name]\n",
        "\n",
        "    #print(\"Extracted column values:\")\n",
        "    #print(value_ag)\n",
        "\n",
        "    # Check if the column contains numeric data\n",
        "    if not pd.api.types.is_numeric_dtype(value_ag):\n",
        "        raise ValueError(f\"The '{column_name}' column must contain numeric data.\")\n",
        "\n",
        "    # Check for missing values in the column\n",
        "    if value_ag.isnull().any():\n",
        "        raise ValueError(f\"The '{column_name}' column contains missing values. Please handle them before processing.\")\n",
        "\n",
        "    # Check if scaling and clipping is necessary\n",
        "    if value_ag.min() < 0 or value_ag.max() > 1:\n",
        "        # Rescale to the range [0, 1]\n",
        "        value_ag_scaled = (value_ag - value_ag.min()) / (value_ag.max() - value_ag.min())\n",
        "\n",
        "        # Clip values to the range [0, 1]\n",
        "        value_ag_scaled_clipped = np.clip(value_ag_scaled, 0, 1)\n",
        "    else:\n",
        "        # No need to rescale or clip\n",
        "        value_ag_scaled_clipped = value_ag\n",
        "\n",
        "    #print(\"Scaled and clipped column values:\")\n",
        "    #print(value_ag_scaled_clipped)\n",
        "\n",
        "    # Calculate bin edges dynamically within the [0, 1] range\n",
        "    bin_edges = np.linspace(0, 1, num=num_intervals + 1)\n",
        "\n",
        "    # Create intervals and assign labels for clipped values\n",
        "    data_inter_clipped = pd.cut(value_ag_scaled_clipped.values, bins=bin_edges, labels=[chr(ord('A') + i) for i in range(len(bin_edges) - 1)])\n",
        "\n",
        "    #print(\"Computed intervals:\")\n",
        "    #print(data_inter_clipped)\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    intervals_df = pd.DataFrame({\n",
        "        'Category': data_inter_clipped.categories,\n",
        "        'Min_Value': [value_ag_scaled_clipped[data_inter_clipped == category].min() for category in data_inter_clipped.categories],\n",
        "        'Max_Value': [value_ag_scaled_clipped[data_inter_clipped == category].max() for category in data_inter_clipped.categories]\n",
        "    })\n",
        "\n",
        "    return intervals_df\n",
        "\n",
        "\n",
        "\n",
        "def handle_missing_values(dataset):\n",
        "    # Check if there are missing values in the dataset\n",
        "    if dataset.isnull().any().any():\n",
        "        # Identify numerical and categorical columns\n",
        "        numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
        "        categorical_cols = dataset.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "        # Impute numerical columns with mean\n",
        "        imputer_numeric = SimpleImputer(strategy='mean')\n",
        "        dataset[numerical_cols] = imputer_numeric.fit_transform(dataset[numerical_cols])\n",
        "\n",
        "        # Impute categorical columns with the most frequent value (mode)\n",
        "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "        dataset[categorical_cols] = imputer_categorical.fit_transform(dataset[categorical_cols])\n",
        "\n",
        "        return dataset\n",
        "    else:\n",
        "        # If no missing values, return the original dataset\n",
        "        return dataset\n",
        "\n",
        "def binarize_dataframe(input_df, num_bins):\n",
        "    df = pd.DataFrame(input_df)\n",
        "    df_binarized = pd.DataFrame()\n",
        "\n",
        "    for column in df.columns:\n",
        "        column_name = column+'_binarized'\n",
        "        bins = pd.qcut(df[column], q=num_bins, labels=False, duplicates='drop')\n",
        "        df_binarized[column_name] = (bins == bins.max()).astype(int)\n",
        "\n",
        "    return df_binarized\n",
        "\n",
        "def transform_matrix_dice(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "    print(transformed_matrix)\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.40] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.40] = 0\n",
        "    return transformed_matrix\n",
        "\n",
        "def transform_matrix_dice(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "    #print(\"Inside Transformed\")\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.40] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.40] = 0\n",
        "    return transformed_matrix\n",
        "\n",
        "\n",
        "def filter_data_for_interval(df, category_col, category):\n",
        "    # Filter data for a specific interval category\n",
        "    return df[df[category_col] == category]\n",
        "\n",
        "def transform_matrix_for_interval(X_interval, support_df):\n",
        "    jaccard_result = calculate_dice_similarity(X_interval, support_df)\n",
        "    #print(\"Inside Transform\")\n",
        "    #print(jaccard_result.shape)\n",
        "    transformed_matrix = transform_matrix_dice(jaccard_result)\n",
        "    print(transformed_matrix.shape)\n",
        "    return transformed_matrix"
      ],
      "metadata": {
        "id": "DHxtFgMmj9yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bayesian model for FP(with interestingness)"
      ],
      "metadata": {
        "id": "EHFStLBXk5nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "def find_separate_itemsets_for_measures(dataset_path, freq_pattern_path, drop_column=None, max_iterations=100, convergence_threshold=1e-4, thresholds=None):\n",
        "    # Load dataset and frequent patterns\n",
        "    dataset = pd.read_excel(dataset_path)\n",
        "    freq_patterns = pd.read_csv(freq_pattern_path, sep='\\t')\n",
        "\n",
        "    # Default thresholds if not provided\n",
        "    if thresholds is None:\n",
        "        thresholds = {\n",
        "            \"support\": 0.01,\n",
        "            \"confidence\": 0.000000000001,\n",
        "            \"lift\": 0.01,\n",
        "            \"leverage\": 0.0,\n",
        "            \"jaccard\": 0.5,\n",
        "            \"cosine\": 0.5\n",
        "        }\n",
        "\n",
        "    # Preprocess dataset - drop column if specified\n",
        "    if drop_column:\n",
        "        X = dataset.drop(columns=[drop_column])\n",
        "    else:\n",
        "        X = dataset  # If no column is specified to drop, use the full dataset\n",
        "\n",
        "    binary_data = Binarizer().fit_transform(X)\n",
        "\n",
        "    # Initialize itemset probabilities pi_s for each pattern\n",
        "    pi = np.ones(len(freq_patterns))  # Initialize all itemsets with probability 1 (uniform prior)\n",
        "\n",
        "    # Helper functions for Bayesian Mixture Model\n",
        "\n",
        "    def e_step(data, pi):\n",
        "        epsilon = 1e-10  # Small value to avoid log of zero\n",
        "        log_resp = np.zeros((data.shape[0], len(pi)))  # Initialize the responsibilities\n",
        "        for i, pattern in enumerate(freq_patterns['Itemsets']):\n",
        "            # Parse the pattern to get item columns\n",
        "            pattern_items = [item.split('=')[0] for item in pattern.split()]\n",
        "            pattern_columns = [col for col in X.columns if col in pattern_items]\n",
        "            if not pattern_columns:\n",
        "                continue  # Skip patterns that don't match any columns\n",
        "\n",
        "            # Indicator function: 1 if itemset is present, 0 if not\n",
        "            si = np.zeros(data.shape[0])\n",
        "            for j, row in enumerate(data):\n",
        "                si[j] = 1 if all(row[X.columns.get_loc(col)] for col in pattern_columns) else 0\n",
        "\n",
        "            # Calculate log-probability of the data under each itemset\n",
        "            log_prob = si * np.log(pi[i] + epsilon) + (1 - si) * np.log(1 - pi[i] + epsilon)\n",
        "            log_resp[:, i] = log_prob\n",
        "\n",
        "        # Normalize the responsibilities across all itemsets (log-sum-exp trick)\n",
        "        log_resp -= logsumexp(log_resp, axis=1)[:, np.newaxis]\n",
        "        return np.exp(log_resp)\n",
        "\n",
        "    def m_step(data, responsibilities):\n",
        "        N, D = data.shape\n",
        "        Nk = responsibilities.sum(axis=0)\n",
        "        pi_new = Nk / N  # Update component priors (probabilities of each itemset)\n",
        "        return pi_new\n",
        "\n",
        "    # Run EM algorithm until convergence or maximum iterations\n",
        "    for iteration in range(max_iterations):\n",
        "        prev_pi = pi.copy()\n",
        "\n",
        "        # Perform E-step and M-step\n",
        "        responsibilities = e_step(binary_data, pi)\n",
        "        pi = m_step(binary_data, responsibilities)\n",
        "\n",
        "        # Check for convergence\n",
        "        delta_pi = np.linalg.norm(pi - prev_pi)\n",
        "        if delta_pi < convergence_threshold:\n",
        "            print(f\"Converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "    # Initialize separate results for each interestingness measure\n",
        "    support_results = []\n",
        "    confidence_results = []\n",
        "    lift_results = []\n",
        "    leverage_results = []\n",
        "    jaccard_results = []\n",
        "    cosine_results = []\n",
        "\n",
        "    # Calculate interestingness measures for each pattern\n",
        "    for _, row in freq_patterns.iterrows():\n",
        "        pattern_string = row['Itemsets']\n",
        "\n",
        "        # Parse the pattern string into column names and values\n",
        "        pattern_items = [\n",
        "            item.strip().split('=')[0]  # Extract the attribute (column name)\n",
        "            for item in pattern_string.split()  # Split by spaces\n",
        "            if '=' in item  # Ignore parts without '=' (e.g., \"count=8\")\n",
        "        ]\n",
        "\n",
        "        # Match column names in the dataset\n",
        "        pattern_columns = [col for col in X.columns if col in pattern_items]\n",
        "        if not pattern_columns:\n",
        "            continue  # Skip patterns with no matching columns\n",
        "\n",
        "        # Calculate support\n",
        "        support_count = (binary_data[:, [X.columns.get_loc(col) for col in pattern_columns]].sum(axis=1)\n",
        "                         == len(pattern_columns)).sum()\n",
        "        support = support_count / binary_data.shape[0]\n",
        "\n",
        "        # Confidence (Assuming a simple rule A -> B)\n",
        "        if len(pattern_columns) > 1:\n",
        "            A, B = pattern_columns[:-1], pattern_columns[-1]\n",
        "            support_A = (binary_data[:, [X.columns.get_loc(col) for col in A]].sum(axis=1) == len(A)).sum() / binary_data.shape[0]\n",
        "            confidence = support / (support_A + 1e-10)\n",
        "        else:\n",
        "            confidence = np.nan\n",
        "\n",
        "        # Lift\n",
        "        if len(pattern_columns) > 1:\n",
        "            support_B = (binary_data[:, X.columns.get_loc(B)] == 1).sum() / binary_data.shape[0]\n",
        "            lift = support / (support_A * support_B + 1e-10)\n",
        "        else:\n",
        "            lift = np.nan\n",
        "\n",
        "        # Leverage\n",
        "        leverage = support - (support_A * support_B) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Jaccard Index\n",
        "        jaccard = support / (support_A + support_B - support) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Cosine Similarity\n",
        "        cosine = support / (support_A * support_B + 1e-10) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Store results if they exceed the threshold\n",
        "        if support >= thresholds['support']:\n",
        "            support_results.append({\"Pattern\": pattern_string, \"Support\": support})\n",
        "        if confidence >= thresholds['confidence']:\n",
        "            confidence_results.append({\"Pattern\": pattern_string, \"Confidence\": confidence})\n",
        "        if lift >= thresholds['lift']:\n",
        "            lift_results.append({\"Pattern\": pattern_string, \"Lift\": lift})\n",
        "        if leverage >= thresholds['leverage']:\n",
        "            leverage_results.append({\"Pattern\": pattern_string, \"Leverage\": leverage})\n",
        "        if jaccard >= thresholds['jaccard']:\n",
        "            jaccard_results.append({\"Pattern\": pattern_string, \"Jaccard\": jaccard})\n",
        "        if cosine >= thresholds['cosine']:\n",
        "            cosine_results.append({\"Pattern\": pattern_string, \"Cosine\": cosine})\n",
        "\n",
        "    # Convert results to DataFrames\n",
        "    support_df = pd.DataFrame(support_results)\n",
        "    confidence_df = pd.DataFrame(confidence_results)\n",
        "    lift_df = pd.DataFrame(lift_results)\n",
        "    leverage_df = pd.DataFrame(leverage_results)\n",
        "    jaccard_df = pd.DataFrame(jaccard_results)\n",
        "    cosine_df = pd.DataFrame(cosine_results)\n",
        "\n",
        "    return support_df, confidence_df, lift_df, leverage_df, jaccard_df, cosine_df"
      ],
      "metadata": {
        "id": "iOGOD8xpk1ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Density->process_interval"
      ],
      "metadata": {
        "id": "_57fOy5snJ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_density(matrix):\n",
        "    num_ones = matrix.nnz if isinstance(matrix, csr_matrix) else np.count_nonzero(matrix)\n",
        "    total_elements = matrix.shape[0] * matrix.shape[1]\n",
        "    density = num_ones / total_elements\n",
        "    return density\n",
        "\n",
        "# def process_interval(row):\n",
        "#     category = row['Category']\n",
        "#     interval_min = row['Min_Value']\n",
        "#     interval_max = row['Max_Value']\n",
        "\n",
        "#     filtered_data = filter_data_for_interval(Data_discretized, 'Category', category)\n",
        "#     if not filtered_data.empty:\n",
        "#         X_interval = filtered_data.drop(columns=['Category'])\n",
        "#         print(X_interval.shape)\n",
        "#         val = calculate_dice_similarity(X_interval, support_df)\n",
        "#         #print(val.shape)\n",
        "#         val1 = transform_matrix_for_interval(X_interval, support_df)\n",
        "#         SXmat = val1\n",
        "#         SXmat = pd.DataFrame(SXmat)\n",
        "#         # Calculate Density\n",
        "#         density = calculate_density(SXmat)\n",
        "#         print(f\"Density of incidence matrix for this interval (Category {category}): {density}\")\n",
        "#         weights, w_norm = fis_algorithm_with_auxiliary(SXmat)\n",
        "#         return category, {'weights': weights, 'wnorm': w_norm}\n",
        "#     else:\n",
        "#         print(f\"No data for interval {category}\")\n",
        "#         return category, None\n",
        "\n",
        "def process_interval(row):\n",
        "    category = row['Category']\n",
        "    interval_min = row['Min_Value']\n",
        "    interval_max = row['Max_Value']\n",
        "\n",
        "    filtered_data = filter_data_for_interval(Data_discretized, 'Category', category)\n",
        "    if not filtered_data.empty:\n",
        "        X_interval = filtered_data.drop(columns=['Category'])\n",
        "        print(X_interval.shape)\n",
        "        val = calculate_dice_similarity(X_interval, support_df)\n",
        "        print(val.shape)\n",
        "        val1 = transform_matrix_for_interval(X_interval, support_df)\n",
        "        SXmat = val1\n",
        "        SXmat = pd.DataFrame(SXmat)\n",
        "        print(SXmat.head(1))\n",
        "        df_SXmat = pd.DataFrame(SXmat)\n",
        "        sxmat_file_path = os.path.join(save_path, f'SXmat_Category_{category}.csv')\n",
        "        df_SXmat.to_csv(sxmat_file_path, index=False)\n",
        "        print(f\"SXmat for Category {category} saved at {sxmat_file_path}\")\n",
        "        # Calculate Density\n",
        "        density = calculate_density(SXmat)\n",
        "        print(f\"Density of incidence matrix for this interval (Category {category}): {density}\")\n",
        "        weights, w_norm = lbfgs_estimate_weights(SXmat)\n",
        "        print(weights.shape)\n",
        "        #print(wnorm.shape)\n",
        "        return category, {'weights': weights, 'wnorm': w_norm}\n",
        "    else:\n",
        "        print(f\"No data for interval {category}\")\n",
        "        return category, None\n",
        "\n"
      ],
      "metadata": {
        "id": "FA_HMh41lvL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process_test_dataset"
      ],
      "metadata": {
        "id": "QHhgP_pDvL_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_data(Data_test, support_df, categories_weights, constant):\n",
        "    # Create a temporary DataFrame as a copy of Data_test\n",
        "    Data_test_temp = Data_test.copy()\n",
        "\n",
        "    # Iterate through each row in Data_test_temp\n",
        "    for index, row in Data_test_temp.iterrows():\n",
        "        # Convert the row to a DataFrame for compatibility with transform_matrix_for_interval\n",
        "        row_df = pd.DataFrame([row])\n",
        "\n",
        "        # Calculate the Jaccard/Dice similarity for the current row with support_df\n",
        "        jaccard_result = calculate_dice_similarity_demo(row_df, support_df)\n",
        "\n",
        "        # Directly apply binarization with a threshold of 0.4\n",
        "        transformed_row = jaccard_result.copy()\n",
        "        transformed_row[transformed_row > 0.4] = 1\n",
        "        transformed_row[transformed_row <= 0.4] = 0\n",
        "\n",
        "        # Flatten the transformed_row to ensure it is 1D (1, 87) -> (87,)\n",
        "        transformed_row_flat = transformed_row.values.flatten()\n",
        "\n",
        "        max_value = 0.0  # Initialize the max value\n",
        "        predicted_category = None  # Store the category with the highest value\n",
        "\n",
        "        # Iterate over each category and calculate the weighted product\n",
        "        for category, params in categories_weights.items():\n",
        "            # Ensure the structure is valid\n",
        "            if 'weights' in params and 'wnorm' in params:\n",
        "                weights = params['weights']  # Extract weights for the category\n",
        "                wnorm = params['wnorm']      # Extract normalization value\n",
        "\n",
        "                # Reshape weights to (1, 87) to match the transformed_row shape (1, 87)\n",
        "                weights_reshaped = weights.reshape(1, -1)\n",
        "\n",
        "                # Ensure both weights and transformed_row are compatible (both should be (1, 87))\n",
        "                if weights_reshaped.shape != transformed_row.shape:\n",
        "                    print(f\"Shape mismatch between weights and transformed_row for category {category}\")\n",
        "                    continue\n",
        "\n",
        "                product = weights_reshaped * transformed_row  # Element-wise multiplication\n",
        "                # Sum the product over all columns (axis=1 sums across columns)\n",
        "                product_sum = product.sum(axis=1)  # This will give a single value\n",
        "                # Calculate the final product with the normalization value and constant\n",
        "                final_product = (product_sum + wnorm) * constant\n",
        "                final_product = final_product.item()\n",
        "                # Check if the current product is higher than max_value\n",
        "                if final_product > max_value:\n",
        "                    max_value = final_product\n",
        "                    predicted_category = category  # Store the category with the highest product\n",
        "\n",
        "        # After processing all categories for the current row, add the predicted category to Data_test_temp\n",
        "        Data_test_temp.loc[index, 'predicted_category'] = predicted_category\n",
        "\n",
        "    # Return the updated DataFrame with the predicted category column\n",
        "    return Data_test_temp"
      ],
      "metadata": {
        "id": "jZBVMhOYvFI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RMSE calculation"
      ],
      "metadata": {
        "id": "Q5Dl1zcjvsZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_rmse_category(X_test, intervals_df, updated_data):\n",
        "    sum_squared_diff = 0.0\n",
        "    sum_absolute_diff = 0.0\n",
        "    actual_values = []\n",
        "    predicted_values = []\n",
        "\n",
        "    # Iterate through the test dataset to calculate RMSE, MAE, and collect values for PCC\n",
        "    for i, row in X_test.iterrows():\n",
        "        actual_category = row['Category']  # Actual category\n",
        "        predicted_category = updated_data.loc[i, 'predicted_category']  # Predicted category from updated_data\n",
        "        #print(f\"Predicted Category: {predicted_category}\")\n",
        "\n",
        "        # Get the actual 'Max_Value' based on the actual category\n",
        "        actual_max_value = intervals_df[intervals_df['Category'] == actual_category]['Max_Value'].values\n",
        "\n",
        "        # Check if predicted_category exists in intervals_df\n",
        "        if predicted_category in intervals_df['Category'].values:\n",
        "            predicted_max_value = intervals_df[intervals_df['Category'] == predicted_category]['Max_Value'].values\n",
        "        else:\n",
        "            # Handle case where predicted category is not found in intervals_df\n",
        "            #print(f\"Predicted category '{predicted_category}' not found in intervals_df.\")\n",
        "            predicted_max_value = np.nan  # Set predicted_max_value to NaN or handle it as needed\n",
        "\n",
        "        #print(f\"Actual Max Value: {actual_max_value}, Predicted Max Value: {predicted_max_value}\")\n",
        "\n",
        "        # Ensure the actual max value exists and predicted max value is not NaN\n",
        "        if len(actual_max_value) > 0:\n",
        "            actual_max_value = actual_max_value[0]\n",
        "\n",
        "            # Handle NaN values in actual_max_value or predicted_max_value\n",
        "            if np.isnan(actual_max_value) or np.isnan(predicted_max_value):\n",
        "                #print(f\"Skipping pair due to NaN value (Actual: {actual_max_value}, Predicted: {predicted_max_value})\")\n",
        "\n",
        "                # Option 1: Set difference to 0 if both are NaN\n",
        "                squared_diff = 0\n",
        "                absolute_diff = 0\n",
        "            else:\n",
        "                # Calculate squared difference for RMSE\n",
        "                squared_diff = (predicted_max_value - actual_max_value) ** 2\n",
        "                absolute_diff = abs(predicted_max_value - actual_max_value)\n",
        "\n",
        "            sum_squared_diff += squared_diff\n",
        "            sum_absolute_diff += absolute_diff\n",
        "\n",
        "            # Collect values for PCC calculation\n",
        "            actual_values.append(actual_max_value)\n",
        "            predicted_values.append(predicted_max_value)\n",
        "\n",
        "    # Ensure we have data to calculate the metrics\n",
        "    if len(X_test) > 0:\n",
        "        # Calculate RMSE\n",
        "        mean_squared_error = sum_squared_diff / len(X_test)\n",
        "        rmse = np.sqrt(mean_squared_error).item()  # Convert numpy array to scalar\n",
        "\n",
        "        # Calculate MAE\n",
        "        mae = (sum_absolute_diff / len(X_test)).item()  # Convert numpy array to scalar\n",
        "\n",
        "        return rmse, mae\n",
        "\n",
        "    # Return None if there are no rows in X_test\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "jYsZvnKJvrZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check dataset"
      ],
      "metadata": {
        "id": "hKKKLS_xBUq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load Dataset while skipping the 'datetime' column\n",
        "dataset = \"/content/drive/MyDrive/houses_new.xlsx\"\n",
        "concrete = pd.read_excel(dataset)\n",
        "concrete.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "HyIK3REIBXP9",
        "outputId": "ccdeb1f1-09b0-46e3-fd68-e528023c2010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   INTERCEPT  MEDIAN INCOME  MEDIAN INCOME2  MEDIAN INCOME3  ln(MEDIAN AGE)  \\\n",
              "0     452600         8.3252              41             880             129   \n",
              "1     358500         8.3014              21            7099            1106   \n",
              "\n",
              "   ln(TOTAL ROOMS/ POPULATION)  ln(BEDROOMS/ POPULATION)  \\\n",
              "0                          322                       126   \n",
              "1                         2401                      1138   \n",
              "\n",
              "   ln(POPULATION/ HOUSEHOLDS)  ln(HOUSEHOLDS)  \n",
              "0                       37.88         -122.23  \n",
              "1                       37.86         -122.22  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33bd8957-c467-4c47-a0b0-2941e2104739\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INTERCEPT</th>\n",
              "      <th>MEDIAN INCOME</th>\n",
              "      <th>MEDIAN INCOME2</th>\n",
              "      <th>MEDIAN INCOME3</th>\n",
              "      <th>ln(MEDIAN AGE)</th>\n",
              "      <th>ln(TOTAL ROOMS/ POPULATION)</th>\n",
              "      <th>ln(BEDROOMS/ POPULATION)</th>\n",
              "      <th>ln(POPULATION/ HOUSEHOLDS)</th>\n",
              "      <th>ln(HOUSEHOLDS)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>452600</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>41</td>\n",
              "      <td>880</td>\n",
              "      <td>129</td>\n",
              "      <td>322</td>\n",
              "      <td>126</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>358500</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>21</td>\n",
              "      <td>7099</td>\n",
              "      <td>1106</td>\n",
              "      <td>2401</td>\n",
              "      <td>1138</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33bd8957-c467-4c47-a0b0-2941e2104739')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33bd8957-c467-4c47-a0b0-2941e2104739 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33bd8957-c467-4c47-a0b0-2941e2104739');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-603f5830-324d-4a5a-9646-7b86c5614d6a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-603f5830-324d-4a5a-9646-7b86c5614d6a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-603f5830-324d-4a5a-9646-7b86c5614d6a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "concrete",
              "summary": "{\n  \"name\": \"concrete\",\n  \"rows\": 20640,\n  \"fields\": [\n    {\n      \"column\": \"INTERCEPT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115395,\n        \"min\": 14999,\n        \"max\": 500001,\n        \"num_unique_values\": 3842,\n        \"samples\": [\n          194300,\n          379000,\n          230100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEDIAN INCOME\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8998217179452728,\n        \"min\": 0.4999,\n        \"max\": 15.0001,\n        \"num_unique_values\": 12928,\n        \"samples\": [\n          5.0286,\n          2.04329999999999,\n          6.12279999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEDIAN INCOME2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 52,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          35,\n          25,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEDIAN INCOME3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2181,\n        \"min\": 2,\n        \"max\": 39320,\n        \"num_unique_values\": 5926,\n        \"samples\": [\n          699,\n          1544,\n          3966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ln(MEDIAN AGE)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 421,\n        \"min\": 1,\n        \"max\": 6445,\n        \"num_unique_values\": 1928,\n        \"samples\": [\n          1045,\n          2728,\n          2118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ln(TOTAL ROOMS/ POPULATION)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1132,\n        \"min\": 3,\n        \"max\": 35682,\n        \"num_unique_values\": 3888,\n        \"samples\": [\n          4169,\n          636,\n          3367\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ln(BEDROOMS/ POPULATION)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 382,\n        \"min\": 1,\n        \"max\": 6082,\n        \"num_unique_values\": 1815,\n        \"samples\": [\n          21,\n          750,\n          1447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ln(POPULATION/ HOUSEHOLDS)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1359523974571393,\n        \"min\": 32.5399999999999,\n        \"max\": 41.95,\n        \"num_unique_values\": 862,\n        \"samples\": [\n          33.7,\n          34.4099999999999,\n          38.24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ln(HOUSEHOLDS)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.0035317235025762,\n        \"min\": -124.349999999999,\n        \"max\": -114.31,\n        \"num_unique_values\": 844,\n        \"samples\": [\n          -118.63,\n          -119.86,\n          -121.26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calling the function with dataset input"
      ],
      "metadata": {
        "id": "1mCv4_NMnWvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load Dataset while skipping the 'datetime' column\n",
        "dataset = \"/content/drive/MyDrive/houses_new.xlsx\"\n",
        "concrete = pd.read_excel(dataset)\n",
        "dependent = \"ln(HOUSEHOLDS)\"\n",
        "concrete_temp = concrete.copy()\n",
        "X = concrete_temp.drop(dependent, axis=1).copy()\n",
        "y = concrete[dependent]\n",
        "# Usage example:\n",
        "dataset_path = \"/content/drive/MyDrive/houses_new.xlsx\"\n",
        "freq_pattern_path = '/content/drive/MyDrive/AprioriOutput/closed_itemsets_houses.csv'\n",
        "drop_column = 'ln(HOUSEHOLDS)'  # Example of the column to drop\n",
        "support_df, confidence_df, lift_df, leverage_df, jaccard_df, cosine_df = find_separate_itemsets_for_measures(dataset_path, freq_pattern_path, drop_column=drop_column)\n",
        "support_df = pd.read_csv(freq_pattern_path,sep='\\t')\n",
        "#support_df = support_df.drop(columns=['Support'])\n",
        "# Handle Missing Values\n",
        "X = handle_missing_values(X)\n",
        "# Binarize DataFrame\n",
        "num_bins = 15\n",
        "df_binarized = binarize_dataframe(pd.DataFrame(concrete), num_bins)\n",
        "# Create Intervals DataFrame\n",
        "num_intervals = 4\n",
        "intervals_df = create_intervals_df(concrete, dependent, num_intervals)\n",
        "print(intervals_df)\n",
        "# Categorize Column\n",
        "result_df = categorize_column(concrete, dependent, num_intervals)\n",
        "P_c = 1 / num_intervals\n",
        "P_x = 1 / concrete.shape[1]\n",
        "constant = P_c / P_x\n",
        "Data_discretized = pd.read_csv('/content/drive/MyDrive/Discretized_datasets/discretized_houses.csv')\n",
        "Data_discretized['Category'] = result_df['Category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhhBhVfcnOzF",
        "outputId": "0226a67a-a7a2-4098-f62e-a2ab29cc983e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 4 iterations.\n",
            "  Category  Min_Value  Max_Value\n",
            "0        A   0.004980   0.250000\n",
            "1        B   0.250996   0.500000\n",
            "2        C   0.500996   0.749004\n",
            "3        D   0.750000   1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Jaccard Similarity and Train GIS for Each Interval\n",
        "categories_weights = {}\n",
        "categories = Data_discretized['Category'].unique()\n",
        "# Use ThreadPoolExecutor to run computations in parallel\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Create a list of futures\n",
        "    futures = {executor.submit(process_interval, row): row for _, row in intervals_df.iterrows()}\n",
        "    # Process completed futures\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        category, weights_wnorm = future.result()\n",
        "        if weights_wnorm is not None:\n",
        "            categories_weights[category] = weights_wnorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2w61PLCnOlK",
        "outputId": "9206b576-c407-488c-d37c-c0bc0ad4fe81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4971, 8)\n",
            "(4123, 8)\n",
            "(359, 8)\n",
            "(11187, 8)\n",
            "(359, 377)\n",
            "(359, 377)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_367  Pattern_368  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          1.0   \n",
            "\n",
            "   Pattern_369  Pattern_370  Pattern_371  Pattern_372  Pattern_373  \\\n",
            "0          1.0          1.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_374  Pattern_375  Pattern_376  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 377 columns]\n",
            "SXmat for Category D saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_D.csv\n",
            "Density of incidence matrix for this interval (Category D): 0.5310802922943928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 0 iterations with grad_norm: 0.00000\n",
            "(377,)\n",
            "(4123, 377)\n",
            "(4971, 377)\n",
            "(4123, 377)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_367  Pattern_368  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          1.0   \n",
            "\n",
            "   Pattern_369  Pattern_370  Pattern_371  Pattern_372  Pattern_373  \\\n",
            "0          1.0          1.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_374  Pattern_375  Pattern_376  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 377 columns]\n",
            "SXmat for Category B saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_B.csv\n",
            "Density of incidence matrix for this interval (Category B): 0.5287058237705156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 0 iterations with grad_norm: 0.00000\n",
            "(377,)\n",
            "(4971, 377)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_367  Pattern_368  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          1.0   \n",
            "\n",
            "   Pattern_369  Pattern_370  Pattern_371  Pattern_372  Pattern_373  \\\n",
            "0          1.0          1.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_374  Pattern_375  Pattern_376  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 377 columns]\n",
            "SXmat for Category A saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_A.csv\n",
            "Density of incidence matrix for this interval (Category A): 0.5352337990050515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 0 iterations with grad_norm: 0.00000\n",
            "(377,)\n",
            "(11187, 377)\n",
            "(11187, 377)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_367  Pattern_368  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          1.0   \n",
            "\n",
            "   Pattern_369  Pattern_370  Pattern_371  Pattern_372  Pattern_373  \\\n",
            "0          1.0          1.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_374  Pattern_375  Pattern_376  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 377 columns]\n",
            "SXmat for Category C saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_C.csv\n",
            "Density of incidence matrix for this interval (Category C): 0.5697665844141279\n",
            "Converged in 0 iterations with grad_norm: 0.00000\n",
            "(377,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing datasets for model evaluation\n",
        "Data_train, Data_test = train_test_split(Data_discretized, test_size=0.2, random_state=5)\n",
        "# Example of calling the function\n",
        "updated_data = process_test_data(Data_test, support_df, categories_weights, constant)\n",
        "# Calculate the root mean squared error (RMSE) for the test data\n",
        "rmse = calculate_rmse_category(Data_test,intervals_df,updated_data)  # Use a custom RMSE calculation function\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl-JQrervTvG",
        "outputId": "a8594307-d900-4624-9054-d6751e44ff16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: (0.1591190820120225, 0.07711330067328334)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqvO8nzdvii2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def find_best_rmse_random_state(Data_discretized, intervals_df, support_df, categories_weights, constant):\n",
        "    best_random_state = None\n",
        "    min_rmse = float('inf')  # Initialize with a large value to ensure any RMSE is smaller\n",
        "\n",
        "    # Iterate through random states from 5 to 200\n",
        "    for random_state in range(5, 201):\n",
        "        # Split the data into training and testing datasets\n",
        "        Data_train, Data_test = train_test_split(Data_discretized, test_size=0.2, random_state=random_state)\n",
        "\n",
        "        # Process the test data\n",
        "        updated_data = process_test_data(Data_test, support_df, categories_weights, constant)\n",
        "\n",
        "        # Calculate the root mean squared error (RMSE) and MAE\n",
        "        rmse, mae = calculate_rmse_category(Data_test, intervals_df, updated_data)\n",
        "\n",
        "        # Compare only the first value (RMSE) from the tuple\n",
        "        if rmse is not None and rmse < min_rmse:\n",
        "            min_rmse = rmse\n",
        "            min_mae = mae\n",
        "            best_random_state = random_state\n",
        "\n",
        "    return best_random_state, min_rmse,min_mae\n",
        "\n",
        "# Example usage\n",
        "best_random_state, min_rmse,min_mae = find_best_rmse_random_state(Data_discretized, intervals_df, support_df, categories_weights, constant)\n",
        "print(f\"Best Random State: {best_random_state}, RMSE: {min_rmse,min_mae}\")\n"
      ],
      "metadata": {
        "id": "mHvIo0GmO9Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5xYsi_4EEAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7J2lB7MO-Bk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}