{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = input(\"Enter the path to the dataset CSV file: \")\n",
        "concrete = pd.read_csv(dataset)\n",
        "dependent = input(\"Enter the name of the dependent column: \")\n",
        "concrete_temp = concrete.copy()\n",
        "X = concrete_temp.drop(dependent, axis=1).copy()\n",
        "# Now, try accessing the column again\n",
        "y = concrete[dependent]\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "frequent_itemset_file_path = input(\"Enter the path to the frequent itemset CSV file: \")\n",
        "# Define your column names\n",
        "column_names = ['Frequent_Itemsets_CHARM']\n",
        "# Read the CSV file into a DataFrame with specified column names\n",
        "freq_item = pd.read_csv(frequent_itemset_file_path, names=column_names)\n",
        "one_hot_encoded = pd.get_dummies(freq_item['Frequent_Itemsets_CHARM'].str.split(expand=True), prefix='', prefix_sep='')\n",
        "from sklearn.impute import SimpleImputer\n",
        "def handle_missing_values(dataset):\n",
        "    # Check if there are missing values in the dataset\n",
        "    if dataset.isnull().any().any():\n",
        "        # Identify numerical and categorical columns\n",
        "        numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
        "        categorical_cols = dataset.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "        # Impute numerical columns with mean\n",
        "        imputer_numeric = SimpleImputer(strategy='mean')\n",
        "        dataset[numerical_cols] = imputer_numeric.fit_transform(dataset[numerical_cols])\n",
        "\n",
        "        # Impute categorical columns with the most frequent value (mode)\n",
        "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "        dataset[categorical_cols] = imputer_categorical.fit_transform(dataset[categorical_cols])\n",
        "\n",
        "        return dataset\n",
        "    else:\n",
        "        # If no missing values, return the original dataset\n",
        "        return dataset\n",
        "X = handle_missing_values(X)\n",
        "def binarize_dataframe(input_df, num_bins):\n",
        "    df = pd.DataFrame(input_df)\n",
        "    df_binarized = pd.DataFrame()\n",
        "\n",
        "    for column in df.columns:\n",
        "        column_name = column+'_binarized'\n",
        "        bins = pd.qcut(df[column], q=num_bins, labels=False, duplicates='drop')\n",
        "        df_binarized[column_name] = (bins == bins.max()).astype(int)\n",
        "\n",
        "    return df_binarized\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def create_intervals_df(data_frame, column_name, num_intervals):\n",
        "    # Extract the specified column\n",
        "    value_ag = data_frame[column_name]\n",
        "\n",
        "    #print(\"Extracted column values:\")\n",
        "    #print(value_ag)\n",
        "\n",
        "    # Check if the column contains numeric data\n",
        "    if not pd.api.types.is_numeric_dtype(value_ag):\n",
        "        raise ValueError(f\"The '{column_name}' column must contain numeric data.\")\n",
        "\n",
        "    # Check for missing values in the column\n",
        "    if value_ag.isnull().any():\n",
        "        raise ValueError(f\"The '{column_name}' column contains missing values. Please handle them before processing.\")\n",
        "\n",
        "    # Check if scaling and clipping is necessary\n",
        "    if value_ag.min() < 0 or value_ag.max() > 1:\n",
        "        # Rescale to the range [0, 1]\n",
        "        value_ag_scaled = (value_ag - value_ag.min()) / (value_ag.max() - value_ag.min())\n",
        "\n",
        "        # Clip values to the range [0, 1]\n",
        "        value_ag_scaled_clipped = np.clip(value_ag_scaled, 0, 1)\n",
        "    else:\n",
        "        # No need to rescale or clip\n",
        "        value_ag_scaled_clipped = value_ag\n",
        "\n",
        "    #print(\"Scaled and clipped column values:\")\n",
        "    #print(value_ag_scaled_clipped)\n",
        "\n",
        "    # Calculate bin edges dynamically within the [0, 1] range\n",
        "    bin_edges = np.linspace(0, 1, num=num_intervals + 1)\n",
        "\n",
        "    # Create intervals and assign labels for clipped values\n",
        "    data_inter_clipped = pd.cut(value_ag_scaled_clipped.values, bins=bin_edges, labels=[chr(ord('A') + i) for i in range(len(bin_edges) - 1)])\n",
        "\n",
        "    #print(\"Computed intervals:\")\n",
        "    #print(data_inter_clipped)\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    intervals_df = pd.DataFrame({\n",
        "        'Category': data_inter_clipped.categories,\n",
        "        'Min_Value': [value_ag_scaled_clipped[data_inter_clipped == category].min() for category in data_inter_clipped.categories],\n",
        "        'Max_Value': [value_ag_scaled_clipped[data_inter_clipped == category].max() for category in data_inter_clipped.categories]\n",
        "    })\n",
        "\n",
        "    return intervals_df\n",
        "num_intervals = 10\n",
        "dependent_column_name = dependent\n",
        "intervals_df = create_intervals_df(concrete, dependent_column_name, num_intervals)\n",
        "def categorize_column(dataset, dependent_column_name, num_intervals):\n",
        "    # Fill NaN values with the mean of the column\n",
        "    dataset_filled = dataset.fillna(dataset[dependent_column_name].mean())\n",
        "\n",
        "    # Extract the specified dependent column\n",
        "    value_ag_filled = dataset_filled[dependent_column_name]\n",
        "\n",
        "    # Define the target range (0-1)\n",
        "    target_range = (0, 1)\n",
        "\n",
        "    # Scale to the target range [0, 1] using np.interp\n",
        "    value_ag_scaled_filled = np.interp(value_ag_filled, (value_ag_filled.min(), value_ag_filled.max()), target_range)\n",
        "\n",
        "    # Create intervals and assign labels for scaled values\n",
        "    data_inter_filled = pd.cut(value_ag_scaled_filled, bins=num_intervals, labels=[chr(ord('A') + i) for i in range(num_intervals)])\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'Category': data_inter_filled,\n",
        "        dependent_column_name: value_ag_scaled_filled\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "num_intervals = 10\n",
        "result_df = categorize_column(concrete, dependent, num_intervals)\n",
        "#Data Binary\n",
        "df = pd.DataFrame(concrete)\n",
        "num_bins = 15\n",
        "# Use the previously defined binarize_dataframe function\n",
        "df_binarized = binarize_dataframe(df, num_bins)\n",
        "dependent_column_name = dependent\n",
        "# Rename columns and create 'Data_binary'\n",
        "df_binarized.columns = df_binarized.columns.str.replace('_binarized', '')\n",
        "data_incidence_input = df_binarized.drop(columns=[dependent_column_name])\n",
        "Data_binary = data_incidence_input.copy()\n",
        "Data_binary['Category'] = result_df['Category']\n",
        "Data_binary[dependent_column_name] = df_binarized[dependent_column_name]\n",
        "#Jaccard similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_jaccard_similarity(Data_binary, one_hot_encoded):\n",
        "    # Number of rows in the Data_binary DataFrame\n",
        "    num_rows_binary = Data_binary.shape[0]\n",
        "\n",
        "    # Number of rows in the one_hot_encoded DataFrame\n",
        "    num_rows_encoded = one_hot_encoded.shape[0]\n",
        "\n",
        "    # Number of columns in the one_hot_encoded DataFrame\n",
        "    num_cols_encoded = one_hot_encoded.shape[1]\n",
        "\n",
        "    # Creating a matrix to store Jaccard similarities\n",
        "    result_matrix = np.zeros((num_rows_binary, num_rows_encoded))\n",
        "\n",
        "    # Calculating Jaccard similarity for each cell (pair of row in Data_binary and row in one_hot_encoded)\n",
        "    for i in range(num_rows_binary):\n",
        "        for j in range(num_rows_encoded):\n",
        "            set_row_binary = set(Data_binary.iloc[i, 1:])  # Exclude the 'Category' column\n",
        "            set_row_encoded = set(one_hot_encoded.iloc[j, :])\n",
        "            intersection_size = len(set_row_binary.intersection(set_row_encoded))\n",
        "            union_size = len(set_row_binary.union(set_row_encoded))\n",
        "            similarity = intersection_size / union_size if union_size != 0 else 0\n",
        "            result_matrix[i, j] = similarity\n",
        "\n",
        "    # Convert the result matrix to a DataFrame\n",
        "    result_df = pd.DataFrame(result_matrix, columns=one_hot_encoded.index)\n",
        "\n",
        "    return result_df\n",
        "jaccard_result = calculate_jaccard_similarity(Data_binary, one_hot_encoded)\n",
        "#Transform Matrix\n",
        "def transform_matrix(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.50] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.50] = 0\n",
        "    return transformed_matrix\n",
        "transformed_matrix = transform_matrix(jaccard_result)\n",
        "from scipy.sparse import csr_matrix\n",
        "SXmat = csr_matrix(transformed_matrix)\n",
        "from scipy.sparse import dok_matrix\n",
        "def train_GIS_single_label(SX):\n",
        "    \"\"\"\n",
        "    Trains the GIS algorithm for parameter estimation based on the sparse incidence matrix SX.\n",
        "    \"\"\"\n",
        "    # Calculate the observed supports\n",
        "    OS = np.array(SX.sum(axis=0)).flatten()\n",
        "\n",
        "    # Initialize the weights and expected supports\n",
        "    w = np.zeros(SX.shape[1], dtype=np.float32)\n",
        "    ES = dok_matrix((1, SX.shape[1]), dtype=np.float32)\n",
        "\n",
        "    # Iterate until convergence\n",
        "    loop = True\n",
        "    while loop:\n",
        "        # Calculate the probabilities\n",
        "        P = np.exp(SX.dot(w))\n",
        "\n",
        "        # Update the expected supports\n",
        "        ES = dok_matrix((1, SX.shape[1]), dtype=np.float32)\n",
        "        for i in range(SX.shape[0]):\n",
        "            P[i] /= np.sum(P)  # Normalize the probabilities\n",
        "            ES += P[i] * SX[i]\n",
        "\n",
        "        # Update the weights\n",
        "        wold = w.copy()\n",
        "        w += np.log(OS) - np.log(ES.toarray().flatten())  # Convert ES to a 1D array for element-wise operations\n",
        "        wnorm = np.sum(np.exp(w))\n",
        "        w /= wnorm  # Normalize the weights\n",
        "        wprecision = np.max(np.abs(w - wold))\n",
        "        if wprecision < 0.01:\n",
        "            loop = False\n",
        "\n",
        "    return w, wnorm\n",
        "#Weights and wnorm for category\n",
        "def calculate_weights_and_wnorm_for_category(category, work1_filled, one_hot_encoded):\n",
        "    # Calculate Jaccard similarity\n",
        "    jaccard_similarity_matrix = calculate_jaccard_similarity(Data_binary, one_hot_encoded)\n",
        "\n",
        "    # Transform the Jaccard similarity matrix\n",
        "    transformed_matrix = transform_matrix(jaccard_similarity_matrix)\n",
        "\n",
        "    # Convert data to a scipy sparse format\n",
        "    SXmat = csr_matrix(transformed_matrix)\n",
        "\n",
        "    # Train GIS for the category using the transformed matrix\n",
        "    w, wnorm = train_GIS_single_label(SXmat)\n",
        "\n",
        "    return w, wnorm\n",
        "\n",
        "# Iterate over categories in the 'Category' column of Work1_filled\n",
        "categories_weights = {}\n",
        "\n",
        "# Assuming 'Category' is the column containing category labels in 'Work1_filled'\n",
        "categories = Data_binary['Category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    # Calculate weights and wnorm for the current category\n",
        "    weights, wnorm = calculate_weights_and_wnorm_for_category(category, Data_binary, one_hot_encoded)\n",
        "\n",
        "    # Store the weights and wnorm for the category\n",
        "    categories_weights[category] = {'weights': weights, 'wnorm': wnorm}\n",
        "def calculate_x_by_c(X, one_hot_encoded, category_weights):\n",
        "    \"\"\"\n",
        "    Calculate x_by_c for each row in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - X: DataFrame containing the dataset.\n",
        "    - one_hot_encoded: DataFrame containing the one-hot-encoded matrix.\n",
        "    - category_weights: Dictionary containing weights and wnorm for each category.\n",
        "\n",
        "    Returns:\n",
        "    - x_by_c: DataFrame with x_by_c values for each row.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate Jaccard similarity matrix for the entire dataset once\n",
        "    jaccard_result = calculate_jaccard_similarity(X, one_hot_encoded)\n",
        "\n",
        "    # Create an empty DataFrame to store x_by_c values\n",
        "    x_by_c = pd.DataFrame(index=X.index)\n",
        "\n",
        "    # Iterate over each row in the dataset\n",
        "    for index, row in X.iterrows():\n",
        "        try:\n",
        "            # Extract the category for the current row\n",
        "            category = row['Category']\n",
        "\n",
        "            # Extract only the row corresponding to the current row in X\n",
        "            jaccard_row = jaccard_result.loc[index]\n",
        "\n",
        "            # Transform the row to a 1D array\n",
        "            jaccard_row_array = jaccard_row.values.reshape(1, -1)\n",
        "\n",
        "            # Get weights and wnorm for the current category from the provided dictionary\n",
        "            category_info = category_weights.get(category)\n",
        "            if category_info is None:\n",
        "                raise ValueError(f\"No weights and wnorm found for category: {category}\")\n",
        "            weights = category_info['weights']\n",
        "            wnorm = category_info['wnorm']\n",
        "            weights = np.expand_dims(weights, axis=0)  # Ensure weights is a row vector\n",
        "\n",
        "            # Perform matrix multiplication, ensuring shapes are aligned\n",
        "            hm = np.dot(jaccard_row_array, weights.T)\n",
        "\n",
        "            row_value = np.squeeze(hm) + wnorm\n",
        "\n",
        "            # Set the entire column at once using .loc\n",
        "            x_by_c.loc[index, 'P(x/c)'] = row_value\n",
        "        except ValueError as e:\n",
        "            print(f\"Error in row {index}: {e}\")\n",
        "            raise\n",
        "\n",
        "    return x_by_c\n",
        "x_by_c_values = calculate_x_by_c(Data_binary, one_hot_encoded, categories_weights)\n",
        "# Category probability dataframe\n",
        "def calculate_category_probabilities(data_frame, category_column):\n",
        "    # Count the number of rows for each category\n",
        "    category_counts = data_frame[category_column].value_counts()\n",
        "\n",
        "    # Calculate probabilities for each category\n",
        "    category_probabilities = category_counts / len(data_frame)\n",
        "\n",
        "    # Create a DataFrame with category_probabilities\n",
        "    category_probabilities_df = pd.DataFrame({\n",
        "        category_column: category_probabilities.index,\n",
        "        'Probability': category_probabilities.values\n",
        "    })\n",
        "\n",
        "    return category_counts, category_probabilities_df\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'Data_binary' is your DataFrame with 'Category' column\n",
        "category_counts, category_probabilities_df = calculate_category_probabilities(Data_binary, 'Category')\n",
        "#Record probability per category\n",
        "categories = np.sort(Data_binary['Category'].unique())\n",
        "\n",
        "# Set constant values\n",
        "record_prob = 1 / len(Data_binary)\n",
        "\n",
        "# Calculate record_prob / P(c) for each category\n",
        "record_prob_per_category = pd.DataFrame({\n",
        "    'Category': categories,\n",
        "    'Record_Prob_Per_Category': record_prob / category_probabilities_df.loc[category_probabilities_df['Category'].isin(categories), 'Probability'].values\n",
        "})\n",
        "# Maximum Posterior category\n",
        "def calculate_max_posterior_category(binary_dataset, x_by_c_values, record_prob_per_category):\n",
        "    predicted_categories = []\n",
        "    posterior_probabilities = []\n",
        "\n",
        "    for index, row_values in x_by_c_values.iterrows():\n",
        "        # Extract the 'P(x/c)' value\n",
        "        p_x_c = row_values['P(x/c)']\n",
        "\n",
        "        # Calculate P(c/x) for each category\n",
        "        category_probs = p_x_c * record_prob_per_category['Record_Prob_Per_Category']\n",
        "\n",
        "        # Determine the predicted category as the one with the maximum P(c/x)\n",
        "        predicted_category = record_prob_per_category['Category'][category_probs.idxmax()]\n",
        "        predicted_categories.append(predicted_category)\n",
        "\n",
        "        # Store the maximum posterior probability for the row\n",
        "        max_posterior_probability = category_probs.max()\n",
        "        posterior_probabilities.append(max_posterior_probability)\n",
        "\n",
        "    # Add columns to the binary_dataset DataFrame\n",
        "    binary_dataset['Predicted_Category'] = predicted_categories\n",
        "    #binary_dataset['Posterior_Probability'] = posterior_probabilities\n",
        "\n",
        "    return binary_dataset\n",
        "\n",
        "# Example usage:\n",
        "ankit_result = calculate_max_posterior_category(Data_binary, x_by_c_values, record_prob_per_category)\n",
        "# Train-Test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(Data_binary, Data_binary[dependent], test_size=0.3, random_state=42)\n",
        "#Calculate Root Mean Square Error\n",
        "def calculate_rmse(X_test, intervals_df):\n",
        "    # Calculate predicted category\n",
        "    prediction = calculate_max_posterior_category(Data_binary, x_by_c_values, record_prob_per_category)\n",
        "    predicted_column = prediction['Predicted_Category']\n",
        "\n",
        "    # Initialize squared differences array\n",
        "    squared_differences = np.zeros(len(X_test))\n",
        "    sum = 0.0\n",
        "    # Iterate over rows of X_test\n",
        "    for i, row in X_test.iterrows():\n",
        "        # Extract actual and predicted categories for the current row\n",
        "        actual_category = row['Category']\n",
        "        predicted_category = predicted_column[i]\n",
        "\n",
        "        # Extract the max values for the actual and predicted categories\n",
        "        actual_max_value = intervals_df[intervals_df['Category'] == actual_category]['Max_Value'].values[0]\n",
        "        predicted_max_value = intervals_df[intervals_df['Category'] == predicted_category]['Max_Value'].values[0]\n",
        "\n",
        "        # Calculate squared difference for the current row\n",
        "        dif = predicted_max_value - actual_max_value\n",
        "        sum+=dif;\n",
        "\n",
        "    # Calculate mean squared error\n",
        "    mean_squared_error = sum/len(X_test)\n",
        "\n",
        "    # Calculate root mean squared error\n",
        "    rmse = np.sqrt(mean_squared_error)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "# Assuming X_test is your DataFrame\n",
        "result_rmse = calculate_rmse(X_test, intervals_df)\n",
        "print(\"Root Mean Squared Error (RMSE):\", result_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjBzAW_74oXl",
        "outputId": "174d83da-7d3a-41b9-9670-962c4e3b5146"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to the dataset CSV file: /content/drive/MyDrive/bolts.csv\n",
            "Enter the name of the dependent column: T20BOLT\n",
            "Enter the path to the frequent itemset CSV file: /content/drive/MyDrive/freq_item_bolts.csv\n",
            "Root Mean Squared Error (RMSE): 0.7478076799715105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnc_JasqNsut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-FPYdE5-Ntac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4hOzsp3SLeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Aneesh Sir's conditions"
      ],
      "metadata": {
        "id": "ntyE1-bONt4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_curves(X_train, y_train, X_val, y_val, X_test, y_test, intervals_df, num_intervals):\n",
        "    # Initialize lists to store training and validation errors\n",
        "    train_errors = []\n",
        "    val_errors = []\n",
        "\n",
        "    # Train the model on varying sizes of the training set\n",
        "    for i in range(10, len(X_train) + 1, 10):\n",
        "        # Train the model on the first i samples\n",
        "        X_train_subset = X_train.iloc[:i]\n",
        "        y_train_subset = y_train.iloc[:i]\n",
        "\n",
        "        # Train the model on the subset\n",
        "        x_by_c_values_subset = calculate_x_by_c(X_train_subset, one_hot_encoded, categories_weights)\n",
        "\n",
        "        # Calculate RMSE on the validation set\n",
        "        val_rmse = calculate_rmse(X_val, intervals_df)\n",
        "\n",
        "        # Calculate RMSE on the training set\n",
        "        train_rmse = calculate_rmse(X_train_subset, intervals_df)\n",
        "\n",
        "        # Append errors to the lists\n",
        "        train_errors.append(train_rmse)\n",
        "        val_errors.append(val_rmse)\n",
        "\n",
        "    # Plot the learning curves\n",
        "    plt.plot(range(10, len(X_train) + 1, 10), train_errors, label='Training RMSE')\n",
        "    plt.plot(range(10, len(X_train) + 1, 10), val_errors, label='Validation RMSE')\n",
        "    plt.xlabel('Number of Training Samples')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Learning Curves')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to create learning curves\n",
        "create_learning_curves(X_train, y_train, X_val, y_val, X_test, y_test, intervals_df, num_intervals)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "4LvO-jEVNwhZ",
        "outputId": "8030564d-5837-496c-cbb5-dab57aa54288"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "15",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 15 is not in range",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c775d22cede4>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Call the function to create learning curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcreate_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintervals_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_intervals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-c775d22cede4>\u001b[0m in \u001b[0;36mcreate_learning_curves\u001b[0;34m(X_train, y_train, X_val, y_val, X_test, y_test, intervals_df, num_intervals)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Train the model on the subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx_by_c_values_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_x_by_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Calculate RMSE on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-6e26cc19e74d>\u001b[0m in \u001b[0;36mcalculate_x_by_c\u001b[0;34m(X, one_hot_encoded, category_weights)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Extract only the row corresponding to the current row in X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mjaccard_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# Transform the row to a 1D array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;31m# GH#5567 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4054\u001b[0m                     \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4056\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4058\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 15"
          ]
        }
      ]
    }
  ]
}