{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYejP7ggjas4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "820Qsj3vKCcQ",
        "outputId": "5e7c6311-9b35-48d0-d37d-a8c01e736636"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import statements"
      ],
      "metadata": {
        "id": "ScT6oWtqjxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, dok_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import dok_matrix"
      ],
      "metadata": {
        "id": "ZJa1HVyZjw0-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SCGIS"
      ],
      "metadata": {
        "id": "XCekCfVsj5w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def train_SCGIS(SX, max_iterations=100, tolerance=1e-5):\n",
        "    \"\"\"\n",
        "    Trains the SCGIS algorithm based on the provided sparse incidence matrix SX.\n",
        "    The observed supports (OS) are calculated internally based on SX.\n",
        "\n",
        "    Parameters:\n",
        "    - SX: Sparse incidence matrix (scipy.sparse.csr_matrix) or pandas DataFrame\n",
        "    - max_iterations: Maximum number of iterations for convergence\n",
        "    - tolerance: Convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "    - weights: Estimated weights for the frequent itemsets\n",
        "    - wnorm: Normalization factor\n",
        "    \"\"\"\n",
        "    # Check if the input matrix is empty\n",
        "    if SX.shape[0] == 0 or SX.shape[1] == 0:\n",
        "        raise ValueError(\"Sparse incidence matrix SX is empty\")\n",
        "\n",
        "    # If SX is a DataFrame, convert it to a sparse matrix\n",
        "    if isinstance(SX, pd.DataFrame):\n",
        "        SX = csr_matrix(SX)\n",
        "\n",
        "    # Calculate observed supports (OS) as the sum of each column\n",
        "    OS = np.array(SX.sum(axis=0)).flatten()  # Sum over rows to get supports for each itemset\n",
        "\n",
        "    # Initialize weights and expected supports (ES)\n",
        "    weights = np.zeros(SX.shape[1], dtype=np.float32)\n",
        "    ES = np.zeros(SX.shape[1], dtype=np.float32)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Calculate probabilities\n",
        "        P = np.exp(SX.dot(weights))\n",
        "        if np.sum(P) == 0:\n",
        "            raise ValueError(\"Sum of probabilities P is zero\")\n",
        "\n",
        "        P /= np.sum(P)  # Normalize the probabilities\n",
        "\n",
        "        # Update expected supports\n",
        "        ES.fill(0)\n",
        "        for i in range(SX.shape[0]):\n",
        "            ES += P[i] * SX.getrow(i).toarray().flatten()  # Access each row with getrow(i)\n",
        "\n",
        "        # Update weights\n",
        "        wold = weights.copy()\n",
        "        weights += np.log(OS + 1e-10) - np.log(ES + 1e-10)\n",
        "\n",
        "        # Normalization\n",
        "        wnorm = np.sum(np.exp(weights))\n",
        "        if wnorm == 0:\n",
        "            raise ValueError(\"Normalization factor wnorm is zero\")\n",
        "\n",
        "        weights /= wnorm\n",
        "\n",
        "        # Check for convergence\n",
        "        wprecision = np.max(np.abs(weights - wold))\n",
        "        if wprecision < tolerance:\n",
        "            break\n",
        "\n",
        "    return weights, wnorm\n"
      ],
      "metadata": {
        "id": "b9JUvcKvjzVq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Categorize->Dice->intervals->missing->binarize->transform->filter"
      ],
      "metadata": {
        "id": "IyUaNRCkkg2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_column(dataset, dependent_column_name, num_intervals):\n",
        "    # Fill NaN values with the mean of the column\n",
        "    dataset_filled = dataset.fillna(dataset[dependent_column_name].mean())\n",
        "\n",
        "    # Extract the specified dependent column\n",
        "    value_ag_filled = dataset_filled[dependent_column_name]\n",
        "\n",
        "    # Define the target range (0-1)\n",
        "    target_range = (0, 1)\n",
        "\n",
        "    # Scale to the target range [0, 1] using np.interp\n",
        "    value_ag_scaled_filled = np.interp(value_ag_filled, (value_ag_filled.min(), value_ag_filled.max()), target_range)\n",
        "\n",
        "    # Create intervals and assign labels for scaled values\n",
        "    data_inter_filled = pd.cut(value_ag_scaled_filled, bins=num_intervals, labels=[chr(ord('A') + i) for i in range(num_intervals)])\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'Category': data_inter_filled,\n",
        "        dependent_column_name: value_ag_scaled_filled\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to your Temporary_SXmat directory where X_interval files are saved\n",
        "save_path = '/content/drive/MyDrive/Temporary_SXmat'\n",
        "\n",
        "# Load your frequent_patterns dataframe (replace with your actual frequent patterns dataframe)\n",
        "# For example, assuming `support_df` is loaded from a CSV\n",
        "# support_df = pd.read_csv('path_to_support_patterns.csv')\n",
        "\n",
        "def calculate_dice_similarity(dataset_df, support_df):\n",
        "    \"\"\"\n",
        "    Calculate the Dice-SÃ¸rensen Coefficient (DSC) matrix for a given dataset and frequent patterns.\n",
        "\n",
        "    Args:\n",
        "        dataset_df (pd.DataFrame): The dataset dataframe.\n",
        "        support_df (pd.DataFrame): The frequent patterns dataframe (support patterns).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe containing the DSC matrix.\n",
        "    \"\"\"\n",
        "    unique_items = set()\n",
        "    for pattern in support_df.iloc[:, 0]:  # Assuming patterns are in the first column\n",
        "        pattern = str(pattern)\n",
        "        if pd.notna(pattern):\n",
        "            try:\n",
        "                items = pattern.split(', ')\n",
        "                unique_items.update(items)\n",
        "            except AttributeError:\n",
        "                print(f\"Skipping pattern {pattern} as it's not a valid string.\")\n",
        "\n",
        "    unique_item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
        "    bit_vector_length = len(unique_item_mapping)\n",
        "\n",
        "    # Convert dataset rows to bit vectors\n",
        "    def row_to_bit_vector(row, item_mapping, bit_length):\n",
        "        bit_vector = np.zeros(bit_length, dtype=int)\n",
        "        for col, value in row.items():\n",
        "            item = f\"{col}={value}\"\n",
        "            if item in item_mapping:\n",
        "                bit_vector[item_mapping[item]] = 1\n",
        "        return bit_vector\n",
        "\n",
        "    dataset_bit_vectors = np.array([row_to_bit_vector(row, unique_item_mapping, bit_vector_length) for _, row in dataset_df.iterrows()])\n",
        "\n",
        "    # Convert support patterns to bit vectors\n",
        "    def pattern_to_bit_vector(pattern, item_mapping, bit_length):\n",
        "        bit_vector = np.zeros(bit_length, dtype=int)\n",
        "        try:\n",
        "            pattern = str(pattern)\n",
        "            for item in pattern.split(', '):\n",
        "                if item in item_mapping:\n",
        "                    bit_vector[item_mapping[item]] = 1\n",
        "        except AttributeError:\n",
        "            print(f\"Skipping pattern {pattern} as it's not a valid string.\")\n",
        "        return bit_vector\n",
        "\n",
        "    support_pattern_bit_vectors = np.array([\n",
        "        pattern_to_bit_vector(pattern, unique_item_mapping, bit_vector_length)\n",
        "        for pattern in support_df.iloc[:, 0]  # Assuming patterns are in the first column\n",
        "        if pd.notna(pattern)\n",
        "    ])\n",
        "\n",
        "    # Initialize the DSC matrix with the correct shape\n",
        "    dsc_matrix = np.zeros((len(dataset_bit_vectors), len(support_pattern_bit_vectors)))\n",
        "\n",
        "    # Calculate DSC for each combination of dataset row and support pattern\n",
        "    for i, data_vector in enumerate(dataset_bit_vectors):\n",
        "        for j, pattern_vector in enumerate(support_pattern_bit_vectors):\n",
        "            intersection = np.sum(data_vector & pattern_vector)\n",
        "            union_cardinality = np.sum(data_vector) + np.sum(pattern_vector)\n",
        "            if union_cardinality > 0:\n",
        "                dsc_matrix[i, j] = (2 * intersection) / union_cardinality\n",
        "            else:\n",
        "                dsc_matrix[i, j] = 0\n",
        "\n",
        "    # Create a dataframe with the DSC matrix\n",
        "    dsc_df = pd.DataFrame(dsc_matrix, columns=[f'Pattern_{k}' for k in range(len(support_pattern_bit_vectors))])\n",
        "    return dsc_df\n",
        "\n",
        "def create_intervals_df(data_frame, column_name, num_intervals):\n",
        "    # Extract the specified column\n",
        "    value_ag = data_frame[column_name]\n",
        "\n",
        "    #print(\"Extracted column values:\")\n",
        "    #print(value_ag)\n",
        "\n",
        "    # Check if the column contains numeric data\n",
        "    if not pd.api.types.is_numeric_dtype(value_ag):\n",
        "        raise ValueError(f\"The '{column_name}' column must contain numeric data.\")\n",
        "\n",
        "    # Check for missing values in the column\n",
        "    if value_ag.isnull().any():\n",
        "        raise ValueError(f\"The '{column_name}' column contains missing values. Please handle them before processing.\")\n",
        "\n",
        "    # Check if scaling and clipping is necessary\n",
        "    if value_ag.min() < 0 or value_ag.max() > 1:\n",
        "        # Rescale to the range [0, 1]\n",
        "        value_ag_scaled = (value_ag - value_ag.min()) / (value_ag.max() - value_ag.min())\n",
        "\n",
        "        # Clip values to the range [0, 1]\n",
        "        value_ag_scaled_clipped = np.clip(value_ag_scaled, 0, 1)\n",
        "    else:\n",
        "        # No need to rescale or clip\n",
        "        value_ag_scaled_clipped = value_ag\n",
        "\n",
        "    #print(\"Scaled and clipped column values:\")\n",
        "    #print(value_ag_scaled_clipped)\n",
        "\n",
        "    # Calculate bin edges dynamically within the [0, 1] range\n",
        "    bin_edges = np.linspace(0, 1, num=num_intervals + 1)\n",
        "\n",
        "    # Create intervals and assign labels for clipped values\n",
        "    data_inter_clipped = pd.cut(value_ag_scaled_clipped.values, bins=bin_edges, labels=[chr(ord('A') + i) for i in range(len(bin_edges) - 1)])\n",
        "\n",
        "    #print(\"Computed intervals:\")\n",
        "    #print(data_inter_clipped)\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    intervals_df = pd.DataFrame({\n",
        "        'Category': data_inter_clipped.categories,\n",
        "        'Min_Value': [value_ag_scaled_clipped[data_inter_clipped == category].min() for category in data_inter_clipped.categories],\n",
        "        'Max_Value': [value_ag_scaled_clipped[data_inter_clipped == category].max() for category in data_inter_clipped.categories]\n",
        "    })\n",
        "\n",
        "    return intervals_df\n",
        "\n",
        "\n",
        "\n",
        "def handle_missing_values(dataset):\n",
        "    # Check if there are missing values in the dataset\n",
        "    if dataset.isnull().any().any():\n",
        "        # Identify numerical and categorical columns\n",
        "        numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
        "        categorical_cols = dataset.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "        # Impute numerical columns with mean\n",
        "        imputer_numeric = SimpleImputer(strategy='mean')\n",
        "        dataset[numerical_cols] = imputer_numeric.fit_transform(dataset[numerical_cols])\n",
        "\n",
        "        # Impute categorical columns with the most frequent value (mode)\n",
        "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "        dataset[categorical_cols] = imputer_categorical.fit_transform(dataset[categorical_cols])\n",
        "\n",
        "        return dataset\n",
        "    else:\n",
        "        # If no missing values, return the original dataset\n",
        "        return dataset\n",
        "\n",
        "def binarize_dataframe(input_df, num_bins):\n",
        "    df = pd.DataFrame(input_df)\n",
        "    df_binarized = pd.DataFrame()\n",
        "\n",
        "    for column in df.columns:\n",
        "        column_name = column+'_binarized'\n",
        "        bins = pd.qcut(df[column], q=num_bins, labels=False, duplicates='drop')\n",
        "        df_binarized[column_name] = (bins == bins.max()).astype(int)\n",
        "\n",
        "    return df_binarized\n",
        "\n",
        "def transform_matrix_dice(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "    print(transformed_matrix)\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.40] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.40] = 0\n",
        "    return transformed_matrix\n",
        "\n",
        "def transform_matrix_dice(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "    #print(\"Inside Transformed\")\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.40] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.40] = 0\n",
        "    return transformed_matrix\n",
        "\n",
        "\n",
        "def filter_data_for_interval(df, category_col, category):\n",
        "    # Filter data for a specific interval category\n",
        "    return df[df[category_col] == category]\n",
        "\n",
        "def transform_matrix_for_interval(X_interval, support_df):\n",
        "    jaccard_result = calculate_dice_similarity(X_interval, support_df)\n",
        "    #print(\"Inside Transform\")\n",
        "    #print(jaccard_result.shape)\n",
        "    transformed_matrix = transform_matrix_dice(jaccard_result)\n",
        "    print(transformed_matrix.shape)\n",
        "    return transformed_matrix"
      ],
      "metadata": {
        "id": "DHxtFgMmj9yF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bayesian model for FP(with interestingness)"
      ],
      "metadata": {
        "id": "EHFStLBXk5nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "def find_separate_itemsets_for_measures(dataset_path, freq_pattern_path, drop_column=None, max_iterations=100, convergence_threshold=1e-4, thresholds=None):\n",
        "    # Load dataset and frequent patterns\n",
        "    dataset = pd.read_csv(dataset_path)\n",
        "    freq_patterns = pd.read_csv(freq_pattern_path, sep='\\t')\n",
        "\n",
        "    # Default thresholds if not provided\n",
        "    if thresholds is None:\n",
        "        thresholds = {\n",
        "            \"support\": 0.01,\n",
        "            \"confidence\": 0.000000000001,\n",
        "            \"lift\": 0.01,\n",
        "            \"leverage\": 0.0,\n",
        "            \"jaccard\": 0.5,\n",
        "            \"cosine\": 0.5\n",
        "        }\n",
        "\n",
        "    # Preprocess dataset - drop column if specified\n",
        "    if drop_column:\n",
        "        X = dataset.drop(columns=[drop_column])\n",
        "    else:\n",
        "        X = dataset  # If no column is specified to drop, use the full dataset\n",
        "\n",
        "    binary_data = Binarizer().fit_transform(X)\n",
        "\n",
        "    # Initialize itemset probabilities pi_s for each pattern\n",
        "    pi = np.ones(len(freq_patterns))  # Initialize all itemsets with probability 1 (uniform prior)\n",
        "\n",
        "    # Helper functions for Bayesian Mixture Model\n",
        "\n",
        "    def e_step(data, pi):\n",
        "        epsilon = 1e-10  # Small value to avoid log of zero\n",
        "        log_resp = np.zeros((data.shape[0], len(pi)))  # Initialize the responsibilities\n",
        "        for i, pattern in enumerate(freq_patterns['Itemsets']):\n",
        "            # Parse the pattern to get item columns\n",
        "            pattern_items = [item.split('=')[0] for item in pattern.split()]\n",
        "            pattern_columns = [col for col in X.columns if col in pattern_items]\n",
        "            if not pattern_columns:\n",
        "                continue  # Skip patterns that don't match any columns\n",
        "\n",
        "            # Indicator function: 1 if itemset is present, 0 if not\n",
        "            si = np.zeros(data.shape[0])\n",
        "            for j, row in enumerate(data):\n",
        "                si[j] = 1 if all(row[X.columns.get_loc(col)] for col in pattern_columns) else 0\n",
        "\n",
        "            # Calculate log-probability of the data under each itemset\n",
        "            log_prob = si * np.log(pi[i] + epsilon) + (1 - si) * np.log(1 - pi[i] + epsilon)\n",
        "            log_resp[:, i] = log_prob\n",
        "\n",
        "        # Normalize the responsibilities across all itemsets (log-sum-exp trick)\n",
        "        log_resp -= logsumexp(log_resp, axis=1)[:, np.newaxis]\n",
        "        return np.exp(log_resp)\n",
        "\n",
        "    def m_step(data, responsibilities):\n",
        "        N, D = data.shape\n",
        "        Nk = responsibilities.sum(axis=0)\n",
        "        pi_new = Nk / N  # Update component priors (probabilities of each itemset)\n",
        "        return pi_new\n",
        "\n",
        "    # Run EM algorithm until convergence or maximum iterations\n",
        "    for iteration in range(max_iterations):\n",
        "        prev_pi = pi.copy()\n",
        "\n",
        "        # Perform E-step and M-step\n",
        "        responsibilities = e_step(binary_data, pi)\n",
        "        pi = m_step(binary_data, responsibilities)\n",
        "\n",
        "        # Check for convergence\n",
        "        delta_pi = np.linalg.norm(pi - prev_pi)\n",
        "        if delta_pi < convergence_threshold:\n",
        "            print(f\"Converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "    # Initialize separate results for each interestingness measure\n",
        "    support_results = []\n",
        "    confidence_results = []\n",
        "    lift_results = []\n",
        "    leverage_results = []\n",
        "    jaccard_results = []\n",
        "    cosine_results = []\n",
        "\n",
        "    # Calculate interestingness measures for each pattern\n",
        "    for _, row in freq_patterns.iterrows():\n",
        "        pattern_string = row['Itemsets']\n",
        "\n",
        "        # Parse the pattern string into column names and values\n",
        "        pattern_items = [\n",
        "            item.strip().split('=')[0]  # Extract the attribute (column name)\n",
        "            for item in pattern_string.split()  # Split by spaces\n",
        "            if '=' in item  # Ignore parts without '=' (e.g., \"count=8\")\n",
        "        ]\n",
        "\n",
        "        # Match column names in the dataset\n",
        "        pattern_columns = [col for col in X.columns if col in pattern_items]\n",
        "        if not pattern_columns:\n",
        "            continue  # Skip patterns with no matching columns\n",
        "\n",
        "        # Calculate support\n",
        "        support_count = (binary_data[:, [X.columns.get_loc(col) for col in pattern_columns]].sum(axis=1)\n",
        "                         == len(pattern_columns)).sum()\n",
        "        support = support_count / binary_data.shape[0]\n",
        "\n",
        "        # Confidence (Assuming a simple rule A -> B)\n",
        "        if len(pattern_columns) > 1:\n",
        "            A, B = pattern_columns[:-1], pattern_columns[-1]\n",
        "            support_A = (binary_data[:, [X.columns.get_loc(col) for col in A]].sum(axis=1) == len(A)).sum() / binary_data.shape[0]\n",
        "            confidence = support / (support_A + 1e-10)\n",
        "        else:\n",
        "            confidence = np.nan\n",
        "\n",
        "        # Lift\n",
        "        if len(pattern_columns) > 1:\n",
        "            support_B = (binary_data[:, X.columns.get_loc(B)] == 1).sum() / binary_data.shape[0]\n",
        "            lift = support / (support_A * support_B + 1e-10)\n",
        "        else:\n",
        "            lift = np.nan\n",
        "\n",
        "        # Leverage\n",
        "        leverage = support - (support_A * support_B) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Jaccard Index\n",
        "        jaccard = support / (support_A + support_B - support) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Cosine Similarity\n",
        "        cosine = support / (support_A * support_B + 1e-10) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Store results if they exceed the threshold\n",
        "        if support >= thresholds['support']:\n",
        "            support_results.append({\"Pattern\": pattern_string, \"Support\": support})\n",
        "        if confidence >= thresholds['confidence']:\n",
        "            confidence_results.append({\"Pattern\": pattern_string, \"Confidence\": confidence})\n",
        "        if lift >= thresholds['lift']:\n",
        "            lift_results.append({\"Pattern\": pattern_string, \"Lift\": lift})\n",
        "        if leverage >= thresholds['leverage']:\n",
        "            leverage_results.append({\"Pattern\": pattern_string, \"Leverage\": leverage})\n",
        "        if jaccard >= thresholds['jaccard']:\n",
        "            jaccard_results.append({\"Pattern\": pattern_string, \"Jaccard\": jaccard})\n",
        "        if cosine >= thresholds['cosine']:\n",
        "            cosine_results.append({\"Pattern\": pattern_string, \"Cosine\": cosine})\n",
        "\n",
        "    # Convert results to DataFrames\n",
        "    support_df = pd.DataFrame(support_results)\n",
        "    confidence_df = pd.DataFrame(confidence_results)\n",
        "    lift_df = pd.DataFrame(lift_results)\n",
        "    leverage_df = pd.DataFrame(leverage_results)\n",
        "    jaccard_df = pd.DataFrame(jaccard_results)\n",
        "    cosine_df = pd.DataFrame(cosine_results)\n",
        "\n",
        "    return support_df, confidence_df, lift_df, leverage_df, jaccard_df, cosine_df"
      ],
      "metadata": {
        "id": "iOGOD8xpk1ue"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Density->process_interval"
      ],
      "metadata": {
        "id": "_57fOy5snJ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_density(matrix):\n",
        "    num_ones = matrix.nnz if isinstance(matrix, csr_matrix) else np.count_nonzero(matrix)\n",
        "    total_elements = matrix.shape[0] * matrix.shape[1]\n",
        "    density = num_ones / total_elements\n",
        "    return density\n",
        "\n",
        "# def process_interval(row):\n",
        "#     category = row['Category']\n",
        "#     interval_min = row['Min_Value']\n",
        "#     interval_max = row['Max_Value']\n",
        "\n",
        "#     filtered_data = filter_data_for_interval(Data_discretized, 'Category', category)\n",
        "#     if not filtered_data.empty:\n",
        "#         X_interval = filtered_data.drop(columns=['Category'])\n",
        "#         print(X_interval.shape)\n",
        "#         val = calculate_dice_similarity(X_interval, support_df)\n",
        "#         #print(val.shape)\n",
        "#         val1 = transform_matrix_for_interval(X_interval, support_df)\n",
        "#         SXmat = val1\n",
        "#         SXmat = pd.DataFrame(SXmat)\n",
        "#         # Calculate Density\n",
        "#         density = calculate_density(SXmat)\n",
        "#         print(f\"Density of incidence matrix for this interval (Category {category}): {density}\")\n",
        "#         weights, w_norm = fis_algorithm_with_auxiliary(SXmat)\n",
        "#         return category, {'weights': weights, 'wnorm': w_norm}\n",
        "#     else:\n",
        "#         print(f\"No data for interval {category}\")\n",
        "#         return category, None\n",
        "\n",
        "def process_interval(row):\n",
        "    category = row['Category']\n",
        "    interval_min = row['Min_Value']\n",
        "    interval_max = row['Max_Value']\n",
        "\n",
        "    filtered_data = filter_data_for_interval(Data_discretized, 'Category', category)\n",
        "    if not filtered_data.empty:\n",
        "        X_interval = filtered_data.drop(columns=['Category'])\n",
        "        print(X_interval.shape)\n",
        "        val = calculate_dice_similarity(X_interval, support_df)\n",
        "        print(val.shape)\n",
        "        val1 = transform_matrix_for_interval(X_interval, support_df)\n",
        "        SXmat = val1\n",
        "        SXmat = pd.DataFrame(SXmat)\n",
        "        print(SXmat.head(1))\n",
        "        df_SXmat = pd.DataFrame(SXmat)\n",
        "        sxmat_file_path = os.path.join(save_path, f'SXmat_Category_{category}.csv')\n",
        "        df_SXmat.to_csv(sxmat_file_path, index=False)\n",
        "        print(f\"SXmat for Category {category} saved at {sxmat_file_path}\")\n",
        "        # Calculate Density\n",
        "        density = calculate_density(SXmat)\n",
        "        print(f\"Density of incidence matrix for this interval (Category {category}): {density}\")\n",
        "        weights, w_norm = train_SCGIS(SXmat)\n",
        "        print(weights.shape)\n",
        "        #print(wnorm.shape)\n",
        "        return category, {'weights': weights, 'wnorm': w_norm}\n",
        "    else:\n",
        "        print(f\"No data for interval {category}\")\n",
        "        return category, None\n",
        "\n"
      ],
      "metadata": {
        "id": "FA_HMh41lvL0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process_test_dataset"
      ],
      "metadata": {
        "id": "QHhgP_pDvL_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def process_test_data(Data_test, support_df, categories_weights, constant):\n",
        "#     # Create a temporary DataFrame as a copy of Data_test\n",
        "#     Data_test_temp = Data_test.copy()\n",
        "\n",
        "#     # Iterate through each row in Data_test_temp\n",
        "#     for index, row in Data_test_temp.iterrows():\n",
        "#         # Convert the row to a DataFrame for compatibility with transform_matrix_for_interval\n",
        "#         row_df = pd.DataFrame([row])\n",
        "\n",
        "#         # Calculate the Jaccard/Dice similarity for the current row with support_df\n",
        "#         jaccard_result = calculate_dice_similarity_demo(row_df, support_df)\n",
        "\n",
        "#         # Directly apply binarization with a threshold of 0.4\n",
        "#         transformed_row = jaccard_result.copy()\n",
        "#         transformed_row[transformed_row > 0.4] = 1\n",
        "#         transformed_row[transformed_row <= 0.4] = 0\n",
        "\n",
        "#         # Flatten the transformed_row to ensure it is 1D (1, 87) -> (87,)\n",
        "#         transformed_row_flat = transformed_row.values.flatten()\n",
        "\n",
        "#         max_value = 0.0  # Initialize the max value\n",
        "#         predicted_category = None  # Store the category with the highest value\n",
        "\n",
        "#         # Iterate over each category and calculate the weighted product\n",
        "#         for category, params in categories_weights.items():\n",
        "#             # Ensure the structure is valid\n",
        "#             if 'weights' in params and 'wnorm' in params:\n",
        "#                 weights = params['weights']  # Extract weights for the category\n",
        "#                 wnorm = params['wnorm']      # Extract normalization value\n",
        "\n",
        "#                 # Reshape weights to (1, 87) to match the transformed_row shape (1, 87)\n",
        "#                 weights_reshaped = weights.reshape(1, -1)\n",
        "\n",
        "#                 # Ensure both weights and transformed_row are compatible (both should be (1, 87))\n",
        "#                 if weights_reshaped.shape != transformed_row.shape:\n",
        "#                     print(f\"Shape mismatch between weights and transformed_row for category {category}\")\n",
        "#                     continue\n",
        "\n",
        "#                 product = weights_reshaped * transformed_row  # Element-wise multiplication\n",
        "#                 # Sum the product over all columns (axis=1 sums across columns)\n",
        "#                 product_sum = product.sum(axis=1)  # This will give a single value\n",
        "#                 # Calculate the final product with the normalization value and constant\n",
        "#                 final_product = (product_sum + wnorm) * constant\n",
        "#                 final_product = final_product.item()\n",
        "#                 # Check if the current product is higher than max_value\n",
        "#                 if final_product > max_value:\n",
        "#                     max_value = final_product\n",
        "#                     predicted_category = category  # Store the category with the highest product\n",
        "\n",
        "#         # After processing all categories for the current row, add the predicted category to Data_test_temp\n",
        "#         Data_test_temp.loc[index, 'predicted_category'] = predicted_category\n",
        "\n",
        "#     # Return the updated DataFrame with the predicted category column\n",
        "#     return Data_test_temp"
      ],
      "metadata": {
        "id": "jZBVMhOYvFI9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_data(Data_test, support_df, categories_weights, constant):\n",
        "    # Create a temporary DataFrame as a copy of Data_test\n",
        "    Data_test_temp = Data_test.copy()\n",
        "\n",
        "    # Iterate through each row in Data_test_temp\n",
        "    for index, row in Data_test_temp.iterrows():\n",
        "        # Convert the row to a DataFrame for compatibility with transform_matrix_for_interval\n",
        "        row_df = pd.DataFrame([row])\n",
        "\n",
        "        # Calculate the Jaccard/Dice similarity for the current row with support_df\n",
        "        jaccard_result = calculate_dice_similarity(row_df, support_df)\n",
        "\n",
        "        # Directly apply binarization with a threshold of 0.4\n",
        "        transformed_row = jaccard_result.copy()\n",
        "        transformed_row[transformed_row > 0.4] = 1\n",
        "        transformed_row[transformed_row <= 0.4] = 0\n",
        "\n",
        "        # Flatten the transformed_row to ensure it is 1D (1, 87) -> (87,)\n",
        "        transformed_row_flat = transformed_row.values.flatten()\n",
        "\n",
        "        max_value = 0.0  # Initialize the max value\n",
        "        predicted_category = None  # Store the category with the highest value\n",
        "\n",
        "        # Iterate over each category and calculate the weighted product\n",
        "        for category, params in categories_weights.items():\n",
        "            # Ensure the structure is valid\n",
        "            if 'weights' in params and 'wnorm' in params:\n",
        "                weights = params['weights']  # Extract weights for the category\n",
        "                wnorm = params['wnorm']      # Extract normalization value\n",
        "\n",
        "                # Reshape weights to (1, 87) to match the transformed_row shape (1, 87)\n",
        "                weights_reshaped = weights.reshape(1, -1)\n",
        "\n",
        "                # Ensure both weights and transformed_row are compatible (both should be (1, 87))\n",
        "                if weights_reshaped.shape != transformed_row.shape:\n",
        "                    print(f\"Shape mismatch between weights and transformed_row for category {category}\")\n",
        "                    continue\n",
        "\n",
        "                # Raise each weight to the power of the corresponding binary value in transformed_row\n",
        "                product = weights_reshaped ** transformed_row  # Element-wise exponentiation\n",
        "\n",
        "                # Sum the product over all columns (axis=1 sums across columns)\n",
        "                product_sum = product.sum(axis=1)  # This will give a single value\n",
        "\n",
        "                # Calculate the final product with the normalization value and constant\n",
        "                final_product = (product_sum + wnorm) * constant\n",
        "                final_product = final_product.item()\n",
        "\n",
        "                # Check if the current product is higher than max_value\n",
        "                if final_product > max_value:\n",
        "                    max_value = final_product\n",
        "                    predicted_category = category  # Store the category with the highest product\n",
        "\n",
        "        # After processing all categories for the current row, add the predicted category to Data_test_temp\n",
        "        Data_test_temp.loc[index, 'predicted_category'] = predicted_category\n",
        "\n",
        "    # Return the updated DataFrame with the predicted category column\n",
        "    return Data_test_temp\n"
      ],
      "metadata": {
        "id": "Mgp_CGNCikeI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RMSE calculation"
      ],
      "metadata": {
        "id": "Q5Dl1zcjvsZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_rmse_category(X_test, intervals_df, updated_data):\n",
        "    sum_squared_diff = 0.0\n",
        "    sum_absolute_diff = 0.0\n",
        "    actual_values = []\n",
        "    predicted_values = []\n",
        "\n",
        "    # Iterate through the test dataset to calculate RMSE, MAE, and collect values for PCC\n",
        "    for i, row in X_test.iterrows():\n",
        "        actual_category = row['Category']  # Actual category\n",
        "        predicted_category = updated_data.loc[i, 'predicted_category']  # Predicted category from updated_data\n",
        "        #print(f\"Predicted Category: {predicted_category}\")\n",
        "\n",
        "        # Get the actual 'Max_Value' based on the actual category\n",
        "        actual_max_value = intervals_df[intervals_df['Category'] == actual_category]['Max_Value'].values\n",
        "\n",
        "        # Check if predicted_category exists in intervals_df\n",
        "        if predicted_category in intervals_df['Category'].values:\n",
        "            predicted_max_value = intervals_df[intervals_df['Category'] == predicted_category]['Max_Value'].values\n",
        "        else:\n",
        "            # Handle case where predicted category is not found in intervals_df\n",
        "            #print(f\"Predicted category '{predicted_category}' not found in intervals_df.\")\n",
        "            predicted_max_value = np.nan  # Set predicted_max_value to NaN or handle it as needed\n",
        "\n",
        "        #print(f\"Actual Max Value: {actual_max_value}, Predicted Max Value: {predicted_max_value}\")\n",
        "\n",
        "        # Ensure the actual max value exists and predicted max value is not NaN\n",
        "        if len(actual_max_value) > 0:\n",
        "            actual_max_value = actual_max_value[0]\n",
        "\n",
        "            # Handle NaN values in actual_max_value or predicted_max_value\n",
        "            if np.isnan(actual_max_value) or np.isnan(predicted_max_value):\n",
        "                #print(f\"Skipping pair due to NaN value (Actual: {actual_max_value}, Predicted: {predicted_max_value})\")\n",
        "\n",
        "                # Option 1: Set difference to 0 if both are NaN\n",
        "                squared_diff = 0\n",
        "                absolute_diff = 0\n",
        "            else:\n",
        "                # Calculate squared difference for RMSE\n",
        "                squared_diff = (predicted_max_value - actual_max_value) ** 2\n",
        "                absolute_diff = abs(predicted_max_value - actual_max_value)\n",
        "\n",
        "            sum_squared_diff += squared_diff\n",
        "            sum_absolute_diff += absolute_diff\n",
        "\n",
        "            # Collect values for PCC calculation\n",
        "            actual_values.append(actual_max_value)\n",
        "            predicted_values.append(predicted_max_value)\n",
        "\n",
        "    # Ensure we have data to calculate the metrics\n",
        "    if len(X_test) > 0:\n",
        "        # Calculate RMSE\n",
        "        mean_squared_error = sum_squared_diff / len(X_test)\n",
        "        rmse = np.sqrt(mean_squared_error).item()  # Convert numpy array to scalar\n",
        "\n",
        "        # Calculate MAE\n",
        "        mae = (sum_absolute_diff / len(X_test)).item()  # Convert numpy array to scalar\n",
        "\n",
        "        return rmse, mae\n",
        "\n",
        "    # Return None if there are no rows in X_test\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "jYsZvnKJvrZN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check dataset"
      ],
      "metadata": {
        "id": "hKKKLS_xBUq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load Dataset while skipping the 'datetime' column\n",
        "dataset = \"/content/drive/MyDrive/socmob_transformed.csv\"\n",
        "concrete = pd.read_csv(dataset)\n",
        "concrete.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "HyIK3REIBXP9",
        "outputId": "f83404aa-c9d8-4a5b-c08d-9abba1817302"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  fathers_occupation  sons_occupation  family_structure  race  \\\n",
              "0   1                  12               12                 0     1   \n",
              "1   2                  12               11                 0     1   \n",
              "\n",
              "   counts_for_sons_first_occupation  counts_for_sons_current_occupation  \n",
              "0                              22.9                                31.3  \n",
              "1                              96.2                                86.6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3a8c663-b263-4fbf-b55b-71066a4a9e79\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>fathers_occupation</th>\n",
              "      <th>sons_occupation</th>\n",
              "      <th>family_structure</th>\n",
              "      <th>race</th>\n",
              "      <th>counts_for_sons_first_occupation</th>\n",
              "      <th>counts_for_sons_current_occupation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>22.9</td>\n",
              "      <td>31.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>96.2</td>\n",
              "      <td>86.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3a8c663-b263-4fbf-b55b-71066a4a9e79')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3a8c663-b263-4fbf-b55b-71066a4a9e79 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3a8c663-b263-4fbf-b55b-71066a4a9e79');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc9c97f2-0932-4c25-9f22-a638a548996b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc9c97f2-0932-4c25-9f22-a638a548996b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc9c97f2-0932-4c25-9f22-a638a548996b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "concrete",
              "summary": "{\n  \"name\": \"concrete\",\n  \"rows\": 1156,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 333,\n        \"min\": 1,\n        \"max\": 1156,\n        \"num_unique_values\": 1156,\n        \"samples\": [\n          941,\n          266,\n          110\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fathers_occupation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 16,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          12,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sons_occupation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 16,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          12,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"family_structure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"race\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"counts_for_sons_first_occupation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44.358269174094616,\n        \"min\": 0.0,\n        \"max\": 746.3,\n        \"num_unique_values\": 358,\n        \"samples\": [\n          11.6,\n          49.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"counts_for_sons_current_occupation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41.00364497068885,\n        \"min\": 0.0,\n        \"max\": 414.0,\n        \"num_unique_values\": 361,\n        \"samples\": [\n          5.8,\n          41.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calling the function with dataset input"
      ],
      "metadata": {
        "id": "1mCv4_NMnWvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load Dataset while skipping the 'datetime' column\n",
        "dataset = \"/content/drive/MyDrive/bolts.csv\"\n",
        "concrete = pd.read_csv(dataset)\n",
        "dependent = \"T20BOLT\"\n",
        "concrete_temp = concrete.copy()\n",
        "X = concrete_temp.drop(dependent, axis=1).copy()\n",
        "y = concrete[dependent]\n",
        "# Usage example:\n",
        "dataset_path = \"/content/drive/MyDrive/bolts.csv\"\n",
        "freq_pattern_path = '/content/drive/MyDrive/AprioriOutput/closed_itemsets_bolts.csv'\n",
        "drop_column = 'T20BOLT'  # Example of the column to drop\n",
        "support_df, confidence_df, lift_df, leverage_df, jaccard_df, cosine_df = find_separate_itemsets_for_measures(dataset_path, freq_pattern_path, drop_column=drop_column)\n",
        "support_df = pd.read_csv(freq_pattern_path,sep='\\t')\n",
        "#support_df = support_df.drop(columns=['Support'])\n",
        "# Handle Missing Values\n",
        "X = handle_missing_values(X)\n",
        "# Binarize DataFrame\n",
        "num_bins = 15\n",
        "df_binarized = binarize_dataframe(pd.DataFrame(concrete), num_bins)\n",
        "# Create Intervals DataFrame\n",
        "num_intervals = 4\n",
        "intervals_df = create_intervals_df(concrete, dependent, num_intervals)\n",
        "print(intervals_df)\n",
        "# Categorize Column\n",
        "result_df = categorize_column(concrete, dependent, num_intervals)\n",
        "P_c = 1 / num_intervals\n",
        "P_x = 1 / concrete.shape[1]\n",
        "constant = P_c / P_x\n",
        "Data_discretized = pd.read_csv('/content/drive/MyDrive/Discretized_datasets/discretized_bolts.csv')\n",
        "Data_discretized['Category'] = result_df['Category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XhhBhVfcnOzF",
        "outputId": "e0ef628d-abe3-4221-efcf-20c5b35608c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 4 iterations.\n",
            "  Category  Min_Value  Max_Value\n",
            "0        A        NaN        NaN\n",
            "1        B   0.420886   0.462025\n",
            "2        C   0.515823   0.750000\n",
            "3        D   0.750316   1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Jaccard Similarity and Train GIS for Each Interval\n",
        "categories_weights = {}\n",
        "categories = Data_discretized['Category'].unique()\n",
        "# Use ThreadPoolExecutor to run computations in parallel\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Create a list of futures\n",
        "    futures = {executor.submit(process_interval, row): row for _, row in intervals_df.iterrows()}\n",
        "    # Process completed futures\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        category, weights_wnorm = future.result()\n",
        "        if weights_wnorm is not None:\n",
        "            categories_weights[category] = weights_wnorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v2w61PLCnOlK",
        "outputId": "dd24d08b-f68e-4863-88b3-d782e8b1b5aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6)(2, 6)\n",
            "\n",
            "(1, 138)\n",
            "(2, 138)\n",
            "(1, 138)(837, 6)\n",
            "\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_128  Pattern_129  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          0.0   \n",
            "\n",
            "   Pattern_130  Pattern_131  Pattern_132  Pattern_133  Pattern_134  \\\n",
            "0          1.0          0.0          0.0          0.0          1.0   \n",
            "\n",
            "   Pattern_135  Pattern_136  Pattern_137  \n",
            "0          0.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 138 columns]\n",
            "(2, 138)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_128  Pattern_129  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          0.0   \n",
            "\n",
            "   Pattern_130  Pattern_131  Pattern_132  Pattern_133  Pattern_134  \\\n",
            "0          1.0          0.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_135  Pattern_136  Pattern_137  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 138 columns]\n",
            "(2268, 6)\n",
            "SXmat for Category A saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_A.csv\n",
            "Density of incidence matrix for this interval (Category A): 0.32608695652173914\n",
            "SXmat for Category B saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_B.csv\n",
            "Density of incidence matrix for this interval (Category B): 0.38405797101449274\n",
            "(138,)\n",
            "(138,)\n",
            "(837, 138)\n",
            "(837, 138)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_128  Pattern_129  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          0.0   \n",
            "\n",
            "   Pattern_130  Pattern_131  Pattern_132  Pattern_133  Pattern_134  \\\n",
            "0          1.0          0.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_135  Pattern_136  Pattern_137  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 138 columns]\n",
            "SXmat for Category C saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_C.csv\n",
            "Density of incidence matrix for this interval (Category C): 0.36337506276730214\n",
            "(2268, 138)\n",
            "(138,)\n",
            "(2268, 138)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_128  Pattern_129  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          0.0   \n",
            "\n",
            "   Pattern_130  Pattern_131  Pattern_132  Pattern_133  Pattern_134  \\\n",
            "0          1.0          0.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_135  Pattern_136  Pattern_137  \n",
            "0          1.0          1.0          1.0  \n",
            "\n",
            "[1 rows x 138 columns]\n",
            "SXmat for Category D saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_D.csv\n",
            "Density of incidence matrix for this interval (Category D): 0.3273106612478593\n",
            "(138,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CCmMNDeEcUZp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing datasets for model evaluation\n",
        "Data_train, Data_test = train_test_split(Data_discretized, test_size=0.2, random_state=5)\n",
        "# Example of calling the function\n",
        "updated_data = process_test_data(Data_test, support_df, categories_weights, constant)\n",
        "# Calculate the root mean squared error (RMSE) for the test data\n",
        "rmse = calculate_rmse_category(Data_test,intervals_df,updated_data)  # Use a custom RMSE calculation function\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "kl-JQrervTvG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5b01938f-1ea5-4895-88b1-875c92201ffc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: (0.13546642516684076, 0.07240831942692011)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqvO8nzdvii2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def find_best_rmse_random_state(Data_discretized, intervals_df, support_df, categories_weights, constant):\n",
        "    best_random_state = None\n",
        "    min_rmse = float('inf')  # Initialize with a large value to ensure any RMSE is smaller\n",
        "\n",
        "    # Iterate through random states from 5 to 200\n",
        "    for random_state in range(3, 101):\n",
        "        # Split the data into training and testing datasets\n",
        "        Data_train, Data_test = train_test_split(Data_discretized, test_size=0.2, random_state=random_state)\n",
        "\n",
        "        # Process the test data\n",
        "        updated_data = process_test_data(Data_test, support_df, categories_weights, constant)\n",
        "\n",
        "        # Calculate the root mean squared error (RMSE) and MAE\n",
        "        rmse, mae = calculate_rmse_category(Data_test, intervals_df, updated_data)\n",
        "\n",
        "        # Compare only the first value (RMSE) from the tuple\n",
        "        if rmse is not None and rmse < min_rmse:\n",
        "            min_rmse = rmse\n",
        "            min_mae = mae\n",
        "            best_random_state = random_state\n",
        "\n",
        "    return best_random_state, min_rmse,min_mae\n",
        "\n",
        "# Example usage\n",
        "best_random_state, min_rmse,min_mae = find_best_rmse_random_state(Data_discretized, intervals_df, support_df, categories_weights, constant)\n",
        "print(f\"Best Random State: {best_random_state}, RMSE: {min_rmse,min_mae}\")\n"
      ],
      "metadata": {
        "id": "mHvIo0GmO9Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7J2lB7MO-Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PBG27_V__QC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}