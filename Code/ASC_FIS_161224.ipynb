{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYejP7ggjas4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import statements"
      ],
      "metadata": {
        "id": "ScT6oWtqjxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, dok_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import dok_matrix"
      ],
      "metadata": {
        "id": "ZJa1HVyZjw0-"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIS"
      ],
      "metadata": {
        "id": "XCekCfVsj5w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def fis_algorithm_with_auxiliary(X, num_iterations=200, learning_rate=0.001, reg_strength=0.001):\n",
        "    \"\"\"\n",
        "    Apply the FIS algorithm to learn weights for binary matrix X with normalization and scaling.\n",
        "    Takes a NumPy array or pandas DataFrame as input for X.\n",
        "    \"\"\"\n",
        "\n",
        "    def initialize_weights(num_features):\n",
        "        \"\"\"Initialize weights to random values, instead of zeros.\"\"\"\n",
        "        return np.random.randn(num_features)  # Random initialization from a normal distribution\n",
        "\n",
        "    def calculate_normalization(X, weights, clip_value=500):\n",
        "        \"\"\"Calculate row-wise normalization Z(x) and return a global normalization parameter.\"\"\"\n",
        "        exp_values = np.exp(X @ weights)  # Shape: (num_samples,)\n",
        "        exp_values = np.clip(exp_values, None, clip_value)  # Clip to avoid overflow\n",
        "        row_norms = exp_values  # Row-wise normalization since exp_values is already 1D\n",
        "        global_norm = np.mean(row_norms)  # Aggregate to a scalar\n",
        "        return global_norm, row_norms\n",
        "\n",
        "    def calculate_log_likelihood(X, weights):\n",
        "        \"\"\"Calculate the log-likelihood using row-specific normalization.\"\"\"\n",
        "        global_norm, row_norms = calculate_normalization(X, weights)\n",
        "        log_likelihood_per_row = X @ weights - np.log(row_norms)\n",
        "        total_log_likelihood = np.sum(log_likelihood_per_row)  # Sum over rows\n",
        "        return total_log_likelihood, global_norm\n",
        "\n",
        "    def auxiliary_function(X, weights, delta):\n",
        "        \"\"\"Auxiliary function Q(δ) derived from the change in log-likelihood.\"\"\"\n",
        "        observed_support = np.sum(X, axis=0)\n",
        "        global_norm, row_norms = calculate_normalization(X, weights)\n",
        "        expected_support = np.sum(np.exp(X @ (weights + delta)) / row_norms, axis=0)\n",
        "        Q = np.sum(delta * observed_support - expected_support)\n",
        "        return Q\n",
        "\n",
        "    def update_weights_using_auxiliary(X, weights, learning_rate=0.001, reg_strength=0.001, max_weight_value=100):\n",
        "        \"\"\"Update weights based on the auxiliary function and row-specific normalization.\"\"\"\n",
        "        num_features = X.shape[1]\n",
        "        delta = np.zeros(num_features)\n",
        "        global_norm, row_norms = calculate_normalization(X, weights)\n",
        "        observed_support = np.sum(X, axis=0)\n",
        "\n",
        "        for i in range(num_features):\n",
        "            expected_support = np.sum(np.exp(X[:, i] * weights[i]) / row_norms)  # Row-specific normalization\n",
        "            delta[i] = observed_support[i] - expected_support\n",
        "\n",
        "        weights += learning_rate * (delta - reg_strength * weights)\n",
        "        weights = np.clip(weights, -max_weight_value, max_weight_value)\n",
        "        return weights, global_norm\n",
        "\n",
        "    def scale_to_01(values):\n",
        "        \"\"\"Scale the input values to the range [0, 1].\"\"\"\n",
        "        min_val = np.min(values)\n",
        "        max_val = np.max(values)\n",
        "        return (values - min_val) / (max_val - min_val) if max_val > min_val else values\n",
        "\n",
        "    # Ensure X is a NumPy array\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
        "\n",
        "    # Initialize weights\n",
        "    num_features = X.shape[1]\n",
        "    weights = initialize_weights(num_features)\n",
        "    w_norm = None  # Track the global normalization parameter\n",
        "\n",
        "    # Apply the FIS algorithm to the matrix X\n",
        "    for iteration in range(num_iterations):\n",
        "        # Calculate log-likelihood and normalization\n",
        "        log_likelihood, w_norm = calculate_log_likelihood(X, weights)\n",
        "\n",
        "        # Scale the log-likelihood to [0, 1]\n",
        "        log_likelihood_scaled = scale_to_01(np.array([log_likelihood]))[0]\n",
        "\n",
        "        print(f\"Iteration {iteration}, Log-Likelihood (scaled): {log_likelihood_scaled}\")\n",
        "\n",
        "        # Update weights using auxiliary function and normalization\n",
        "        weights, w_norm = update_weights_using_auxiliary(X, weights, learning_rate, reg_strength)\n",
        "\n",
        "        # Scale weights to [0, 1]\n",
        "        weights = scale_to_01(weights)\n",
        "\n",
        "    print(\"Learned Weights (scaled):\", weights)\n",
        "    print(\"Final Normalization Parameter (w_norm):\", w_norm)\n",
        "\n",
        "    return weights, w_norm"
      ],
      "metadata": {
        "id": "b9JUvcKvjzVq"
      },
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Categorize->Dice->intervals->missing->binarize->transform->filter"
      ],
      "metadata": {
        "id": "IyUaNRCkkg2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_column(dataset, dependent_column_name, num_intervals):\n",
        "    # Fill NaN values with the mean of the column\n",
        "    dataset_filled = dataset.fillna(dataset[dependent_column_name].mean())\n",
        "\n",
        "    # Extract the specified dependent column\n",
        "    value_ag_filled = dataset_filled[dependent_column_name]\n",
        "\n",
        "    # Define the target range (0-1)\n",
        "    target_range = (0, 1)\n",
        "\n",
        "    # Scale to the target range [0, 1] using np.interp\n",
        "    value_ag_scaled_filled = np.interp(value_ag_filled, (value_ag_filled.min(), value_ag_filled.max()), target_range)\n",
        "\n",
        "    # Create intervals and assign labels for scaled values\n",
        "    data_inter_filled = pd.cut(value_ag_scaled_filled, bins=num_intervals, labels=[chr(ord('A') + i) for i in range(num_intervals)])\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'Category': data_inter_filled,\n",
        "        dependent_column_name: value_ag_scaled_filled\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to your Temporary_SXmat directory where X_interval files are saved\n",
        "save_path = '/content/drive/MyDrive/Temporary_SXmat'\n",
        "\n",
        "# Load your frequent_patterns dataframe (replace with your actual frequent patterns dataframe)\n",
        "# For example, assuming `support_df` is loaded from a CSV\n",
        "# support_df = pd.read_csv('path_to_support_patterns.csv')\n",
        "\n",
        "def calculate_dice_similarity(dataset_df, support_df):\n",
        "    \"\"\"\n",
        "    Calculate the Dice-Sørensen Coefficient (DSC) matrix for a given dataset and frequent patterns.\n",
        "\n",
        "    Args:\n",
        "        dataset_df (pd.DataFrame): The dataset dataframe.\n",
        "        support_df (pd.DataFrame): The frequent patterns dataframe (support patterns).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe containing the DSC matrix.\n",
        "    \"\"\"\n",
        "    unique_items = set()\n",
        "    for pattern in support_df.iloc[:, 0]:  # Assuming patterns are in the first column\n",
        "        pattern = str(pattern)\n",
        "        if pd.notna(pattern):\n",
        "            try:\n",
        "                items = pattern.split(', ')\n",
        "                unique_items.update(items)\n",
        "            except AttributeError:\n",
        "                print(f\"Skipping pattern {pattern} as it's not a valid string.\")\n",
        "\n",
        "    unique_item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
        "    bit_vector_length = len(unique_item_mapping)\n",
        "\n",
        "    # Convert dataset rows to bit vectors\n",
        "    def row_to_bit_vector(row, item_mapping, bit_length):\n",
        "        bit_vector = np.zeros(bit_length, dtype=int)\n",
        "        for col, value in row.items():\n",
        "            item = f\"{col}={value}\"\n",
        "            if item in item_mapping:\n",
        "                bit_vector[item_mapping[item]] = 1\n",
        "        return bit_vector\n",
        "\n",
        "    dataset_bit_vectors = np.array([row_to_bit_vector(row, unique_item_mapping, bit_vector_length) for _, row in dataset_df.iterrows()])\n",
        "\n",
        "    # Convert support patterns to bit vectors\n",
        "    def pattern_to_bit_vector(pattern, item_mapping, bit_length):\n",
        "        bit_vector = np.zeros(bit_length, dtype=int)\n",
        "        try:\n",
        "            pattern = str(pattern)\n",
        "            for item in pattern.split(', '):\n",
        "                if item in item_mapping:\n",
        "                    bit_vector[item_mapping[item]] = 1\n",
        "        except AttributeError:\n",
        "            print(f\"Skipping pattern {pattern} as it's not a valid string.\")\n",
        "        return bit_vector\n",
        "\n",
        "    support_pattern_bit_vectors = np.array([\n",
        "        pattern_to_bit_vector(pattern, unique_item_mapping, bit_vector_length)\n",
        "        for pattern in support_df.iloc[:, 0]  # Assuming patterns are in the first column\n",
        "        if pd.notna(pattern)\n",
        "    ])\n",
        "\n",
        "    # Initialize the DSC matrix with the correct shape\n",
        "    dsc_matrix = np.zeros((len(dataset_bit_vectors), len(support_pattern_bit_vectors)))\n",
        "\n",
        "    # Calculate DSC for each combination of dataset row and support pattern\n",
        "    for i, data_vector in enumerate(dataset_bit_vectors):\n",
        "        for j, pattern_vector in enumerate(support_pattern_bit_vectors):\n",
        "            intersection = np.sum(data_vector & pattern_vector)\n",
        "            union_cardinality = np.sum(data_vector) + np.sum(pattern_vector)\n",
        "            if union_cardinality > 0:\n",
        "                dsc_matrix[i, j] = (2 * intersection) / union_cardinality\n",
        "            else:\n",
        "                dsc_matrix[i, j] = 0\n",
        "\n",
        "    # Create a dataframe with the DSC matrix\n",
        "    dsc_df = pd.DataFrame(dsc_matrix, columns=[f'Pattern_{k}' for k in range(len(support_pattern_bit_vectors))])\n",
        "    return dsc_df\n",
        "\n",
        "def create_intervals_df(data_frame, column_name, num_intervals):\n",
        "    # Extract the specified column\n",
        "    value_ag = data_frame[column_name]\n",
        "\n",
        "    #print(\"Extracted column values:\")\n",
        "    #print(value_ag)\n",
        "\n",
        "    # Check if the column contains numeric data\n",
        "    if not pd.api.types.is_numeric_dtype(value_ag):\n",
        "        raise ValueError(f\"The '{column_name}' column must contain numeric data.\")\n",
        "\n",
        "    # Check for missing values in the column\n",
        "    if value_ag.isnull().any():\n",
        "        raise ValueError(f\"The '{column_name}' column contains missing values. Please handle them before processing.\")\n",
        "\n",
        "    # Check if scaling and clipping is necessary\n",
        "    if value_ag.min() < 0 or value_ag.max() > 1:\n",
        "        # Rescale to the range [0, 1]\n",
        "        value_ag_scaled = (value_ag - value_ag.min()) / (value_ag.max() - value_ag.min())\n",
        "\n",
        "        # Clip values to the range [0, 1]\n",
        "        value_ag_scaled_clipped = np.clip(value_ag_scaled, 0, 1)\n",
        "    else:\n",
        "        # No need to rescale or clip\n",
        "        value_ag_scaled_clipped = value_ag\n",
        "\n",
        "    #print(\"Scaled and clipped column values:\")\n",
        "    #print(value_ag_scaled_clipped)\n",
        "\n",
        "    # Calculate bin edges dynamically within the [0, 1] range\n",
        "    bin_edges = np.linspace(0, 1, num=num_intervals + 1)\n",
        "\n",
        "    # Create intervals and assign labels for clipped values\n",
        "    data_inter_clipped = pd.cut(value_ag_scaled_clipped.values, bins=bin_edges, labels=[chr(ord('A') + i) for i in range(len(bin_edges) - 1)])\n",
        "\n",
        "    #print(\"Computed intervals:\")\n",
        "    #print(data_inter_clipped)\n",
        "\n",
        "    # Store intervals in a DataFrame\n",
        "    intervals_df = pd.DataFrame({\n",
        "        'Category': data_inter_clipped.categories,\n",
        "        'Min_Value': [value_ag_scaled_clipped[data_inter_clipped == category].min() for category in data_inter_clipped.categories],\n",
        "        'Max_Value': [value_ag_scaled_clipped[data_inter_clipped == category].max() for category in data_inter_clipped.categories]\n",
        "    })\n",
        "\n",
        "    return intervals_df\n",
        "\n",
        "\n",
        "\n",
        "def handle_missing_values(dataset):\n",
        "    # Check if there are missing values in the dataset\n",
        "    if dataset.isnull().any().any():\n",
        "        # Identify numerical and categorical columns\n",
        "        numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
        "        categorical_cols = dataset.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "        # Impute numerical columns with mean\n",
        "        imputer_numeric = SimpleImputer(strategy='mean')\n",
        "        dataset[numerical_cols] = imputer_numeric.fit_transform(dataset[numerical_cols])\n",
        "\n",
        "        # Impute categorical columns with the most frequent value (mode)\n",
        "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "        dataset[categorical_cols] = imputer_categorical.fit_transform(dataset[categorical_cols])\n",
        "\n",
        "        return dataset\n",
        "    else:\n",
        "        # If no missing values, return the original dataset\n",
        "        return dataset\n",
        "\n",
        "def binarize_dataframe(input_df, num_bins):\n",
        "    df = pd.DataFrame(input_df)\n",
        "    df_binarized = pd.DataFrame()\n",
        "\n",
        "    for column in df.columns:\n",
        "        column_name = column+'_binarized'\n",
        "        bins = pd.qcut(df[column], q=num_bins, labels=False, duplicates='drop')\n",
        "        df_binarized[column_name] = (bins == bins.max()).astype(int)\n",
        "\n",
        "    return df_binarized\n",
        "\n",
        "def transform_matrix_dice(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "    print(transformed_matrix)\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.40] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.40] = 0\n",
        "    return transformed_matrix\n",
        "\n",
        "def transform_matrix_dice(matrix):\n",
        "    # Create a copy of the input matrix\n",
        "    transformed_matrix = matrix.copy()\n",
        "    #print(\"Inside Transformed\")\n",
        "    # Apply the transformation\n",
        "    transformed_matrix[transformed_matrix > 0.40] = 1\n",
        "    transformed_matrix[transformed_matrix <= 0.40] = 0\n",
        "    return transformed_matrix\n",
        "\n",
        "\n",
        "def filter_data_for_interval(df, category_col, category):\n",
        "    # Filter data for a specific interval category\n",
        "    return df[df[category_col] == category]\n",
        "\n",
        "def transform_matrix_for_interval(X_interval, support_df):\n",
        "    jaccard_result = calculate_dice_similarity(X_interval, support_df)\n",
        "    #print(\"Inside Transform\")\n",
        "    #print(jaccard_result.shape)\n",
        "    transformed_matrix = transform_matrix_dice(jaccard_result)\n",
        "    print(transformed_matrix.shape)\n",
        "    return transformed_matrix"
      ],
      "metadata": {
        "id": "DHxtFgMmj9yF"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bayesian model for FP(with interestingness)"
      ],
      "metadata": {
        "id": "EHFStLBXk5nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "def find_separate_itemsets_for_measures(dataset_path, freq_pattern_path, drop_column=None, max_iterations=100, convergence_threshold=1e-4, thresholds=None):\n",
        "    # Load dataset and frequent patterns\n",
        "    dataset = pd.read_csv(dataset_path)\n",
        "    freq_patterns = pd.read_csv(freq_pattern_path, sep='\\t')\n",
        "\n",
        "    # Default thresholds if not provided\n",
        "    if thresholds is None:\n",
        "        thresholds = {\n",
        "            \"support\": 0.01,\n",
        "            \"confidence\": 0.000000000001,\n",
        "            \"lift\": 0.01,\n",
        "            \"leverage\": 0.0,\n",
        "            \"jaccard\": 0.5,\n",
        "            \"cosine\": 0.5\n",
        "        }\n",
        "\n",
        "    # Preprocess dataset - drop column if specified\n",
        "    if drop_column:\n",
        "        X = dataset.drop(columns=[drop_column])\n",
        "    else:\n",
        "        X = dataset  # If no column is specified to drop, use the full dataset\n",
        "\n",
        "    binary_data = Binarizer().fit_transform(X)\n",
        "\n",
        "    # Initialize itemset probabilities pi_s for each pattern\n",
        "    pi = np.ones(len(freq_patterns))  # Initialize all itemsets with probability 1 (uniform prior)\n",
        "\n",
        "    # Helper functions for Bayesian Mixture Model\n",
        "\n",
        "    def e_step(data, pi):\n",
        "        epsilon = 1e-10  # Small value to avoid log of zero\n",
        "        log_resp = np.zeros((data.shape[0], len(pi)))  # Initialize the responsibilities\n",
        "        for i, pattern in enumerate(freq_patterns['Itemsets']):\n",
        "            # Parse the pattern to get item columns\n",
        "            pattern_items = [item.split('=')[0] for item in pattern.split()]\n",
        "            pattern_columns = [col for col in X.columns if col in pattern_items]\n",
        "            if not pattern_columns:\n",
        "                continue  # Skip patterns that don't match any columns\n",
        "\n",
        "            # Indicator function: 1 if itemset is present, 0 if not\n",
        "            si = np.zeros(data.shape[0])\n",
        "            for j, row in enumerate(data):\n",
        "                si[j] = 1 if all(row[X.columns.get_loc(col)] for col in pattern_columns) else 0\n",
        "\n",
        "            # Calculate log-probability of the data under each itemset\n",
        "            log_prob = si * np.log(pi[i] + epsilon) + (1 - si) * np.log(1 - pi[i] + epsilon)\n",
        "            log_resp[:, i] = log_prob\n",
        "\n",
        "        # Normalize the responsibilities across all itemsets (log-sum-exp trick)\n",
        "        log_resp -= logsumexp(log_resp, axis=1)[:, np.newaxis]\n",
        "        return np.exp(log_resp)\n",
        "\n",
        "    def m_step(data, responsibilities):\n",
        "        N, D = data.shape\n",
        "        Nk = responsibilities.sum(axis=0)\n",
        "        pi_new = Nk / N  # Update component priors (probabilities of each itemset)\n",
        "        return pi_new\n",
        "\n",
        "    # Run EM algorithm until convergence or maximum iterations\n",
        "    for iteration in range(max_iterations):\n",
        "        prev_pi = pi.copy()\n",
        "\n",
        "        # Perform E-step and M-step\n",
        "        responsibilities = e_step(binary_data, pi)\n",
        "        pi = m_step(binary_data, responsibilities)\n",
        "\n",
        "        # Check for convergence\n",
        "        delta_pi = np.linalg.norm(pi - prev_pi)\n",
        "        if delta_pi < convergence_threshold:\n",
        "            print(f\"Converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"Reached maximum iterations without convergence.\")\n",
        "\n",
        "    # Initialize separate results for each interestingness measure\n",
        "    support_results = []\n",
        "    confidence_results = []\n",
        "    lift_results = []\n",
        "    leverage_results = []\n",
        "    jaccard_results = []\n",
        "    cosine_results = []\n",
        "\n",
        "    # Calculate interestingness measures for each pattern\n",
        "    for _, row in freq_patterns.iterrows():\n",
        "        pattern_string = row['Itemsets']\n",
        "\n",
        "        # Parse the pattern string into column names and values\n",
        "        pattern_items = [\n",
        "            item.strip().split('=')[0]  # Extract the attribute (column name)\n",
        "            for item in pattern_string.split()  # Split by spaces\n",
        "            if '=' in item  # Ignore parts without '=' (e.g., \"count=8\")\n",
        "        ]\n",
        "\n",
        "        # Match column names in the dataset\n",
        "        pattern_columns = [col for col in X.columns if col in pattern_items]\n",
        "        if not pattern_columns:\n",
        "            continue  # Skip patterns with no matching columns\n",
        "\n",
        "        # Calculate support\n",
        "        support_count = (binary_data[:, [X.columns.get_loc(col) for col in pattern_columns]].sum(axis=1)\n",
        "                         == len(pattern_columns)).sum()\n",
        "        support = support_count / binary_data.shape[0]\n",
        "\n",
        "        # Confidence (Assuming a simple rule A -> B)\n",
        "        if len(pattern_columns) > 1:\n",
        "            A, B = pattern_columns[:-1], pattern_columns[-1]\n",
        "            support_A = (binary_data[:, [X.columns.get_loc(col) for col in A]].sum(axis=1) == len(A)).sum() / binary_data.shape[0]\n",
        "            confidence = support / (support_A + 1e-10)\n",
        "        else:\n",
        "            confidence = np.nan\n",
        "\n",
        "        # Lift\n",
        "        if len(pattern_columns) > 1:\n",
        "            support_B = (binary_data[:, X.columns.get_loc(B)] == 1).sum() / binary_data.shape[0]\n",
        "            lift = support / (support_A * support_B + 1e-10)\n",
        "        else:\n",
        "            lift = np.nan\n",
        "\n",
        "        # Leverage\n",
        "        leverage = support - (support_A * support_B) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Jaccard Index\n",
        "        jaccard = support / (support_A + support_B - support) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Cosine Similarity\n",
        "        cosine = support / (support_A * support_B + 1e-10) if len(pattern_columns) > 1 else np.nan\n",
        "\n",
        "        # Store results if they exceed the threshold\n",
        "        if support >= thresholds['support']:\n",
        "            support_results.append({\"Pattern\": pattern_string, \"Support\": support})\n",
        "        if confidence >= thresholds['confidence']:\n",
        "            confidence_results.append({\"Pattern\": pattern_string, \"Confidence\": confidence})\n",
        "        if lift >= thresholds['lift']:\n",
        "            lift_results.append({\"Pattern\": pattern_string, \"Lift\": lift})\n",
        "        if leverage >= thresholds['leverage']:\n",
        "            leverage_results.append({\"Pattern\": pattern_string, \"Leverage\": leverage})\n",
        "        if jaccard >= thresholds['jaccard']:\n",
        "            jaccard_results.append({\"Pattern\": pattern_string, \"Jaccard\": jaccard})\n",
        "        if cosine >= thresholds['cosine']:\n",
        "            cosine_results.append({\"Pattern\": pattern_string, \"Cosine\": cosine})\n",
        "\n",
        "    # Convert results to DataFrames\n",
        "    support_df = pd.DataFrame(support_results)\n",
        "    confidence_df = pd.DataFrame(confidence_results)\n",
        "    lift_df = pd.DataFrame(lift_results)\n",
        "    leverage_df = pd.DataFrame(leverage_results)\n",
        "    jaccard_df = pd.DataFrame(jaccard_results)\n",
        "    cosine_df = pd.DataFrame(cosine_results)\n",
        "\n",
        "    return support_df, confidence_df, lift_df, leverage_df, jaccard_df, cosine_df"
      ],
      "metadata": {
        "id": "iOGOD8xpk1ue"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Density->process_interval"
      ],
      "metadata": {
        "id": "_57fOy5snJ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_density(matrix):\n",
        "    num_ones = matrix.nnz if isinstance(matrix, csr_matrix) else np.count_nonzero(matrix)\n",
        "    total_elements = matrix.shape[0] * matrix.shape[1]\n",
        "    density = num_ones / total_elements\n",
        "    return density\n",
        "\n",
        "# def process_interval(row):\n",
        "#     category = row['Category']\n",
        "#     interval_min = row['Min_Value']\n",
        "#     interval_max = row['Max_Value']\n",
        "\n",
        "#     filtered_data = filter_data_for_interval(Data_discretized, 'Category', category)\n",
        "#     if not filtered_data.empty:\n",
        "#         X_interval = filtered_data.drop(columns=['Category'])\n",
        "#         print(X_interval.shape)\n",
        "#         val = calculate_dice_similarity(X_interval, support_df)\n",
        "#         #print(val.shape)\n",
        "#         val1 = transform_matrix_for_interval(X_interval, support_df)\n",
        "#         SXmat = val1\n",
        "#         SXmat = pd.DataFrame(SXmat)\n",
        "#         # Calculate Density\n",
        "#         density = calculate_density(SXmat)\n",
        "#         print(f\"Density of incidence matrix for this interval (Category {category}): {density}\")\n",
        "#         weights, w_norm = fis_algorithm_with_auxiliary(SXmat)\n",
        "#         return category, {'weights': weights, 'wnorm': w_norm}\n",
        "#     else:\n",
        "#         print(f\"No data for interval {category}\")\n",
        "#         return category, None\n",
        "\n",
        "def process_interval(row):\n",
        "    category = row['Category']\n",
        "    interval_min = row['Min_Value']\n",
        "    interval_max = row['Max_Value']\n",
        "\n",
        "    filtered_data = filter_data_for_interval(Data_discretized, 'Category', category)\n",
        "    if not filtered_data.empty:\n",
        "        X_interval = filtered_data.drop(columns=['Category'])\n",
        "        print(X_interval.shape)\n",
        "        val = calculate_dice_similarity_demo(X_interval, support_df)\n",
        "        print(val.shape)\n",
        "        val1 = transform_matrix_for_interval(X_interval, support_df)\n",
        "        SXmat = val1\n",
        "        SXmat = pd.DataFrame(SXmat)\n",
        "        print(SXmat.head(1))\n",
        "        df_SXmat = pd.DataFrame(SXmat)\n",
        "        sxmat_file_path = os.path.join(save_path, f'SXmat_Category_{category}.csv')\n",
        "        df_SXmat.to_csv(sxmat_file_path, index=False)\n",
        "        print(f\"SXmat for Category {category} saved at {sxmat_file_path}\")\n",
        "        # Calculate Density\n",
        "        density = calculate_density(SXmat)\n",
        "        print(f\"Density of incidence matrix for this interval (Category {category}): {density}\")\n",
        "        weights, w_norm = fis_algorithm_with_auxiliary(SXmat)\n",
        "        print(weights.shape)\n",
        "        #print(wnorm.shape)\n",
        "        return category, {'weights': weights, 'wnorm': w_norm}\n",
        "    else:\n",
        "        print(f\"No data for interval {category}\")\n",
        "        return category, None\n",
        "\n"
      ],
      "metadata": {
        "id": "FA_HMh41lvL0"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process_test_dataset"
      ],
      "metadata": {
        "id": "QHhgP_pDvL_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_data(Data_test, support_df, categories_weights, constant):\n",
        "    # Create a temporary DataFrame as a copy of Data_test\n",
        "    Data_test_temp = Data_test.copy()\n",
        "\n",
        "    # Iterate through each row in Data_test_temp\n",
        "    for index, row in Data_test_temp.iterrows():\n",
        "        # Convert the row to a DataFrame for compatibility with transform_matrix_for_interval\n",
        "        row_df = pd.DataFrame([row])\n",
        "\n",
        "        # Calculate the Jaccard/Dice similarity for the current row with support_df\n",
        "        jaccard_result = calculate_dice_similarity_demo(row_df, support_df)\n",
        "\n",
        "        # Directly apply binarization with a threshold of 0.4\n",
        "        transformed_row = jaccard_result.copy()\n",
        "        transformed_row[transformed_row > 0.4] = 1\n",
        "        transformed_row[transformed_row <= 0.4] = 0\n",
        "\n",
        "        # Flatten the transformed_row to ensure it is 1D (1, 87) -> (87,)\n",
        "        transformed_row_flat = transformed_row.values.flatten()\n",
        "\n",
        "        max_value = 0.0  # Initialize the max value\n",
        "        predicted_category = None  # Store the category with the highest value\n",
        "\n",
        "        # Iterate over each category and calculate the weighted product\n",
        "        for category, params in categories_weights.items():\n",
        "            # Ensure the structure is valid\n",
        "            if 'weights' in params and 'wnorm' in params:\n",
        "                weights = params['weights']  # Extract weights for the category\n",
        "                wnorm = params['wnorm']      # Extract normalization value\n",
        "\n",
        "                # Reshape weights to (1, 87) to match the transformed_row shape (1, 87)\n",
        "                weights_reshaped = weights.reshape(1, -1)\n",
        "\n",
        "                # Ensure both weights and transformed_row are compatible (both should be (1, 87))\n",
        "                if weights_reshaped.shape != transformed_row.shape:\n",
        "                    print(f\"Shape mismatch between weights and transformed_row for category {category}\")\n",
        "                    continue\n",
        "\n",
        "                product = weights_reshaped * transformed_row  # Element-wise multiplication\n",
        "                # Sum the product over all columns (axis=1 sums across columns)\n",
        "                product_sum = product.sum(axis=1)  # This will give a single value\n",
        "                # Calculate the final product with the normalization value and constant\n",
        "                final_product = (product_sum + wnorm) * constant\n",
        "                final_product = final_product.item()\n",
        "                # Check if the current product is higher than max_value\n",
        "                if final_product > max_value:\n",
        "                    max_value = final_product\n",
        "                    predicted_category = category  # Store the category with the highest product\n",
        "\n",
        "        # After processing all categories for the current row, add the predicted category to Data_test_temp\n",
        "        Data_test_temp.loc[index, 'predicted_category'] = predicted_category\n",
        "\n",
        "    # Return the updated DataFrame with the predicted category column\n",
        "    return Data_test_temp"
      ],
      "metadata": {
        "id": "jZBVMhOYvFI9"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RMSE calculation"
      ],
      "metadata": {
        "id": "Q5Dl1zcjvsZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_rmse_category(X_test, intervals_df, updated_data):\n",
        "    sum_squared_diff = 0.0\n",
        "    sum_absolute_diff = 0.0\n",
        "    actual_values = []\n",
        "    predicted_values = []\n",
        "\n",
        "    # Iterate through the test dataset to calculate RMSE, MAE, and collect values for PCC\n",
        "    for i, row in X_test.iterrows():\n",
        "        actual_category = row['Category']  # Actual category\n",
        "        predicted_category = updated_data.loc[i, 'predicted_category']  # Predicted category from updated_data\n",
        "        #print(f\"Predicted Category: {predicted_category}\")\n",
        "\n",
        "        # Get the actual 'Max_Value' based on the actual category\n",
        "        actual_max_value = intervals_df[intervals_df['Category'] == actual_category]['Max_Value'].values\n",
        "\n",
        "        # Check if predicted_category exists in intervals_df\n",
        "        if predicted_category in intervals_df['Category'].values:\n",
        "            predicted_max_value = intervals_df[intervals_df['Category'] == predicted_category]['Max_Value'].values\n",
        "        else:\n",
        "            # Handle case where predicted category is not found in intervals_df\n",
        "            #print(f\"Predicted category '{predicted_category}' not found in intervals_df.\")\n",
        "            predicted_max_value = np.nan  # Set predicted_max_value to NaN or handle it as needed\n",
        "\n",
        "        #print(f\"Actual Max Value: {actual_max_value}, Predicted Max Value: {predicted_max_value}\")\n",
        "\n",
        "        # Ensure the actual max value exists and predicted max value is not NaN\n",
        "        if len(actual_max_value) > 0:\n",
        "            actual_max_value = actual_max_value[0]\n",
        "\n",
        "            # Handle NaN values in actual_max_value or predicted_max_value\n",
        "            if np.isnan(actual_max_value) or np.isnan(predicted_max_value):\n",
        "                #print(f\"Skipping pair due to NaN value (Actual: {actual_max_value}, Predicted: {predicted_max_value})\")\n",
        "\n",
        "                # Option 1: Set difference to 0 if both are NaN\n",
        "                squared_diff = 0\n",
        "                absolute_diff = 0\n",
        "            else:\n",
        "                # Calculate squared difference for RMSE\n",
        "                squared_diff = (predicted_max_value - actual_max_value) ** 2\n",
        "                absolute_diff = abs(predicted_max_value - actual_max_value)\n",
        "\n",
        "            sum_squared_diff += squared_diff\n",
        "            sum_absolute_diff += absolute_diff\n",
        "\n",
        "            # Collect values for PCC calculation\n",
        "            actual_values.append(actual_max_value)\n",
        "            predicted_values.append(predicted_max_value)\n",
        "\n",
        "    # Ensure we have data to calculate the metrics\n",
        "    if len(X_test) > 0:\n",
        "        # Calculate RMSE\n",
        "        mean_squared_error = sum_squared_diff / len(X_test)\n",
        "        rmse = np.sqrt(mean_squared_error).item()  # Convert numpy array to scalar\n",
        "\n",
        "        # Calculate MAE\n",
        "        mae = (sum_absolute_diff / len(X_test)).item()  # Convert numpy array to scalar\n",
        "\n",
        "        return rmse, mae\n",
        "\n",
        "    # Return None if there are no rows in X_test\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "jYsZvnKJvrZN"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check dataset"
      ],
      "metadata": {
        "id": "hKKKLS_xBUq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load Dataset while skipping the 'datetime' column\n",
        "dataset = \"/content/drive/MyDrive/concrete_updated.csv\"\n",
        "concrete = pd.read_csv(dataset)\n",
        "concrete.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "HyIK3REIBXP9",
        "outputId": "617d973b-bdb0-4590-b128-80e38dac3b6f"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   No  Cement   Slag  Fly ash  Water    SP  Coarse Aggr.  Fine Aggr.  \\\n",
              "0   1   273.0   82.0    105.0  210.0   9.0         904.0       680.0   \n",
              "1   2   163.0  149.0    191.0  180.0  12.0         843.0       746.0   \n",
              "\n",
              "   SLUMP(cm)  FLOW(cm)  Strength  \n",
              "0       23.0      62.0     34.99  \n",
              "1        0.0      20.0     41.14  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87064ad7-30d4-49f1-b0f9-8d3b3ba8d57f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Cement</th>\n",
              "      <th>Slag</th>\n",
              "      <th>Fly ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>SP</th>\n",
              "      <th>Coarse Aggr.</th>\n",
              "      <th>Fine Aggr.</th>\n",
              "      <th>SLUMP(cm)</th>\n",
              "      <th>FLOW(cm)</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>904.0</td>\n",
              "      <td>680.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>34.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>163.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>191.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>843.0</td>\n",
              "      <td>746.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>41.14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87064ad7-30d4-49f1-b0f9-8d3b3ba8d57f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87064ad7-30d4-49f1-b0f9-8d3b3ba8d57f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87064ad7-30d4-49f1-b0f9-8d3b3ba8d57f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-adbc6fe3-e589-42ac-94bf-50fef4891114\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-adbc6fe3-e589-42ac-94bf-50fef4891114')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-adbc6fe3-e589-42ac-94bf-50fef4891114 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "concrete",
              "summary": "{\n  \"name\": \"concrete\",\n  \"rows\": 104,\n  \"fields\": [\n    {\n      \"column\": \"No\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 1,\n        \"max\": 103,\n        \"num_unique_values\": 103,\n        \"samples\": [\n          31,\n          68,\n          63\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 79.35321560426127,\n        \"min\": 137.0,\n        \"max\": 374.0,\n        \"num_unique_values\": 80,\n        \"samples\": [\n          165.0,\n          273.0,\n          314.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Slag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 60.64978369405715,\n        \"min\": 0.0,\n        \"max\": 193.0,\n        \"num_unique_values\": 63,\n        \"samples\": [\n          40.9,\n          153.4,\n          82.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fly ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85.31228781703457,\n        \"min\": 0.0,\n        \"max\": 260.0,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          105.0,\n          178.0,\n          111.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Water\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20.140497919863968,\n        \"min\": 160.0,\n        \"max\": 240.0,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          174.0,\n          210.0,\n          193.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.7958012902151914,\n        \"min\": 4.4,\n        \"max\": 19.0,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          7.7,\n          4.7,\n          6.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coarse Aggr.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 88.48227545364563,\n        \"min\": 708.0,\n        \"max\": 1049.9,\n        \"num_unique_values\": 92,\n        \"samples\": [\n          879.0,\n          883.0,\n          881.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fine Aggr.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63.059965978312725,\n        \"min\": 640.6,\n        \"max\": 902.0,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          815.0,\n          749.0,\n          799.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SLUMP(cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.774224974231723,\n        \"min\": 0.0,\n        \"max\": 29.0,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          2.0,\n          21.25,\n          20.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLOW(cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.703359949740676,\n        \"min\": 20.0,\n        \"max\": 78.0,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          75.0,\n          39.0,\n          27.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Strength\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.8993504420709675,\n        \"min\": 17.19,\n        \"max\": 58.53,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          52.65,\n          34.99,\n          18.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calling the function with dataset input"
      ],
      "metadata": {
        "id": "1mCv4_NMnWvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load Dataset while skipping the 'datetime' column\n",
        "dataset = \"/content/drive/MyDrive/bolts.csv\"\n",
        "concrete = pd.read_csv(dataset)\n",
        "dependent = \"T20BOLT\"\n",
        "concrete_temp = concrete.copy()\n",
        "X = concrete_temp.drop(dependent, axis=1).copy()\n",
        "y = concrete[dependent]\n",
        "# Usage example:\n",
        "dataset_path = \"/content/drive/MyDrive/bolts.csv\"\n",
        "freq_pattern_path = '/content/drive/MyDrive/AprioriOutput/closed_itemsets_bolts.csv'\n",
        "drop_column = 'T20BOLT'  # Example of the column to drop\n",
        "support_df, confidence_df, lift_df, leverage_df, jaccard_df, cosine_df = find_separate_itemsets_for_measures(dataset_path, freq_pattern_path, drop_column=drop_column)\n",
        "support_df = support_df.drop(columns=['Support'])\n",
        "# Handle Missing Values\n",
        "X = handle_missing_values(X)\n",
        "# Binarize DataFrame\n",
        "num_bins = 15\n",
        "df_binarized = binarize_dataframe(pd.DataFrame(concrete), num_bins)\n",
        "# Create Intervals DataFrame\n",
        "num_intervals = 4\n",
        "intervals_df = create_intervals_df(concrete, dependent, num_intervals)\n",
        "print(intervals_df)\n",
        "# Categorize Column\n",
        "result_df = categorize_column(concrete, dependent, num_intervals)\n",
        "P_c = 1 / num_intervals\n",
        "P_x = 1 / concrete.shape[1]\n",
        "constant = P_c / P_x\n",
        "Data_discretized = pd.read_csv('/content/drive/MyDrive/Discretized_datasets/discretized_bolts.csv')\n",
        "Data_discretized['Category'] = result_df['Category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhhBhVfcnOzF",
        "outputId": "5bf3d036-cf6f-4bbb-c689-2483a3127a47"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 15 iterations.\n",
            "  Category  Min_Value  Max_Value\n",
            "0        A   0.006828   0.237991\n",
            "1        B   0.317240   0.338942\n",
            "2        C   0.615216   0.746159\n",
            "3        D   0.776030   1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Jaccard Similarity and Train GIS for Each Interval\n",
        "categories_weights = {}\n",
        "categories = Data_discretized['Category'].unique()\n",
        "# Use ThreadPoolExecutor to run computations in parallel\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Create a list of futures\n",
        "    futures = {executor.submit(process_interval, row): row for _, row in intervals_df.iterrows()}\n",
        "    # Process completed futures\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        category, weights_wnorm = future.result()\n",
        "        if weights_wnorm is not None:\n",
        "            categories_weights[category] = weights_wnorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2w61PLCnOlK",
        "outputId": "4fd79cec-21df-4aae-bfd8-50ac0d81dbe3"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 7)\n",
            "(26, 7)\n",
            "(3, 7)\n",
            "(9, 7)\n",
            "(3, 199)(3, 199)\n",
            "\n",
            "(3, 199)\n",
            "(9, 199)(3, 199)\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_189  Pattern_190  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          0.0          0.0   \n",
            "\n",
            "   Pattern_191  Pattern_192  Pattern_193  Pattern_194  Pattern_195  \\\n",
            "0          0.0          0.0          0.0          0.0          0.0   \n",
            "\n",
            "   Pattern_196  Pattern_197  Pattern_198  \n",
            "0          0.0          0.0          0.0  \n",
            "\n",
            "[1 rows x 199 columns]\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_189  Pattern_190  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          0.0   \n",
            "\n",
            "   Pattern_191  Pattern_192  Pattern_193  Pattern_194  Pattern_195  \\\n",
            "0          0.0          1.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_196  Pattern_197  Pattern_198  \n",
            "0          1.0          0.0          0.0  \n",
            "\n",
            "[1 rows x 199 columns]\n",
            "\n",
            "SXmat for Category B saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_B.csv\n",
            "Density of incidence matrix for this interval (Category B): 0.23283082077051925\n",
            "Iteration 0, Log-Likelihood (scaled): 22.81033891348799\n",
            "Iteration 1, Log-Likelihood (scaled): 57.13539383412265\n",
            "Iteration 2, Log-Likelihood (scaled): 57.292363198949644SXmat for Category C saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_C.csv\n",
            "Density of incidence matrix for this interval (Category C): 0.23618090452261306\n",
            "Iteration 0, Log-Likelihood (scaled): 5.551115123125783e-17\n",
            "Iteration 1, Log-Likelihood (scaled): 97.29414253455826\n",
            "Iteration 2, Log-Likelihood (scaled): 97.50167461835193\n",
            "\n",
            "Iteration 3, Log-Likelihood (scaled): 57.4491751808505\n",
            "Iteration 4, Log-Likelihood (scaled): 57.60582993574567\n",
            "Iteration 5, Log-Likelihood (scaled): 57.76232761940123\n",
            "Iteration 3, Log-Likelihood (scaled): 97.70941245278733\n",
            "Iteration 4, Log-Likelihood (scaled): 97.9173562374786\n",
            "Iteration 5, Log-Likelihood (scaled): 98.12550617221393\n",
            "Iteration 6, Log-Likelihood (scaled): 57.91866838742917\n",
            "Iteration 7, Log-Likelihood (scaled): 58.07485239528748Iteration 6, Log-Likelihood (scaled): 98.33386245695591\n",
            "Iteration 8, Log-Likelihood (scaled): 58.23087979828036\n",
            "Iteration 9, Log-Likelihood (scaled): 58.38675075155839\n",
            "\n",
            "Iteration 7, Log-Likelihood (scaled): 98.54242529184134\n",
            "Iteration 8, Log-Likelihood (scaled): 98.75119487718155Iteration 10, Log-Likelihood (scaled): 58.54246541011853\n",
            "Iteration 11, Log-Likelihood (scaled): 58.6980239288045(9, 199)\n",
            "\n",
            "Iteration 9, Log-Likelihood (scaled): 98.96017141346215(26, 199)\n",
            "\n",
            "Iteration 12, Log-Likelihood (scaled): 58.85342646230679\n",
            "Iteration 13, Log-Likelihood (scaled): 59.00867316516286\n",
            "\n",
            "Iteration 10, Log-Likelihood (scaled): 99.16935510134317\n",
            "Iteration 11, Log-Likelihood (scaled): 99.37874614165929\n",
            "Iteration 12, Log-Likelihood (scaled): 99.58834473541958\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_189  Pattern_190  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          0.0          0.0   \n",
            "\n",
            "   Pattern_191  Pattern_192  Pattern_193  Pattern_194  Pattern_195  \\\n",
            "0          0.0          0.0          0.0          0.0          0.0   \n",
            "\n",
            "   Pattern_196  Pattern_197  Pattern_198  \n",
            "0          0.0          0.0          0.0  \n",
            "\n",
            "[1 rows x 199 columns]Iteration 14, Log-Likelihood (scaled): 59.16376419175728\n",
            "Iteration 15, Log-Likelihood (scaled): 59.31869969632185\n",
            "\n",
            "Iteration 13, Log-Likelihood (scaled): 99.79815108380795Iteration 16, Log-Likelihood (scaled): 59.473479832935844\n",
            "Iteration 14, Log-Likelihood (scaled): 100.00816538818273\n",
            "Iteration 17, Log-Likelihood (scaled): 59.628104755526095\n",
            "Iteration 15, Log-Likelihood (scaled): 100.21838785007708\n",
            "Iteration 18, Log-Likelihood (scaled): 59.7825746178672\n",
            "Iteration 16, Log-Likelihood (scaled): 100.42881867119883\n",
            "Iteration 17, Log-Likelihood (scaled): 100.63945805343062\n",
            "Iteration 19, Log-Likelihood (scaled): 59.93688957358157\n",
            "Iteration 20, Log-Likelihood (scaled): 60.09104977613971\n",
            "Iteration 18, Log-Likelihood (scaled): 100.85030619882997\n",
            "Iteration 21, Log-Likelihood (scaled): 60.24505537886034\n",
            "\n",
            "Iteration 19, Log-Likelihood (scaled): 101.06136330962923Iteration 22, Log-Likelihood (scaled): 60.39890653491043\n",
            "Iteration 20, Log-Likelihood (scaled): 101.27262958823565\n",
            "Iteration 23, Log-Likelihood (scaled): 60.55260339730553\n",
            "Iteration 21, Log-Likelihood (scaled): 101.173571974039\n",
            "Iteration 24, Log-Likelihood (scaled): 60.70614611890984SXmat for Category D saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_D.csv\n",
            "Density of incidence matrix for this interval (Category D): 0.18313791178112787\n",
            "Iteration 0, Log-Likelihood (scaled): 0.0\n",
            "\n",
            "Iteration 25, Log-Likelihood (scaled): 60.859534852436305\n",
            "Iteration 22, Log-Likelihood (scaled): 101.026812291813Iteration 1, Log-Likelihood (scaled): inf\n",
            "Iteration 2, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 23, Log-Likelihood (scaled): 100.88034489181499\n",
            "Iteration 26, Log-Likelihood (scaled): 61.01276975044688\n",
            "Iteration 27, Log-Likelihood (scaled): 61.165850965352625\n",
            "Iteration 24, Log-Likelihood (scaled): 100.73416919068862Iteration 3, Log-Likelihood (scaled): inf\n",
            "Iteration 25, Log-Likelihood (scaled): 100.58828460624886\n",
            "\n",
            "Iteration 28, Log-Likelihood (scaled): 61.31877864941384\n",
            "Iteration 26, Log-Likelihood (scaled): 100.44269055747941"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-308-527064c62d59>:26: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood_per_row = X @ weights - np.log(row_norms)\n",
            "<ipython-input-308-527064c62d59>:46: RuntimeWarning: divide by zero encountered in divide\n",
            "  expected_support = np.sum(np.exp(X[:, i] * weights[i]) / row_norms)  # Row-specific normalization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 29, Log-Likelihood (scaled): 61.471552954740226\n",
            "Iteration 4, Log-Likelihood (scaled): inf\n",
            "Iteration 27, Log-Likelihood (scaled): 100.29738646453049\n",
            "Iteration 30, Log-Likelihood (scaled): 61.61311468017795\n",
            "\n",
            "Iteration 5, Log-Likelihood (scaled): inf\n",
            "Iteration 31, Log-Likelihood (scaled): 61.685619999219604\n",
            "\n",
            "Iteration 28, Log-Likelihood (scaled): 100.15237174871632Iteration 6, Log-Likelihood (scaled): inf\n",
            "Iteration 29, Log-Likelihood (scaled): 100.00764583251285Iteration 32, Log-Likelihood (scaled): 61.7579804931359\n",
            "Iteration 30, Log-Likelihood (scaled): 99.8632081395552\n",
            "Iteration 33, Log-Likelihood (scaled): 61.83019645034709\n",
            "Iteration 7, Log-Likelihood (scaled): inf\n",
            "Iteration 34, Log-Likelihood (scaled): 61.902268158701126\n",
            "Iteration 31, Log-Likelihood (scaled): 99.71905809463556\n",
            "Iteration 8, Log-Likelihood (scaled): inf\n",
            "Iteration 32, Log-Likelihood (scaled): 99.57519512370064\n",
            "Iteration 35, Log-Likelihood (scaled): 61.97419590547497\n",
            "\n",
            "Iteration 9, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 33, Log-Likelihood (scaled): 99.43161865384931Iteration 36, Log-Likelihood (scaled): 62.045979977375495Iteration 10, Log-Likelihood (scaled): inf\n",
            "Iteration 11, Log-Likelihood (scaled): inf\n",
            "Iteration 12, Log-Likelihood (scaled): inf\n",
            "\n",
            "\n",
            "Iteration 34, Log-Likelihood (scaled): 99.28832811333041\n",
            "Iteration 13, Log-Likelihood (scaled): inf\n",
            "Iteration 37, Log-Likelihood (scaled): 62.11762066054087\n",
            "Iteration 14, Log-Likelihood (scaled): inf\n",
            "Iteration 15, Log-Likelihood (scaled): inf\n",
            "Iteration 38, Log-Likelihood (scaled): 62.18911824054144\n",
            "Iteration 39, Log-Likelihood (scaled): 62.260473002381055\n",
            "Iteration 16, Log-Likelihood (scaled): inf\n",
            "Iteration 17, Log-Likelihood (scaled): inf\n",
            "Iteration 18, Log-Likelihood (scaled): inf\n",
            "Iteration 19, Log-Likelihood (scaled): inf\n",
            "Iteration 20, Log-Likelihood (scaled): inf\n",
            "Iteration 21, Log-Likelihood (scaled): inf\n",
            "Iteration 22, Log-Likelihood (scaled): inf\n",
            "Iteration 35, Log-Likelihood (scaled): 99.14532293154016\n",
            "Iteration 23, Log-Likelihood (scaled): inf\n",
            "Iteration 24, Log-Likelihood (scaled): inf\n",
            "Iteration 36, Log-Likelihood (scaled): 99.00260253902007\n",
            "Iteration 37, Log-Likelihood (scaled): 98.86016636745437\n",
            "Iteration 38, Log-Likelihood (scaled): 98.71801384966795\n",
            "Iteration 25, Log-Likelihood (scaled): inf\n",
            "Iteration 26, Log-Likelihood (scaled): inf\n",
            "Iteration 27, Log-Likelihood (scaled): inf\n",
            "Iteration 28, Log-Likelihood (scaled): inf\n",
            "Iteration 29, Log-Likelihood (scaled): inf\n",
            "Iteration 40, Log-Likelihood (scaled): 62.33168523049806\n",
            "Iteration 41, Log-Likelihood (scaled): 62.402755208766465\n",
            "Iteration 42, Log-Likelihood (scaled): 62.47368322049706\n",
            "Iteration 43, Log-Likelihood (scaled): 62.54446954843848\n",
            "Iteration 44, Log-Likelihood (scaled): 62.61511447477844\n",
            "Iteration 45, Log-Likelihood (scaled): 62.6856182811447\n",
            "Iteration 46, Log-Likelihood (scaled): 62.75598124860624\n",
            "Iteration 47, Log-Likelihood (scaled): 62.82620365767437\n",
            "Iteration 48, Log-Likelihood (scaled): 62.8962857883038\n",
            "Iteration 30, Log-Likelihood (scaled): inf\n",
            "Iteration 39, Log-Likelihood (scaled): 98.57614441962363\n",
            "Iteration 40, Log-Likelihood (scaled): 98.4345575124203\n",
            "Iteration 41, Log-Likelihood (scaled): 98.29325256429031\n",
            "Iteration 42, Log-Likelihood (scaled): 98.15222901259722\n",
            "Iteration 43, Log-Likelihood (scaled): 98.01148629583344Iteration 49, Log-Likelihood (scaled): 62.96622791989384\n",
            "Iteration 50, Log-Likelihood (scaled): 63.03603033128933\n",
            "Iteration 51, Log-Likelihood (scaled): 63.10569330078191\n",
            "Iteration 52, Log-Likelihood (scaled): 63.17521710611098\n",
            "Iteration 31, Log-Likelihood (scaled): inf\n",
            "Iteration 53, Log-Likelihood (scaled): 63.244602024464825\n",
            "Iteration 54, Log-Likelihood (scaled): 63.31384833248181\n",
            "\n",
            "Iteration 55, Log-Likelihood (scaled): 63.38295630625135\n",
            "Iteration 56, Log-Likelihood (scaled): 63.45192622131502\n",
            "Iteration 57, Log-Likelihood (scaled): 63.52075835266762\n",
            "Iteration 58, Log-Likelihood (scaled): 63.58945297475834\n",
            "Iteration 59, Log-Likelihood (scaled): 63.6580103614918\n",
            "Iteration 32, Log-Likelihood (scaled): infIteration 60, Log-Likelihood (scaled): 63.726430786229\n",
            "\n",
            "Iteration 44, Log-Likelihood (scaled): 97.87102385361813\n",
            "Iteration 45, Log-Likelihood (scaled): 97.73084112669464\n",
            "Iteration 33, Log-Likelihood (scaled): inf\n",
            "Iteration 46, Log-Likelihood (scaled): 97.59093755692841\n",
            "Iteration 61, Log-Likelihood (scaled): 63.794714521788634\n",
            "Iteration 62, Log-Likelihood (scaled): 63.862861840448\n",
            "Iteration 63, Log-Likelihood (scaled): 63.93087301394409\n",
            "Iteration 64, Log-Likelihood (scaled): 63.99874831347464\n",
            "Iteration 47, Log-Likelihood (scaled): 97.45131258730459\n",
            "Iteration 65, Log-Likelihood (scaled): 64.0664880096993Iteration 34, Log-Likelihood (scaled): inf\n",
            "Iteration 35, Log-Likelihood (scaled): inf\n",
            "Iteration 36, Log-Likelihood (scaled): inf\n",
            "Iteration 37, Log-Likelihood (scaled): inf\n",
            "Iteration 38, Log-Likelihood (scaled): inf\n",
            "Iteration 39, Log-Likelihood (scaled): inf\n",
            "Iteration 48, Log-Likelihood (scaled): 97.31196566192578\n",
            "Iteration 49, Log-Likelihood (scaled): 97.17289622600974\n",
            "\n",
            "Iteration 66, Log-Likelihood (scaled): 64.13409237274061Iteration 40, Log-Likelihood (scaled): inf\n",
            "(26, 199)\n",
            "\n",
            "Iteration 67, Log-Likelihood (scaled): 64.20156167218508\n",
            "Iteration 68, Log-Likelihood (scaled): 64.26889617708426\n",
            "Iteration 69, Log-Likelihood (scaled): 64.33609615595579\n",
            "Iteration 70, Log-Likelihood (scaled): 64.40316187678442\n",
            "Iteration 71, Log-Likelihood (scaled): 64.47009360702319\n",
            "Iteration 41, Log-Likelihood (scaled): inf\n",
            "Iteration 42, Log-Likelihood (scaled): inf\n",
            "Iteration 72, Log-Likelihood (scaled): 64.53689161359426\n",
            "Iteration 73, Log-Likelihood (scaled): 64.60355616289027\n",
            "Iteration 74, Log-Likelihood (scaled): 64.67008752077508\n",
            "   Pattern_0  Pattern_1  Pattern_2  Pattern_3  Pattern_4  Pattern_5  \\\n",
            "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   Pattern_6  Pattern_7  Pattern_8  Pattern_9  ...  Pattern_189  Pattern_190  \\\n",
            "0        0.0        0.0        0.0        0.0  ...          1.0          1.0   \n",
            "\n",
            "   Pattern_191  Pattern_192  Pattern_193  Pattern_194  Pattern_195  \\\n",
            "0          1.0          1.0          1.0          1.0          1.0   \n",
            "\n",
            "   Pattern_196  Pattern_197  Pattern_198  \n",
            "0          1.0          0.0          0.0  \n",
            "\n",
            "[1 rows x 199 columns]\n",
            "Iteration 50, Log-Likelihood (scaled): 97.03410372588719\n",
            "Iteration 51, Log-Likelihood (scaled): 96.89558760899936\n",
            "Iteration 52, Log-Likelihood (scaled): 96.75734732389603\n",
            "Iteration 43, Log-Likelihood (scaled): inf\n",
            "Iteration 75, Log-Likelihood (scaled): 64.73648595258494\n",
            "Iteration 53, Log-Likelihood (scaled): 96.61938232023289\n",
            "Iteration 44, Log-Likelihood (scaled): inf\n",
            "Iteration 45, Log-Likelihood (scaled): inf\n",
            "Iteration 46, Log-Likelihood (scaled): inf\n",
            "Iteration 54, Log-Likelihood (scaled): 96.48169204876962\n",
            "Iteration 47, Log-Likelihood (scaled): inf\n",
            "Iteration 76, Log-Likelihood (scaled): 64.80275172312963\n",
            "Iteration 77, Log-Likelihood (scaled): 64.86888509669335\n",
            "Iteration 78, Log-Likelihood (scaled): 64.93488633703583\n",
            "Iteration 79, Log-Likelihood (scaled): 65.00075570739341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-308-527064c62d59>:46: RuntimeWarning: divide by zero encountered in divide\n",
            "  expected_support = np.sum(np.exp(X[:, i] * weights[i]) / row_norms)  # Row-specific normalization\n",
            "<ipython-input-308-527064c62d59>:26: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood_per_row = X @ weights - np.log(row_norms)\n",
            "<ipython-input-308-527064c62d59>:46: RuntimeWarning: divide by zero encountered in divide\n",
            "  expected_support = np.sum(np.exp(X[:, i] * weights[i]) / row_norms)  # Row-specific normalization\n",
            "<ipython-input-308-527064c62d59>:26: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood_per_row = X @ weights - np.log(row_norms)\n",
            "<ipython-input-308-527064c62d59>:46: RuntimeWarning: divide by zero encountered in divide\n",
            "  expected_support = np.sum(np.exp(X[:, i] * weights[i]) / row_norms)  # Row-specific normalization\n",
            "<ipython-input-308-527064c62d59>:26: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood_per_row = X @ weights - np.log(row_norms)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, Log-Likelihood (scaled): 96.34427596136743\n",
            "Iteration 56, Log-Likelihood (scaled): 96.207133510987\n",
            "Iteration 57, Log-Likelihood (scaled): 96.07026415168602\n",
            "Iteration 58, Log-Likelihood (scaled): 95.93366733861713\n",
            "Iteration 80, Log-Likelihood (scaled): 65.06649347047994\n",
            "Iteration 81, Log-Likelihood (scaled): 65.13209988848794\n",
            "Iteration 48, Log-Likelihood (scaled): inf\n",
            "Iteration 82, Log-Likelihood (scaled): 65.19757522308961\n",
            "Iteration 83, Log-Likelihood (scaled): 65.26291973543782\n",
            "Iteration 84, Log-Likelihood (scaled): 65.32813368616709\n",
            "Iteration 85, Log-Likelihood (scaled): 65.39321733539482\n",
            "Iteration 86, Log-Likelihood (scaled): 65.45817094272208\n",
            "Iteration 87, Log-Likelihood (scaled): 65.52299476723472\n",
            "Iteration 59, Log-Likelihood (scaled): 95.7973425280256\n",
            "Iteration 60, Log-Likelihood (scaled): 95.6612891772472\n",
            "Iteration 88, Log-Likelihood (scaled): 65.5876890675044\n",
            "Iteration 49, Log-Likelihood (scaled): inf\n",
            "Iteration 50, Log-Likelihood (scaled): inf\n",
            "Iteration 89, Log-Likelihood (scaled): 65.65225410158963\n",
            "Iteration 51, Log-Likelihood (scaled): inf\n",
            "Iteration 61, Log-Likelihood (scaled): 95.52550674470585\n",
            "Iteration 62, Log-Likelihood (scaled): 95.38999468991148\n",
            "Iteration 90, Log-Likelihood (scaled): 65.71669012703677\n",
            "Iteration 63, Log-Likelihood (scaled): 95.25475247345786\n",
            "Iteration 91, Log-Likelihood (scaled): 65.78099740088096\n",
            "Iteration 92, Log-Likelihood (scaled): 65.84517617964727\n",
            "Iteration 64, Log-Likelihood (scaled): 95.11977955702025\n",
            "Iteration 65, Log-Likelihood (scaled): 94.98507540335338\n",
            "Iteration 66, Log-Likelihood (scaled): 94.85063947628913\n",
            "Iteration 67, Log-Likelihood (scaled): 94.71647124073446\n",
            "SXmat for Category A saved at /content/drive/MyDrive/Temporary_SXmat/SXmat_Category_A.csv\n",
            "Iteration 93, Log-Likelihood (scaled): 65.90922671935155\n",
            "Iteration 68, Log-Likelihood (scaled): 94.58257016266896\n",
            "Iteration 69, Log-Likelihood (scaled): 94.44893570914299\n",
            "Iteration 70, Log-Likelihood (scaled): 94.31556734827532\n",
            "Iteration 94, Log-Likelihood (scaled): 65.97314927550163\n",
            "Iteration 52, Log-Likelihood (scaled): inf\n",
            "Iteration 53, Log-Likelihood (scaled): inf\n",
            "Density of incidence matrix for this interval (Category A): 0.15326633165829145\n",
            "Iteration 54, Log-Likelihood (scaled): inf\n",
            "Iteration 55, Log-Likelihood (scaled): inf\n",
            "Iteration 56, Log-Likelihood (scaled): inf\n",
            "Iteration 95, Log-Likelihood (scaled): 66.03694410309816\n",
            "Iteration 96, Log-Likelihood (scaled): 66.10061145663573\n",
            "Iteration 97, Log-Likelihood (scaled): 66.16415159010367\n",
            "Iteration 98, Log-Likelihood (scaled): 66.22756475698739\n",
            "Iteration 0, Log-Likelihood (scaled): 4.440892098500626e-16\n",
            "Iteration 71, Log-Likelihood (scaled): 94.18246454925101\n",
            "Iteration 72, Log-Likelihood (scaled): 94.0496267823192\n",
            "Iteration 73, Log-Likelihood (scaled): 93.91705351879094\n",
            "Iteration 57, Log-Likelihood (scaled): inf\n",
            "Iteration 58, Log-Likelihood (scaled): inf\n",
            "Iteration 59, Log-Likelihood (scaled): inf\n",
            "Iteration 99, Log-Likelihood (scaled): 66.29085121026904\n",
            "Iteration 60, Log-Likelihood (scaled): inf\n",
            "Iteration 61, Log-Likelihood (scaled): inf\n",
            "Iteration 100, Log-Likelihood (scaled): 66.3540112024287\n",
            "Iteration 62, Log-Likelihood (scaled): inf\n",
            "Iteration 63, Log-Likelihood (scaled): inf\n",
            "Iteration 74, Log-Likelihood (scaled): 93.78474423103715\n",
            "Iteration 75, Log-Likelihood (scaled): 93.6526983924863\n",
            "Iteration 64, Log-Likelihood (scaled): inf\n",
            "Iteration 76, Log-Likelihood (scaled): 93.52091547762242\n",
            "Iteration 77, Log-Likelihood (scaled): 93.38939496198277\n",
            "Iteration 78, Log-Likelihood (scaled): 93.25813632215593\n",
            "Iteration 79, Log-Likelihood (scaled): 93.12713903577936\n",
            "Iteration 1, Log-Likelihood (scaled): inf\n",
            "Iteration 2, Log-Likelihood (scaled): inf\n",
            "Iteration 3, Log-Likelihood (scaled): inf\n",
            "Iteration 4, Log-Likelihood (scaled): inf\n",
            "Iteration 5, Log-Likelihood (scaled): inf\n",
            "Iteration 6, Log-Likelihood (scaled): inf\n",
            "Iteration 7, Log-Likelihood (scaled): inf\n",
            "Iteration 8, Log-Likelihood (scaled): inf\n",
            "Iteration 9, Log-Likelihood (scaled): inf\n",
            "Iteration 65, Log-Likelihood (scaled): infIteration 101, Log-Likelihood (scaled): 66.41704498544527\n",
            "Iteration 102, Log-Likelihood (scaled): 66.47995281079761\n",
            "Iteration 103, Log-Likelihood (scaled): 66.54273492946533\n",
            "Iteration 104, Log-Likelihood (scaled): 66.60539159192993\n",
            "\n",
            "Iteration 80, Log-Likelihood (scaled): 92.99640258153764\n",
            "Iteration 81, Log-Likelihood (scaled): 92.86592643916\n",
            "Iteration 66, Log-Likelihood (scaled): inf\n",
            "Iteration 67, Log-Likelihood (scaled): infIteration 10, Log-Likelihood (scaled): inf\n",
            "Iteration 11, Log-Likelihood (scaled): infIteration 82, Log-Likelihood (scaled): 92.7357100894184\n",
            "Iteration 83, Log-Likelihood (scaled): 92.60575301412536\n",
            "\n",
            "Iteration 84, Log-Likelihood (scaled): 92.47605469613183\n",
            "Iteration 85, Log-Likelihood (scaled): 92.34661461932508\n",
            "Iteration 86, Log-Likelihood (scaled): 92.21743226862662\n",
            "Iteration 68, Log-Likelihood (scaled): inf\n",
            "Iteration 105, Log-Likelihood (scaled): 66.66792304817575\n",
            "Iteration 106, Log-Likelihood (scaled): 66.73032954769081\n",
            "Iteration 107, Log-Likelihood (scaled): 66.79261133946812\n",
            "Iteration 108, Log-Likelihood (scaled): 66.85476867200624\n",
            "Iteration 109, Log-Likelihood (scaled): 66.91680179331063\n",
            "Iteration 110, Log-Likelihood (scaled): 66.9787109508944\n",
            "Iteration 111, Log-Likelihood (scaled): 67.04049639177936\n",
            "Iteration 112, Log-Likelihood (scaled): 67.10215836249705\n",
            "Iteration 113, Log-Likelihood (scaled): 67.16369710908948\n",
            "Iteration 69, Log-Likelihood (scaled): inf\n",
            "Iteration 70, Log-Likelihood (scaled): inf\n",
            "Iteration 114, Log-Likelihood (scaled): 67.22511287711046\n",
            "Iteration 87, Log-Likelihood (scaled): 92.08850712999012\n",
            "Iteration 88, Log-Likelihood (scaled): 91.95983869039918\n",
            "Iteration 89, Log-Likelihood (scaled): 91.83142643786542\n",
            "Iteration 90, Log-Likelihood (scaled): 91.70326986142635\n",
            "Iteration 91, Log-Likelihood (scaled): 91.5753684511431\n",
            "Iteration 71, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 12, Log-Likelihood (scaled): inf\n",
            "Iteration 13, Log-Likelihood (scaled): infIteration 115, Log-Likelihood (scaled): 67.28640591162623\n",
            "Iteration 72, Log-Likelihood (scaled): infIteration 92, Log-Likelihood (scaled): 91.44772169809866\n",
            "\n",
            "Iteration 73, Log-Likelihood (scaled): inf\n",
            "Iteration 116, Log-Likelihood (scaled): 67.34757645721662\n",
            "Iteration 117, Log-Likelihood (scaled): 67.4086247579759Iteration 93, Log-Likelihood (scaled): 91.32032909439555\n",
            "Iteration 94, Log-Likelihood (scaled): 91.19319013315376\n",
            "Iteration 95, Log-Likelihood (scaled): 91.06630430850899\n",
            "\n",
            "\n",
            "Iteration 118, Log-Likelihood (scaled): 67.46955105751388\n",
            "Iteration 96, Log-Likelihood (scaled): 90.93967111561005\n",
            "Iteration 97, Log-Likelihood (scaled): 90.81329005061738\n",
            "Iteration 98, Log-Likelihood (scaled): 90.6871606107006\n",
            "Iteration 14, Log-Likelihood (scaled): inf\n",
            "Iteration 15, Log-Likelihood (scaled): inf\n",
            "Iteration 16, Log-Likelihood (scaled): inf\n",
            "Iteration 17, Log-Likelihood (scaled): inf\n",
            "Iteration 18, Log-Likelihood (scaled): inf\n",
            "Iteration 19, Log-Likelihood (scaled): inf\n",
            "Iteration 20, Log-Likelihood (scaled): inf\n",
            "Iteration 99, Log-Likelihood (scaled): 90.56128229403667\n",
            "Iteration 21, Log-Likelihood (scaled): inf\n",
            "Iteration 119, Log-Likelihood (scaled): 67.53035559895673\n",
            "Iteration 120, Log-Likelihood (scaled): 67.59103862494798\n",
            "Iteration 22, Log-Likelihood (scaled): inf\n",
            "Iteration 100, Log-Likelihood (scaled): 90.43565459980763\n",
            "Iteration 101, Log-Likelihood (scaled): 90.3102770281989\n",
            "Iteration 23, Log-Likelihood (scaled): inf\n",
            "Iteration 121, Log-Likelihood (scaled): 67.65160037764952\n",
            "Iteration 122, Log-Likelihood (scaled): 67.71204109874245\n",
            "Iteration 74, Log-Likelihood (scaled): inf\n",
            "Iteration 123, Log-Likelihood (scaled): 67.77236102942817\n",
            "Iteration 124, Log-Likelihood (scaled): 67.83256041042921\n",
            "Iteration 125, Log-Likelihood (scaled): 67.89263948199027\n",
            "Iteration 126, Log-Likelihood (scaled): 67.95259848387906\n",
            "Iteration 127, Log-Likelihood (scaled): 68.01243765538733\n",
            "Iteration 102, Log-Likelihood (scaled): 90.18514908039688\n",
            "Iteration 128, Log-Likelihood (scaled): 68.07215723533179\n",
            "Iteration 103, Log-Likelihood (scaled): 90.06027025858714\n",
            "Iteration 104, Log-Likelihood (scaled): 89.93564006595236\n",
            "Iteration 75, Log-Likelihood (scaled): inf\n",
            "Iteration 24, Log-Likelihood (scaled): inf\n",
            "Iteration 25, Log-Likelihood (scaled): inf\n",
            "Iteration 26, Log-Likelihood (scaled): inf\n",
            "Iteration 105, Log-Likelihood (scaled): 89.8112580066703\n",
            "Iteration 106, Log-Likelihood (scaled): 89.6871235859117\n",
            "Iteration 107, Log-Likelihood (scaled): 89.56323630983836\n",
            "Iteration 27, Log-Likelihood (scaled): inf\n",
            "Iteration 28, Log-Likelihood (scaled): inf\n",
            "Iteration 76, Log-Likelihood (scaled): inf\n",
            "Iteration 29, Log-Likelihood (scaled): inf\n",
            "Iteration 108, Log-Likelihood (scaled): 89.43959568560118\n",
            "Iteration 109, Log-Likelihood (scaled): 89.31620122133802\n",
            "Iteration 77, Log-Likelihood (scaled): infIteration 129, Log-Likelihood (scaled): 68.13175746205496\n",
            "Iteration 130, Log-Likelihood (scaled): 68.19123857342633\n",
            "Iteration 131, Log-Likelihood (scaled): 68.2506008068431\n",
            "Iteration 132, Log-Likelihood (scaled): 68.30984439923111\n",
            "\n",
            "Iteration 110, Log-Likelihood (scaled): 89.19305242617172\n",
            "Iteration 30, Log-Likelihood (scaled): inf\n",
            "Iteration 31, Log-Likelihood (scaled): inf\n",
            "Iteration 32, Log-Likelihood (scaled): inf\n",
            "Iteration 133, Log-Likelihood (scaled): 68.36896958704588\n",
            "Iteration 33, Log-Likelihood (scaled): inf\n",
            "Iteration 78, Log-Likelihood (scaled): inf\n",
            "Iteration 79, Log-Likelihood (scaled): inf\n",
            "Iteration 34, Log-Likelihood (scaled): inf\n",
            "Iteration 35, Log-Likelihood (scaled): inf\n",
            "Iteration 80, Log-Likelihood (scaled): inf\n",
            "Iteration 134, Log-Likelihood (scaled): 68.42797660627349Iteration 81, Log-Likelihood (scaled): inf\n",
            "Iteration 111, Log-Likelihood (scaled): 89.0701488102082\n",
            "Iteration 112, Log-Likelihood (scaled): 88.94748988453443\n",
            "Iteration 82, Log-Likelihood (scaled): inf\n",
            "Iteration 36, Log-Likelihood (scaled): inf\n",
            "Iteration 37, Log-Likelihood (scaled): inf\n",
            "Iteration 113, Log-Likelihood (scaled): 88.82507516121638\n",
            "Iteration 114, Log-Likelihood (scaled): 88.70290415329714\n",
            "Iteration 115, Log-Likelihood (scaled): 88.58097637479472\n",
            "Iteration 116, Log-Likelihood (scaled): 88.45929134070042\n",
            "Iteration 117, Log-Likelihood (scaled): 88.33784856697656\n",
            "Iteration 118, Log-Likelihood (scaled): 88.2166475705547\n",
            "Iteration 38, Log-Likelihood (scaled): inf\n",
            "Iteration 39, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 135, Log-Likelihood (scaled): 68.48686569243151\n",
            "Iteration 136, Log-Likelihood (scaled): 68.54563708056993\n",
            "Iteration 137, Log-Likelihood (scaled): 68.60429100527202\n",
            "Iteration 138, Log-Likelihood (scaled): 68.66282770065544\n",
            "Iteration 40, Log-Likelihood (scaled): inf\n",
            "Iteration 83, Log-Likelihood (scaled): inf\n",
            "Iteration 84, Log-Likelihood (scaled): inf\n",
            "Iteration 85, Log-Likelihood (scaled): inf\n",
            "Iteration 139, Log-Likelihood (scaled): 68.72124740037293\n",
            "Iteration 140, Log-Likelihood (scaled): 68.77955033761337\n",
            "Iteration 119, Log-Likelihood (scaled): 88.09568786933355\n",
            "Iteration 41, Log-Likelihood (scaled): inf\n",
            "Iteration 42, Log-Likelihood (scaled): inf\n",
            "Iteration 43, Log-Likelihood (scaled): inf\n",
            "Iteration 44, Log-Likelihood (scaled): inf\n",
            "Iteration 45, Log-Likelihood (scaled): inf\n",
            "Iteration 120, Log-Likelihood (scaled): 87.97496898217707\n",
            "Iteration 86, Log-Likelihood (scaled): inf\n",
            "Iteration 87, Log-Likelihood (scaled): inf\n",
            "Iteration 88, Log-Likelihood (scaled): inf\n",
            "Iteration 89, Log-Likelihood (scaled): inf\n",
            "Iteration 46, Log-Likelihood (scaled): inf\n",
            "Iteration 47, Log-Likelihood (scaled): inf\n",
            "Iteration 90, Log-Likelihood (scaled): infIteration 48, Log-Likelihood (scaled): inf\n",
            "Iteration 141, Log-Likelihood (scaled): 68.83773674510266\n",
            "Iteration 142, Log-Likelihood (scaled): 68.89580685510461\n",
            "Iteration 49, Log-Likelihood (scaled): inf\n",
            "Iteration 121, Log-Likelihood (scaled): 87.8544904289125\n",
            "Iteration 143, Log-Likelihood (scaled): 68.95376089942194\n",
            "\n",
            "Iteration 91, Log-Likelihood (scaled): inf\n",
            "Iteration 92, Log-Likelihood (scaled): inf\n",
            "Iteration 144, Log-Likelihood (scaled): 69.01159910939708\n",
            "Iteration 145, Log-Likelihood (scaled): 69.06932171591319\n",
            "Iteration 146, Log-Likelihood (scaled): 69.12692894939495\n",
            "Iteration 50, Log-Likelihood (scaled): inf\n",
            "Iteration 51, Log-Likelihood (scaled): inf\n",
            "Iteration 147, Log-Likelihood (scaled): 69.18442103980954\n",
            "Iteration 148, Log-Likelihood (scaled): 69.24179821666758\n",
            "Iteration 52, Log-Likelihood (scaled): inf\n",
            "Iteration 93, Log-Likelihood (scaled): inf\n",
            "Iteration 94, Log-Likelihood (scaled): inf\n",
            "Iteration 95, Log-Likelihood (scaled): inf\n",
            "Iteration 96, Log-Likelihood (scaled): inf\n",
            "Iteration 97, Log-Likelihood (scaled): infIteration 122, Log-Likelihood (scaled): 87.7342517303284\n",
            "Iteration 149, Log-Likelihood (scaled): 69.29906070902395\n",
            "Iteration 150, Log-Likelihood (scaled): 69.35620874547868\n",
            "Iteration 53, Log-Likelihood (scaled): inf\n",
            "Iteration 123, Log-Likelihood (scaled): 87.6142524081728\n",
            "Iteration 124, Log-Likelihood (scaled): 87.49449198515111\n",
            "Iteration 125, Log-Likelihood (scaled): 87.3749699849242\n",
            "Iteration 54, Log-Likelihood (scaled): inf\n",
            "Iteration 55, Log-Likelihood (scaled): inf\n",
            "Iteration 56, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 98, Log-Likelihood (scaled): inf\n",
            "Iteration 99, Log-Likelihood (scaled): inf\n",
            "Iteration 100, Log-Likelihood (scaled): inf\n",
            "Iteration 57, Log-Likelihood (scaled): inf\n",
            "Iteration 101, Log-Likelihood (scaled): inf\n",
            "Iteration 126, Log-Likelihood (scaled): 87.25568593210664\n",
            "Iteration 151, Log-Likelihood (scaled): 69.41324255417803\n",
            "Iteration 58, Log-Likelihood (scaled): inf\n",
            "Iteration 59, Log-Likelihood (scaled): inf\n",
            "Iteration 60, Log-Likelihood (scaled): inf\n",
            "Iteration 61, Log-Likelihood (scaled): inf\n",
            "Iteration 102, Log-Likelihood (scaled): inf\n",
            "Iteration 127, Log-Likelihood (scaled): 87.13663935226452\n",
            "Iteration 62, Log-Likelihood (scaled): inf\n",
            "Iteration 103, Log-Likelihood (scaled): inf\n",
            "Iteration 128, Log-Likelihood (scaled): 87.01782977191372\n",
            "Iteration 129, Log-Likelihood (scaled): 86.89925671851792\n",
            "Iteration 63, Log-Likelihood (scaled): inf\n",
            "Iteration 130, Log-Likelihood (scaled): 86.78091972048654\n",
            "Iteration 104, Log-Likelihood (scaled): inf\n",
            "Iteration 152, Log-Likelihood (scaled): 69.47016236281513\n",
            "Iteration 153, Log-Likelihood (scaled): 69.52696839863103\n",
            "Iteration 154, Log-Likelihood (scaled): 69.58366088841564\n",
            "Iteration 64, Log-Likelihood (scaled): inf\n",
            "Iteration 65, Log-Likelihood (scaled): inf\n",
            "Iteration 155, Log-Likelihood (scaled): 69.64024005850843\n",
            "Iteration 131, Log-Likelihood (scaled): 86.66281830717314\n",
            "Iteration 132, Log-Likelihood (scaled): 86.54495200887325\n",
            "Iteration 156, Log-Likelihood (scaled): 69.6967061347995\n",
            "Iteration 133, Log-Likelihood (scaled): 86.42732035682252\n",
            "Iteration 157, Log-Likelihood (scaled): 69.75305934273047\n",
            "Iteration 105, Log-Likelihood (scaled): inf\n",
            "Iteration 106, Log-Likelihood (scaled): inf\n",
            "Iteration 158, Log-Likelihood (scaled): 69.80929990729516\n",
            "Iteration 134, Log-Likelihood (scaled): 86.30992288319487\n",
            "Iteration 135, Log-Likelihood (scaled): 86.19275912110058\n",
            "Iteration 107, Log-Likelihood (scaled): inf\n",
            "Iteration 108, Log-Likelihood (scaled): inf\n",
            "Iteration 109, Log-Likelihood (scaled): inf\n",
            "Iteration 136, Log-Likelihood (scaled): 86.0758286045843\n",
            "Iteration 137, Log-Likelihood (scaled): 85.95913086862335\n",
            "Iteration 159, Log-Likelihood (scaled): 69.86542805304072\n",
            "Iteration 138, Log-Likelihood (scaled): 85.8426654491256\n",
            "Iteration 139, Log-Likelihood (scaled): 85.72643188292781\n",
            "Iteration 110, Log-Likelihood (scaled): inf\n",
            "Iteration 111, Log-Likelihood (scaled): inf\n",
            "Iteration 140, Log-Likelihood (scaled): 85.61042970779357\n",
            "Iteration 141, Log-Likelihood (scaled): 85.49465846241155\n",
            "Iteration 142, Log-Likelihood (scaled): 85.37911768639358\n",
            "Iteration 160, Log-Likelihood (scaled): 69.92144400406838\n",
            "Iteration 66, Log-Likelihood (scaled): inf\n",
            "Iteration 67, Log-Likelihood (scaled): inf\n",
            "Iteration 112, Log-Likelihood (scaled): inf\n",
            "Iteration 68, Log-Likelihood (scaled): inf\n",
            "Iteration 143, Log-Likelihood (scaled): 85.26380692027277\n",
            "Iteration 144, Log-Likelihood (scaled): 85.14872570550165\n",
            "Iteration 145, Log-Likelihood (scaled): 85.03387358445035\n",
            "Iteration 146, Log-Likelihood (scaled): 84.9192501004047\n",
            "Iteration 161, Log-Likelihood (scaled): 69.9773479840344\n",
            "Iteration 147, Log-Likelihood (scaled): 84.80485479756442\n",
            "Iteration 113, Log-Likelihood (scaled): inf\n",
            "Iteration 148, Log-Likelihood (scaled): 84.69068722104123\n",
            "Iteration 69, Log-Likelihood (scaled): inf\n",
            "Iteration 70, Log-Likelihood (scaled): inf\n",
            "Iteration 71, Log-Likelihood (scaled): inf\n",
            "Iteration 162, Log-Likelihood (scaled): 70.03314021615088\n",
            "Iteration 163, Log-Likelihood (scaled): 70.08882092318666\n",
            "Iteration 164, Log-Likelihood (scaled): 70.14439032746824\n",
            "Iteration 165, Log-Likelihood (scaled): 70.19984865088064\n",
            "Iteration 166, Log-Likelihood (scaled): 70.2551961148682\n",
            "Iteration 167, Log-Likelihood (scaled): 70.31043294043556\n",
            "Iteration 168, Log-Likelihood (scaled): 70.36555934814845\n",
            "Iteration 114, Log-Likelihood (scaled): infIteration 169, Log-Likelihood (scaled): 70.4205755581346\n",
            "Iteration 72, Log-Likelihood (scaled): infIteration 149, Log-Likelihood (scaled): 84.57674691685703\n",
            "Iteration 150, Log-Likelihood (scaled): 84.46303343194204\n",
            "Iteration 151, Log-Likelihood (scaled): 84.34954631413302\n",
            "Iteration 152, Log-Likelihood (scaled): 84.2362851121713\n",
            "Iteration 153, Log-Likelihood (scaled): 84.12324937570116\n",
            "\n",
            "Iteration 115, Log-Likelihood (scaled): inf\n",
            "Iteration 170, Log-Likelihood (scaled): 70.47548179008461\n",
            "Iteration 171, Log-Likelihood (scaled): 70.53027826325277\n",
            "Iteration 172, Log-Likelihood (scaled): 70.58496519645792\n",
            "Iteration 154, Log-Likelihood (scaled): 84.01043865526786\n",
            "Iteration 116, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 173, Log-Likelihood (scaled): 70.63954280808443\n",
            "Iteration 174, Log-Likelihood (scaled): 70.69401131608291\n",
            "Iteration 117, Log-Likelihood (scaled): inf\n",
            "Iteration 175, Log-Likelihood (scaled): 70.74837093797115\n",
            "Iteration 176, Log-Likelihood (scaled): 70.8026218908349\n",
            "Iteration 155, Log-Likelihood (scaled): 83.89785250231584\n",
            "Iteration 156, Log-Likelihood (scaled): 83.78549046918693\n",
            "Iteration 118, Log-Likelihood (scaled): infIteration 73, Log-Likelihood (scaled): inf\n",
            "Iteration 74, Log-Likelihood (scaled): inf\n",
            "Iteration 177, Log-Likelihood (scaled): 70.85676439132888\n",
            "Iteration 178, Log-Likelihood (scaled): 70.91079865567748\n",
            "Iteration 75, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 119, Log-Likelihood (scaled): inf\n",
            "Iteration 120, Log-Likelihood (scaled): inf\n",
            "Iteration 121, Log-Likelihood (scaled): inf\n",
            "Iteration 179, Log-Likelihood (scaled): 70.96472489967562\n",
            "Iteration 180, Log-Likelihood (scaled): 71.01854333868967\n",
            "Iteration 76, Log-Likelihood (scaled): inf\n",
            "Iteration 77, Log-Likelihood (scaled): inf\n",
            "Iteration 122, Log-Likelihood (scaled): inf\n",
            "Iteration 78, Log-Likelihood (scaled): inf\n",
            "Iteration 123, Log-Likelihood (scaled): inf\n",
            "Iteration 157, Log-Likelihood (scaled): 83.67335210911844\n",
            "Iteration 158, Log-Likelihood (scaled): 83.56143697624157\n",
            "Iteration 124, Log-Likelihood (scaled): inf\n",
            "Iteration 79, Log-Likelihood (scaled): inf\n",
            "Iteration 80, Log-Likelihood (scaled): inf\n",
            "Iteration 181, Log-Likelihood (scaled): 71.07225418765833\n",
            "Iteration 182, Log-Likelihood (scaled): 71.12585766109339\n",
            "Iteration 183, Log-Likelihood (scaled): 71.1793539730805\n",
            "Iteration 184, Log-Likelihood (scaled): 71.23274333728023\n",
            "Iteration 185, Log-Likelihood (scaled): 71.28602596692878\n",
            "Iteration 186, Log-Likelihood (scaled): 71.33920207483877\n",
            "Iteration 187, Log-Likelihood (scaled): 71.39227187340019\n",
            "Iteration 188, Log-Likelihood (scaled): 71.44523557458119\n",
            "Iteration 189, Log-Likelihood (scaled): 71.49809338992885\n",
            "Iteration 81, Log-Likelihood (scaled): inf\n",
            "Iteration 159, Log-Likelihood (scaled): 83.44974462557937\n",
            "Iteration 160, Log-Likelihood (scaled): 83.3382746130451\n",
            "Iteration 82, Log-Likelihood (scaled): infIteration 190, Log-Likelihood (scaled): 71.5508455305702\n",
            "Iteration 191, Log-Likelihood (scaled): 71.60349220721277\n",
            "Iteration 161, Log-Likelihood (scaled): 83.22702649544027\n",
            "Iteration 192, Log-Likelihood (scaled): 71.65603363014569\n",
            "Iteration 193, Log-Likelihood (scaled): 71.70847000924033\n",
            "Iteration 162, Log-Likelihood (scaled): 83.11599983045305\n",
            "Iteration 194, Log-Likelihood (scaled): 71.68604641860414\n",
            "Iteration 163, Log-Likelihood (scaled): 83.00519417665632\n",
            "Iteration 164, Log-Likelihood (scaled): 82.89460909350595\n",
            "\n",
            "Iteration 83, Log-Likelihood (scaled): inf\n",
            "Iteration 84, Log-Likelihood (scaled): inf\n",
            "Iteration 125, Log-Likelihood (scaled): inf\n",
            "Iteration 126, Log-Likelihood (scaled): inf\n",
            "Iteration 127, Log-Likelihood (scaled): inf\n",
            "Iteration 128, Log-Likelihood (scaled): inf\n",
            "Iteration 129, Log-Likelihood (scaled): inf\n",
            "Iteration 130, Log-Likelihood (scaled): inf\n",
            "Iteration 165, Log-Likelihood (scaled): 82.78424414133897\n",
            "Iteration 166, Log-Likelihood (scaled): 82.67409888137186\n",
            "Iteration 195, Log-Likelihood (scaled): 71.64861935493659\n",
            "Iteration 196, Log-Likelihood (scaled): 71.61130400242823\n",
            "Iteration 197, Log-Likelihood (scaled): 71.57410002736863\n",
            "Iteration 198, Log-Likelihood (scaled): 71.53700709704609\n",
            "Iteration 199, Log-Likelihood (scaled): 71.50002487974452\n",
            "Iteration 85, Log-Likelihood (scaled): inf\n",
            "Iteration 86, Log-Likelihood (scaled): inf\n",
            "Iteration 87, Log-Likelihood (scaled): inf\n",
            "Iteration 131, Log-Likelihood (scaled): inf\n",
            "Iteration 132, Log-Likelihood (scaled): inf\n",
            "Iteration 88, Log-Likelihood (scaled): infLearned Weights (scaled): [0.54330873 0.56912357 0.27817842 0.36924528 0.41951892 0.37014014\n",
            " 0.31077958 0.29539338 0.12784915 0.2795628  0.18608327 0.35504656\n",
            " 0.37116343 0.45852236 0.21766756 0.26276656 0.36758811 0.47211326\n",
            " 0.59088369 0.51335217 0.29999063 0.59266128 0.62211031 0.72090687\n",
            " 0.57570386 0.20594304 0.13465669 0.09493812 0.34701439 0.39867397\n",
            " 0.32776286 0.39825677 0.27870885 0.33828516 0.47657334 0.57347377\n",
            " 0.03003522 0.35766048 0.6689372  0.46909162 0.65158485 0.39309642\n",
            " 0.21161    0.43903851 0.45677852 0.64279744 0.39719518 0.39070849\n",
            " 0.67853138 0.8281551  0.38727015 0.3453415  0.44892248 0.34648508\n",
            " 0.34772687 0.49294788 0.57351838 0.56415983 0.68435718 0.99327068\n",
            " 0.19769352 0.675907   0.50209537 0.50646613 0.29628512 0.64900741\n",
            " 0.55325362 0.11565989 0.48694116 0.82329183 0.31972189 0.09164566\n",
            " 0.60739953 0.8506054  0.32948989 0.34788447 0.43037606 0.43998498\n",
            " 0.60649989 0.80309744 0.21162137 0.67801519 0.3105981  0.23281939\n",
            " 0.67429185 0.66183361 0.08382734 0.46149591 0.47566785 0.11614079\n",
            " 0.65549094 0.1875914  0.61240711 0.31255335 0.3810718  0.52814232\n",
            " 0.35714764 0.43280693 0.39758166 0.05890259 0.41826329 0.50293685\n",
            " 0.0776545  0.66122658 0.34938974 0.08871382 0.59179954 0.40229092\n",
            " 0.36648598 0.5970472  0.73364153 0.26321475 0.25601419 0.40530852\n",
            " 0.41969334 0.31768231 0.35824487 0.34722387 0.20696307 0.4059511\n",
            " 0.42134423 0.38507989 0.54211926 0.62359026 0.45276714 0.54387654\n",
            " 0.49961213 0.39117615 0.73772456 0.40069041 0.28093234 0.4247742\n",
            " 0.426379   0.39566929 0.75378968 0.41404832 0.44629782 0.376938\n",
            " 0.38349026 0.46098202 0.21546875 0.00765415 0.52776881 0.22112366\n",
            " 0.76167856 0.0804299  0.93893944 0.22551669 0.42082775 0.31021279\n",
            " 0.43118871 0.         0.37825508 0.17268548 0.4265233  0.29143521\n",
            " 0.44694921 0.28429842 0.07108344 0.39710425 0.52934341 0.5459567\n",
            " 0.41895661 0.20372952 0.73982987 0.22036035 0.48722118 0.55260309\n",
            " 0.7413371  0.33994134 0.24343758 0.58951639 0.68600755 0.76304889\n",
            " 0.91220809 0.77920166 0.88811004 0.73880656 0.75245082 0.94781211\n",
            " 0.62691038 0.6845713  0.68220219 0.71060045 0.85217537 0.75986777\n",
            " 0.67377323 0.39990943 0.52572615 1.         0.4522768  0.61556057\n",
            " 0.57150275 0.76705196 0.59458585 0.36491577 0.92671127 0.37199934\n",
            " 0.1969122 ]\n",
            "Iteration 167, Log-Likelihood (scaled): 82.56417287569872\n",
            "Iteration 168, Log-Likelihood (scaled): 82.45446568728948\n",
            "Iteration 169, Log-Likelihood (scaled): 82.34497687998822\n",
            "Final Normalization Parameter (w_norm): 500.0\n",
            "(199,)\n",
            "\n",
            "Iteration 89, Log-Likelihood (scaled): infIteration 133, Log-Likelihood (scaled): inf\n",
            "Iteration 134, Log-Likelihood (scaled): inf\n",
            "Iteration 135, Log-Likelihood (scaled): inf\n",
            "Iteration 136, Log-Likelihood (scaled): inf\n",
            "Iteration 137, Log-Likelihood (scaled): inf\n",
            "Iteration 138, Log-Likelihood (scaled): inf\n",
            "Iteration 139, Log-Likelihood (scaled): inf\n",
            "Iteration 140, Log-Likelihood (scaled): inf\n",
            "Iteration 141, Log-Likelihood (scaled): inf\n",
            "Iteration 170, Log-Likelihood (scaled): 82.2357060185114\n",
            "Iteration 171, Log-Likelihood (scaled): 82.12665266844586\n",
            "Iteration 172, Log-Likelihood (scaled): 82.01781639624744\n",
            "Iteration 173, Log-Likelihood (scaled): 81.90919676923897\n",
            "Iteration 90, Log-Likelihood (scaled): inf\n",
            "Iteration 91, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 142, Log-Likelihood (scaled): inf\n",
            "Iteration 143, Log-Likelihood (scaled): inf\n",
            "Iteration 144, Log-Likelihood (scaled): inf\n",
            "Iteration 145, Log-Likelihood (scaled): inf\n",
            "Iteration 146, Log-Likelihood (scaled): inf\n",
            "Iteration 147, Log-Likelihood (scaled): inf\n",
            "Iteration 148, Log-Likelihood (scaled): inf\n",
            "Iteration 149, Log-Likelihood (scaled): inf\n",
            "Iteration 92, Log-Likelihood (scaled): inf\n",
            "Iteration 174, Log-Likelihood (scaled): 81.80079335560856\n",
            "Iteration 150, Log-Likelihood (scaled): inf\n",
            "Iteration 151, Log-Likelihood (scaled): inf\n",
            "Iteration 93, Log-Likelihood (scaled): inf\n",
            "Iteration 175, Log-Likelihood (scaled): 81.6926057244079\n",
            "Iteration 176, Log-Likelihood (scaled): 81.58463344555052\n",
            "Iteration 177, Log-Likelihood (scaled): 81.47687608981002Iteration 152, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 178, Log-Likelihood (scaled): 81.36933322881835\n",
            "Iteration 153, Log-Likelihood (scaled): inf\n",
            "Iteration 179, Log-Likelihood (scaled): 81.262004435064\n",
            "Iteration 180, Log-Likelihood (scaled): 81.15488928189045\n",
            "Iteration 181, Log-Likelihood (scaled): 81.04798734349427\n",
            "Iteration 94, Log-Likelihood (scaled): inf\n",
            "Iteration 182, Log-Likelihood (scaled): 80.94129819492346\n",
            "Iteration 95, Log-Likelihood (scaled): inf\n",
            "Iteration 96, Log-Likelihood (scaled): inf\n",
            "Iteration 154, Log-Likelihood (scaled): inf\n",
            "Iteration 97, Log-Likelihood (scaled): inf\n",
            "Iteration 155, Log-Likelihood (scaled): inf\n",
            "Iteration 156, Log-Likelihood (scaled): inf\n",
            "Iteration 183, Log-Likelihood (scaled): 80.8348214120758\n",
            "Iteration 184, Log-Likelihood (scaled): 80.72855657169697\n",
            "Iteration 157, Log-Likelihood (scaled): inf\n",
            "Iteration 158, Log-Likelihood (scaled): inf\n",
            "Iteration 159, Log-Likelihood (scaled): inf\n",
            "Iteration 160, Log-Likelihood (scaled): inf\n",
            "Iteration 185, Log-Likelihood (scaled): 80.62250325137913\n",
            "Iteration 186, Log-Likelihood (scaled): 80.51666102955875\n",
            "Iteration 161, Log-Likelihood (scaled): inf\n",
            "Iteration 162, Log-Likelihood (scaled): inf\n",
            "Iteration 187, Log-Likelihood (scaled): 80.41102948551543\n",
            "Iteration 98, Log-Likelihood (scaled): inf\n",
            "Iteration 99, Log-Likelihood (scaled): inf\n",
            "Iteration 100, Log-Likelihood (scaled): inf\n",
            "Iteration 101, Log-Likelihood (scaled): inf\n",
            "Iteration 102, Log-Likelihood (scaled): inf\n",
            "Iteration 103, Log-Likelihood (scaled): inf\n",
            "Iteration 104, Log-Likelihood (scaled): inf\n",
            "Iteration 105, Log-Likelihood (scaled): inf\n",
            "Iteration 163, Log-Likelihood (scaled): inf\n",
            "Iteration 164, Log-Likelihood (scaled): inf\n",
            "Iteration 165, Log-Likelihood (scaled): inf\n",
            "Iteration 166, Log-Likelihood (scaled): inf\n",
            "Iteration 167, Log-Likelihood (scaled): inf\n",
            "Iteration 168, Log-Likelihood (scaled): inf\n",
            "Iteration 169, Log-Likelihood (scaled): inf\n",
            "Iteration 170, Log-Likelihood (scaled): inf\n",
            "Iteration 171, Log-Likelihood (scaled): inf\n",
            "Iteration 172, Log-Likelihood (scaled): inf\n",
            "Iteration 173, Log-Likelihood (scaled): inf\n",
            "Iteration 106, Log-Likelihood (scaled): inf\n",
            "Iteration 107, Log-Likelihood (scaled): inf\n",
            "Iteration 108, Log-Likelihood (scaled): inf\n",
            "Iteration 109, Log-Likelihood (scaled): inf\n",
            "Iteration 174, Log-Likelihood (scaled): inf\n",
            "Iteration 188, Log-Likelihood (scaled): 80.30560819936979\n",
            "Iteration 189, Log-Likelihood (scaled): 80.20039675208204Iteration 175, Log-Likelihood (scaled): inf\n",
            "Iteration 176, Log-Likelihood (scaled): infIteration 110, Log-Likelihood (scaled): inf\n",
            "Iteration 177, Log-Likelihood (scaled): inf\n",
            "Iteration 190, Log-Likelihood (scaled): 80.09539472545012\n",
            "Iteration 191, Log-Likelihood (scaled): 79.99060170210814\n",
            "Iteration 192, Log-Likelihood (scaled): 79.88601726552454\n",
            "Iteration 193, Log-Likelihood (scaled): 79.78164100000053\n",
            "Iteration 194, Log-Likelihood (scaled): 79.6774724906684\n",
            "\n",
            "Iteration 111, Log-Likelihood (scaled): inf\n",
            "Iteration 195, Log-Likelihood (scaled): 79.57351132348978\n",
            "Iteration 196, Log-Likelihood (scaled): 79.46975708525403\n",
            "Iteration 197, Log-Likelihood (scaled): 79.36620936357653\n",
            "Iteration 112, Log-Likelihood (scaled): inf\n",
            "Iteration 113, Log-Likelihood (scaled): inf\n",
            "Iteration 114, Log-Likelihood (scaled): inf\n",
            "Iteration 115, Log-Likelihood (scaled): inf\n",
            "Iteration 116, Log-Likelihood (scaled): inf\n",
            "Iteration 117, Log-Likelihood (scaled): inf\n",
            "Iteration 118, Log-Likelihood (scaled): inf\n",
            "Iteration 119, Log-Likelihood (scaled): inf\n",
            "Iteration 120, Log-Likelihood (scaled): inf\n",
            "Iteration 121, Log-Likelihood (scaled): inf\n",
            "Iteration 122, Log-Likelihood (scaled): inf\n",
            "Iteration 123, Log-Likelihood (scaled): inf\n",
            "Iteration 124, Log-Likelihood (scaled): inf\n",
            "Iteration 125, Log-Likelihood (scaled): inf\n",
            "Iteration 126, Log-Likelihood (scaled): inf\n",
            "Iteration 127, Log-Likelihood (scaled): inf\n",
            "Iteration 128, Log-Likelihood (scaled): inf\n",
            "\n",
            "Iteration 198, Log-Likelihood (scaled): 79.26286774689706\n",
            "Iteration 199, Log-Likelihood (scaled): 79.15973182447811\n",
            "Learned Weights (scaled): [0.44033214 0.51285559 0.44213231 0.45664458 0.43328998 0.36610536\n",
            " 0.44490097 0.52170134 0.50990227 0.50852288 0.38236575 0.43166515\n",
            " 0.45408412 0.40160566 0.44797772 0.39424675 0.44646595 0.5842278\n",
            " 0.82043503 0.62063928 0.99910816 0.59645622 0.59851376 0.63749734\n",
            " 0.79909238 0.65396811 0.47533649 0.54833122 0.52099644 0.43363511\n",
            " 0.448335   0.39734831 0.48472764 0.46580324 0.5212451  0.43222061\n",
            " 0.43722869 0.40892498 0.53506874 0.45034511 0.51271814 0.4817923\n",
            " 0.65404188 0.46453257 0.622565   0.39454529 0.61460333 0.64150015\n",
            " 0.46357311 0.62172559 0.49082073 0.46139344 0.43066695 0.48398542\n",
            " 0.48306585 0.72353833 0.43511145 0.80991213 0.69584692 0.62331399\n",
            " 0.41771772 0.48677979 0.56909153 0.58574687 0.41823047 0.44444608\n",
            " 0.6412394  0.44779075 0.44241448 0.40786205 0.42577645 0.61547699\n",
            " 0.69744832 0.80610389 0.6616327  0.4194606  0.64048972 0.39274653\n",
            " 0.61747006 0.54491612 0.37978971 0.63129178 0.9643466  0.83317909\n",
            " 0.82795144 0.46379085 0.56505838 0.66456468 0.61169314 0.80578229\n",
            " 0.64225878 0.83125873 0.52277982 0.62523469 1.         0.95135201\n",
            " 0.82844241 0.61809315 0.62324398 0.83304893 0.82138742 0.98655628\n",
            " 0.6638656  0.76766083 0.49292597 0.52243343 0.61769845 0.52859495\n",
            " 0.58753129 0.50815316 0.39903783 0.66140793 0.48858593 0.60725178\n",
            " 0.60662904 0.47052157 0.42071718 0.40084483 0.43674727 0.50944289\n",
            " 0.42495427 0.56214363 0.65740846 0.61565213 0.6521056  0.37650678\n",
            " 0.6008701  0.39767849 0.35545922 0.49662736 0.42598714 0.49614695\n",
            " 0.38616669 0.62154752 0.60516273 0.40475857 0.50455356 0.83628606\n",
            " 0.74530789 0.98920167 0.64969054 0.41784808 0.59170612 0.62134651\n",
            " 0.16632754 0.5402496  0.63370799 0.54136375 0.50793394 0.40133027\n",
            " 0.64236439 0.54177899 0.62421755 0.38988648 0.59604161 0.58993418\n",
            " 0.59565856 0.43156301 0.57495879 0.64660407 0.59910397 0.4755469\n",
            " 0.42201503 0.         0.46962852 0.49111376 0.64250877 0.78712568\n",
            " 0.97845888 0.48018208 0.42515304 0.67423898 0.59570668 0.63073562\n",
            " 0.62730556 0.57622262 0.45268982 0.46189017 0.55397149 0.60117372\n",
            " 0.44691076 0.43615011 0.48774882 0.42427448 0.40425472 0.6213633\n",
            " 0.40390672 0.40542415 0.67595937 0.64454242 0.46179235 0.62506062\n",
            " 0.49961062 0.34506196 0.47541827 0.49951608 0.60399463 0.42884972\n",
            " 0.47021274]\n",
            "Final Normalization Parameter (w_norm): 500.0\n",
            "(199,)\n",
            "Iteration 178, Log-Likelihood (scaled): inf\n",
            "Iteration 129, Log-Likelihood (scaled): inf\n",
            "Iteration 130, Log-Likelihood (scaled): inf\n",
            "Iteration 131, Log-Likelihood (scaled): inf\n",
            "Iteration 132, Log-Likelihood (scaled): inf\n",
            "Iteration 133, Log-Likelihood (scaled): inf\n",
            "Iteration 134, Log-Likelihood (scaled): inf\n",
            "Iteration 135, Log-Likelihood (scaled): inf\n",
            "Iteration 136, Log-Likelihood (scaled): inf\n",
            "Iteration 137, Log-Likelihood (scaled): inf\n",
            "Iteration 138, Log-Likelihood (scaled): inf\n",
            "Iteration 139, Log-Likelihood (scaled): inf\n",
            "Iteration 140, Log-Likelihood (scaled): inf\n",
            "Iteration 141, Log-Likelihood (scaled): inf\n",
            "Iteration 142, Log-Likelihood (scaled): inf\n",
            "Iteration 143, Log-Likelihood (scaled): inf\n",
            "Iteration 144, Log-Likelihood (scaled): inf\n",
            "Iteration 145, Log-Likelihood (scaled): inf\n",
            "Iteration 179, Log-Likelihood (scaled): inf\n",
            "Iteration 180, Log-Likelihood (scaled): inf\n",
            "Iteration 146, Log-Likelihood (scaled): inf\n",
            "Iteration 181, Log-Likelihood (scaled): inf\n",
            "Iteration 182, Log-Likelihood (scaled): inf\n",
            "Iteration 183, Log-Likelihood (scaled): inf\n",
            "Iteration 184, Log-Likelihood (scaled): inf\n",
            "Iteration 185, Log-Likelihood (scaled): inf\n",
            "Iteration 186, Log-Likelihood (scaled): inf\n",
            "Iteration 187, Log-Likelihood (scaled): inf\n",
            "Iteration 188, Log-Likelihood (scaled): inf\n",
            "Iteration 147, Log-Likelihood (scaled): inf\n",
            "Iteration 189, Log-Likelihood (scaled): inf\n",
            "Iteration 190, Log-Likelihood (scaled): inf\n",
            "Iteration 191, Log-Likelihood (scaled): inf\n",
            "Iteration 192, Log-Likelihood (scaled): inf\n",
            "Iteration 193, Log-Likelihood (scaled): inf\n",
            "Iteration 194, Log-Likelihood (scaled): inf\n",
            "Iteration 195, Log-Likelihood (scaled): inf\n",
            "Iteration 196, Log-Likelihood (scaled): inf\n",
            "Iteration 197, Log-Likelihood (scaled): inf\n",
            "Iteration 198, Log-Likelihood (scaled): inf\n",
            "Iteration 199, Log-Likelihood (scaled): inf\n",
            "Learned Weights (scaled): [-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100.]\n",
            "Final Normalization Parameter (w_norm): 0.0\n",
            "(199,)\n",
            "Iteration 148, Log-Likelihood (scaled): inf\n",
            "Iteration 149, Log-Likelihood (scaled): inf\n",
            "Iteration 150, Log-Likelihood (scaled): inf\n",
            "Iteration 151, Log-Likelihood (scaled): inf\n",
            "Iteration 152, Log-Likelihood (scaled): inf\n",
            "Iteration 153, Log-Likelihood (scaled): inf\n",
            "Iteration 154, Log-Likelihood (scaled): inf\n",
            "Iteration 155, Log-Likelihood (scaled): inf\n",
            "Iteration 156, Log-Likelihood (scaled): inf\n",
            "Iteration 157, Log-Likelihood (scaled): inf\n",
            "Iteration 158, Log-Likelihood (scaled): inf\n",
            "Iteration 159, Log-Likelihood (scaled): inf\n",
            "Iteration 160, Log-Likelihood (scaled): inf\n",
            "Iteration 161, Log-Likelihood (scaled): inf\n",
            "Iteration 162, Log-Likelihood (scaled): inf\n",
            "Iteration 163, Log-Likelihood (scaled): inf\n",
            "Iteration 164, Log-Likelihood (scaled): inf\n",
            "Iteration 165, Log-Likelihood (scaled): inf\n",
            "Iteration 166, Log-Likelihood (scaled): inf\n",
            "Iteration 167, Log-Likelihood (scaled): inf\n",
            "Iteration 168, Log-Likelihood (scaled): inf\n",
            "Iteration 169, Log-Likelihood (scaled): inf\n",
            "Iteration 170, Log-Likelihood (scaled): inf\n",
            "Iteration 171, Log-Likelihood (scaled): inf\n",
            "Iteration 172, Log-Likelihood (scaled): inf\n",
            "Iteration 173, Log-Likelihood (scaled): inf\n",
            "Iteration 174, Log-Likelihood (scaled): inf\n",
            "Iteration 175, Log-Likelihood (scaled): inf\n",
            "Iteration 176, Log-Likelihood (scaled): inf\n",
            "Iteration 177, Log-Likelihood (scaled): inf\n",
            "Iteration 178, Log-Likelihood (scaled): inf\n",
            "Iteration 179, Log-Likelihood (scaled): inf\n",
            "Iteration 180, Log-Likelihood (scaled): inf\n",
            "Iteration 181, Log-Likelihood (scaled): inf\n",
            "Iteration 182, Log-Likelihood (scaled): inf\n",
            "Iteration 183, Log-Likelihood (scaled): inf\n",
            "Iteration 184, Log-Likelihood (scaled): inf\n",
            "Iteration 185, Log-Likelihood (scaled): inf\n",
            "Iteration 186, Log-Likelihood (scaled): inf\n",
            "Iteration 187, Log-Likelihood (scaled): inf\n",
            "Iteration 188, Log-Likelihood (scaled): inf\n",
            "Iteration 189, Log-Likelihood (scaled): inf\n",
            "Iteration 190, Log-Likelihood (scaled): inf\n",
            "Iteration 191, Log-Likelihood (scaled): inf\n",
            "Iteration 192, Log-Likelihood (scaled): inf\n",
            "Iteration 193, Log-Likelihood (scaled): inf\n",
            "Iteration 194, Log-Likelihood (scaled): inf\n",
            "Iteration 195, Log-Likelihood (scaled): inf\n",
            "Iteration 196, Log-Likelihood (scaled): inf\n",
            "Iteration 197, Log-Likelihood (scaled): inf\n",
            "Iteration 198, Log-Likelihood (scaled): inf\n",
            "Iteration 199, Log-Likelihood (scaled): inf\n",
            "Learned Weights (scaled): [-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n",
            " -100. -100. -100. -100. -100. -100. -100.]\n",
            "Final Normalization Parameter (w_norm): 1.7820693077580047e-131\n",
            "(199,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing datasets for model evaluation\n",
        "Data_train, Data_test = train_test_split(Data_discretized, test_size=0.2, random_state=5])\n",
        "# Example of calling the function\n",
        "updated_data = process_test_data(Data_test, support_df, categories_weights, constant)\n",
        "# Calculate the root mean squared error (RMSE) for the test data\n",
        "rmse = calculate_rmse_category(Data_test,intervals_df,updated_data)  # Use a custom RMSE calculation function\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl-JQrervTvG",
        "outputId": "c946fe2c-6af4-4cac-abec-fc0f738704af"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: C\n",
            "Actual Max Value: [0.74615947], Predicted Max Value: [0.74615947]\n",
            "Predicted Category: B\n",
            "Actual Max Value: [0.33894172], Predicted Max Value: [0.33894172]\n",
            "Predicted Category: C\n",
            "Actual Max Value: [1.], Predicted Max Value: [0.74615947]\n",
            "Predicted Category: B\n",
            "Actual Max Value: [0.23799073], Predicted Max Value: [0.33894172]\n",
            "Predicted Category: C\n",
            "Actual Max Value: [0.23799073], Predicted Max Value: [0.74615947]\n",
            "Predicted Category: B\n",
            "Actual Max Value: [0.23799073], Predicted Max Value: [0.33894172]\n",
            "Predicted Category: B\n",
            "Actual Max Value: [0.33894172], Predicted Max Value: [0.33894172]\n",
            "Predicted Category: C\n",
            "Actual Max Value: [0.23799073], Predicted Max Value: [0.74615947]\n",
            "Predicted Category: C\n",
            "Actual Max Value: [0.74615947], Predicted Max Value: [0.74615947]\n",
            "RMSE: (0.2584759073329495, 0.16356444227695138)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqvO8nzdvii2"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def find_best_rmse_random_state(Data_discretized, intervals_df, support_df, categories_weights, constant):\n",
        "    best_random_state = None\n",
        "    min_rmse = float('inf')  # Initialize with a large value to ensure any RMSE is smaller\n",
        "\n",
        "    # Iterate through random states from 5 to 200\n",
        "    for random_state in range(5, 201):\n",
        "        # Split the data into training and testing datasets\n",
        "        Data_train, Data_test = train_test_split(Data_discretized, test_size=0.2, random_state=random_state)\n",
        "\n",
        "        # Process the test data\n",
        "        updated_data = process_test_data(Data_test, support_df, categories_weights, constant)\n",
        "\n",
        "        # Calculate the root mean squared error (RMSE) and MAE\n",
        "        rmse, mae = calculate_rmse_category(Data_test, intervals_df, updated_data)\n",
        "\n",
        "        # Compare only the first value (RMSE) from the tuple\n",
        "        if rmse is not None and rmse < min_rmse:\n",
        "            min_rmse = rmse\n",
        "            best_random_state = random_state\n",
        "\n",
        "    return best_random_state, min_rmse\n",
        "\n",
        "# Example usage\n",
        "best_random_state, min_rmse = find_best_rmse_random_state(Data_discretized, intervals_df, support_df, categories_weights, constant)\n",
        "print(f\"Best Random State: {best_random_state}, RMSE: {min_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHvIo0GmO9Yq",
        "outputId": "34aeb828-a8c1-44f9-dc96-e5536e4b6232"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random State: 43, RMSE: 0.1612690803690874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7J2lB7MO-Bk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}